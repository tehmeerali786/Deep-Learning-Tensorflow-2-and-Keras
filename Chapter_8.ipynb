{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter 8.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmvMpL03qwSQ",
        "colab_type": "code",
        "outputId": "c4e827cd-bdf9-44ac-b218-b654971acccb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!pip install -q tensorflow==2.1.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 421.8MB 36kB/s \n",
            "\u001b[K     |████████████████████████████████| 450kB 52.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.9MB 60.0MB/s \n",
            "\u001b[?25h  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfGYF2Wxfr59",
        "colab_type": "text"
      },
      "source": [
        "#Text Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1l7tnTTVmf6N",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6tSOVtwq2CK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import re\n",
        "import shutil\n",
        "import tensorflow as tf\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WMQEaDsrjvX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_DIR  = \"./data\"\n",
        "CHECKPOINT_DIR = os.path.join(DATA_DIR, \"checkpoints\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXa5YONar8z7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def download_and_read(urls):\n",
        "  texts = []\n",
        "  for i, url in enumerate(urls):\n",
        "    p = tf.keras.utils.get_file(\"ex1-{:d}.txt\".format(i), url, cache_dir=\".\")\n",
        "    text = open(p, \"r\").read()\n",
        "    # byte remove order mark\n",
        "    text = text.replace(\"\\ufeff\", \"\")\n",
        "    # remove newlines\n",
        "    text = text.replace('\\n', ' ')\n",
        "    text = re.sub(r'\\s+', \" \", text)\n",
        "    #add to its list\n",
        "    texts.extend(text)\n",
        "  return texts\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qf2X-kfgwcC2",
        "colab_type": "code",
        "outputId": "43217702-5947-4167-dd60-162c6fced224",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "texts = download_and_read([\n",
        "                           \"http://www.gutenberg.org/cache/epub/28885/pg28885.txt\",\n",
        "                           \"https://www.gutenberg.org/files/12/12-0.txt\"\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://www.gutenberg.org/cache/epub/28885/pg28885.txt\n",
            "180224/177428 [==============================] - 1s 6us/step\n",
            "Downloading data from https://www.gutenberg.org/files/12/12-0.txt\n",
            "196608/193607 [==============================] - 1s 6us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0x84cKRjwnQv",
        "colab_type": "code",
        "outputId": "69a32814-ebe3-4735-dcf5-b5b1cd147581",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# create the vocabulary\n",
        "vocab = sorted(set(texts))\n",
        "print(\"vocab size: {:d}\".format(len(vocab)))\n",
        "\n",
        "# create mapping from vocab chars to ints\n",
        "char2idx = {c:i for i, c in enumerate(vocab)}\n",
        "idx2char = {i:c for c,i in char2idx.items()} "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab size: 90\n",
            "ERROR! Session/line number was not unique in database. History logging moved to new session 59\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBOlpKa9zqOD",
        "colab_type": "code",
        "outputId": "1951a1b5-7673-4f0b-b410-c40b16d2c5d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "for i, c in idx2char.items():\n",
        "  if i < 50:\n",
        "    print(i, c)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0  \n",
            "1 !\n",
            "2 \"\n",
            "3 #\n",
            "4 $\n",
            "5 %\n",
            "6 &\n",
            "7 '\n",
            "8 (\n",
            "9 )\n",
            "10 *\n",
            "11 ,\n",
            "12 -\n",
            "13 .\n",
            "14 /\n",
            "15 0\n",
            "16 1\n",
            "17 2\n",
            "18 3\n",
            "19 4\n",
            "20 5\n",
            "21 6\n",
            "22 7\n",
            "23 8\n",
            "24 9\n",
            "25 :\n",
            "26 ;\n",
            "27 ?\n",
            "28 @\n",
            "29 A\n",
            "30 B\n",
            "31 C\n",
            "32 D\n",
            "33 E\n",
            "34 F\n",
            "35 G\n",
            "36 H\n",
            "37 I\n",
            "38 J\n",
            "39 K\n",
            "40 L\n",
            "41 M\n",
            "42 N\n",
            "43 O\n",
            "44 P\n",
            "45 Q\n",
            "46 R\n",
            "47 S\n",
            "48 T\n",
            "49 U\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJ3-Z6xN0DkH",
        "colab_type": "code",
        "outputId": "df78cfb5-5add-4a8a-b1cc-eb8860592b9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "vocab"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' ',\n",
              " '!',\n",
              " '\"',\n",
              " '#',\n",
              " '$',\n",
              " '%',\n",
              " '&',\n",
              " \"'\",\n",
              " '(',\n",
              " ')',\n",
              " '*',\n",
              " ',',\n",
              " '-',\n",
              " '.',\n",
              " '/',\n",
              " '0',\n",
              " '1',\n",
              " '2',\n",
              " '3',\n",
              " '4',\n",
              " '5',\n",
              " '6',\n",
              " '7',\n",
              " '8',\n",
              " '9',\n",
              " ':',\n",
              " ';',\n",
              " '?',\n",
              " '@',\n",
              " 'A',\n",
              " 'B',\n",
              " 'C',\n",
              " 'D',\n",
              " 'E',\n",
              " 'F',\n",
              " 'G',\n",
              " 'H',\n",
              " 'I',\n",
              " 'J',\n",
              " 'K',\n",
              " 'L',\n",
              " 'M',\n",
              " 'N',\n",
              " 'O',\n",
              " 'P',\n",
              " 'Q',\n",
              " 'R',\n",
              " 'S',\n",
              " 'T',\n",
              " 'U',\n",
              " 'V',\n",
              " 'W',\n",
              " 'X',\n",
              " 'Y',\n",
              " 'Z',\n",
              " '[',\n",
              " ']',\n",
              " '_',\n",
              " 'a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'd',\n",
              " 'e',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 'j',\n",
              " 'k',\n",
              " 'l',\n",
              " 'm',\n",
              " 'n',\n",
              " 'o',\n",
              " 'p',\n",
              " 'q',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'v',\n",
              " 'w',\n",
              " 'x',\n",
              " 'y',\n",
              " 'z',\n",
              " '·',\n",
              " 'ù',\n",
              " '‘',\n",
              " '’',\n",
              " '“',\n",
              " '”']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWWY3tjm0TT2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# numericize the text\n",
        "texts_as_ints = np.array([char2idx[c] for c in texts])\n",
        "data = tf.data.Dataset.from_tensor_slices(texts_as_ints)\n",
        "\n",
        "# number of characters to show before asking for prediction\n",
        "# sequences: [None, 100]\n",
        "seq_length = 100\n",
        "sequences = data.batch(seq_length + 1, drop_remainder=True)\n",
        "\n",
        "def split_train_labels(sequence):\n",
        "  input_seq = sequence[0:-1]\n",
        "  output_seq = sequence[1:]\n",
        "  return input_seq, output_seq\n",
        "\n",
        "\n",
        "sequences = sequences.map(split_train_labels)\n",
        "\n",
        "# set up for training\n",
        "# batches: [None, 64, 100]\n",
        "\n",
        "batch_size = 64\n",
        "steps_per_epoch = len(texts) // seq_length // batch_size\n",
        "dataset = sequences.shuffle(10000).batch(batch_size, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54z5U_ho44u_",
        "colab_type": "code",
        "outputId": "51485f17-8d83-4e86-c57a-efcfd1c8929d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(sequences)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensorflow.python.data.ops.dataset_ops.MapDataset"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3voYfpo47Ib",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequencesshape = [i for i in sequences.as_numpy_iterator()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_LkWi007Idv",
        "colab_type": "code",
        "outputId": "0fd98772-7ad1-4a5a-fe81-9eaa2a27bedc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(sequencesshape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3423"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpDeTQPw9I0D",
        "colab_type": "code",
        "outputId": "fe96cc75-a8dc-43bc-fc9c-1f7446d99111",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "sequencesshape[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([44, 75, 72, 67, 62, 60, 77,  0, 35, 78, 77, 62, 71, 59, 62, 75, 64,\n",
              "         7, 76,  0, 29, 69, 66, 60, 62,  7, 76,  0, 29, 61, 79, 62, 71, 77,\n",
              "        78, 75, 62, 76,  0, 66, 71,  0, 51, 72, 71, 61, 62, 75, 69, 58, 71,\n",
              "        61, 11,  0, 59, 82,  0, 40, 62, 80, 66, 76,  0, 31, 58, 75, 75, 72,\n",
              "        69, 69,  0, 48, 65, 66, 76,  0, 62, 30, 72, 72, 68,  0, 66, 76,  0,\n",
              "        63, 72, 75,  0, 77, 65, 62,  0, 78, 76, 62,  0, 72, 63,  0]),\n",
              " array([75, 72, 67, 62, 60, 77,  0, 35, 78, 77, 62, 71, 59, 62, 75, 64,  7,\n",
              "        76,  0, 29, 69, 66, 60, 62,  7, 76,  0, 29, 61, 79, 62, 71, 77, 78,\n",
              "        75, 62, 76,  0, 66, 71,  0, 51, 72, 71, 61, 62, 75, 69, 58, 71, 61,\n",
              "        11,  0, 59, 82,  0, 40, 62, 80, 66, 76,  0, 31, 58, 75, 75, 72, 69,\n",
              "        69,  0, 48, 65, 66, 76,  0, 62, 30, 72, 72, 68,  0, 66, 76,  0, 63,\n",
              "        72, 75,  0, 77, 65, 62,  0, 78, 76, 62,  0, 72, 63,  0, 58]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqwvWdAZ9QMQ",
        "colab_type": "code",
        "outputId": "0b1d4c41-4a53-47bd-a2a2-ad7dbaa8f83e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "sequencesshape[3421]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([78, 77,  0, 44, 75, 72, 67, 62, 60, 77,  0, 35, 78, 77, 62, 71, 59,\n",
              "        62, 75, 64, 12, 77, 70, 11,  0, 66, 71, 60, 69, 78, 61, 66, 71, 64,\n",
              "         0, 65, 72, 80,  0, 77, 72,  0, 70, 58, 68, 62,  0, 61, 72, 71, 58,\n",
              "        77, 66, 72, 71, 76,  0, 77, 72,  0, 77, 65, 62,  0, 44, 75, 72, 67,\n",
              "        62, 60, 77,  0, 35, 78, 77, 62, 71, 59, 62, 75, 64,  0, 40, 66, 77,\n",
              "        62, 75, 58, 75, 82,  0, 29, 75, 60, 65, 66, 79, 62,  0, 34]),\n",
              " array([77,  0, 44, 75, 72, 67, 62, 60, 77,  0, 35, 78, 77, 62, 71, 59, 62,\n",
              "        75, 64, 12, 77, 70, 11,  0, 66, 71, 60, 69, 78, 61, 66, 71, 64,  0,\n",
              "        65, 72, 80,  0, 77, 72,  0, 70, 58, 68, 62,  0, 61, 72, 71, 58, 77,\n",
              "        66, 72, 71, 76,  0, 77, 72,  0, 77, 65, 62,  0, 44, 75, 72, 67, 62,\n",
              "        60, 77,  0, 35, 78, 77, 62, 71, 59, 62, 75, 64,  0, 40, 66, 77, 62,\n",
              "        75, 58, 75, 82,  0, 29, 75, 60, 65, 66, 79, 62,  0, 34, 72]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2pjAFk09X2Q",
        "colab_type": "code",
        "outputId": "e89a619b-535d-4c21-aa7c-0f55f3848ec3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "steps_per_epoch"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "54"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NsuQ6EP97EP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CharGenModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, num_timesteps, embedding_dim, rnn_output_dim, **kwargs):\n",
        "    super(CharGenModel, self).__init__(**kwargs)\n",
        "    self.embedding_layer = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.rnn_layer = tf.keras.layers.GRU(num_timesteps, recurrent_initializer=\"glorot_uniform\", recurrent_activation=\"sigmoid\", stateful=True, return_sequences=True)\n",
        "    self.dense_layer = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.embedding_layer(x)\n",
        "    x = self.rnn_layer(x)\n",
        "    x = self.dense_layer(x)\n",
        "    return x\n",
        "\n",
        "  \n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 256\n",
        "rnn_output_dim = 1024\n",
        "\n",
        "\n",
        "\n",
        "model = CharGenModel(vocab_size, seq_length, embedding_dim, rnn_output_dim)\n",
        "model.build(input_shape=(batch_size, seq_length))\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9lXW9x8DI2I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss(labels, predictions):\n",
        "  return tf.losses.sparse_categorical_crossentropy(labels, predictions, from_logits=True)\n",
        "\n",
        "model.compile(optimizer=tf.optimizers.Adam(), loss=loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkVGiXYxD-J1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, prefix_string, char2idx, idx2char, num_chars_to_generate=1000, temperature=1.0):\n",
        "  input = [char2idx[s] for s in prefix_string]\n",
        "  input = tf.expand_dims(input, 0)\n",
        "  text_generated = []\n",
        "  model.reset_states()\n",
        "  for i in range(num_chars_to_generate):\n",
        "    preds = model(input)\n",
        "    preds = tf.squeeze(preds, 0) / temperature\n",
        "    # predict characters by model\n",
        "    pred_id = tf.random.categorical(preds, num_samples=1)[-1, 0].numpy()\n",
        "    text_generated.append(idx2char[pred_id])\n",
        "    # pass prediction to the next input model\n",
        "    input = tf.expand_dims([pred_id], 0)\n",
        "\n",
        "  return prefix_string + \"\".join(text_generated)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aY-6h5Z2L7Gd",
        "colab_type": "code",
        "outputId": "b355f204-ddbc-4c34-b005-c02e69bdb027",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs = 50\n",
        "\n",
        "for i in range(num_epochs // 10):\n",
        "  model.fit(dataset.repeat(), epochs=10, steps_per_epoch=steps_per_epoch)\n",
        "  checkpoint_file = os.path.join(CHECKPOINT_DIR, \"model_epoch_{:d}\".format(i+1))\n",
        "  model.save_weights(checkpoint_file)\n",
        "\n",
        "  # create generative model using the trained model so far\n",
        "  gen_model = CharGenModel(vocab_size, seq_length, embedding_dim, rnn_output_dim)\n",
        "  gen_model.load_weights(checkpoint_file)\n",
        "  gen_model.build(input_shape=(1, seq_length))\n",
        "  \n",
        "\n",
        "  print(\"after epoch: {:d}\".format(i+1) * 10)\n",
        "  print(generate_text(gen_model, \"Alice \", char2idx, idx2char))\n",
        "  print(\"-------------\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 54 steps\n",
            "Epoch 1/10\n",
            "54/54 [==============================] - 14s 265ms/step - loss: 2.9159\n",
            "Epoch 2/10\n",
            "54/54 [==============================] - 14s 254ms/step - loss: 2.5394\n",
            "Epoch 3/10\n",
            "54/54 [==============================] - 14s 256ms/step - loss: 2.3694\n",
            "Epoch 4/10\n",
            "54/54 [==============================] - 14s 256ms/step - loss: 2.2465\n",
            "Epoch 5/10\n",
            "54/54 [==============================] - 14s 256ms/step - loss: 2.1535\n",
            "Epoch 6/10\n",
            "54/54 [==============================] - 14s 259ms/step - loss: 2.0690\n",
            "Epoch 7/10\n",
            "54/54 [==============================] - 14s 257ms/step - loss: 2.0010\n",
            "Epoch 8/10\n",
            "54/54 [==============================] - 14s 257ms/step - loss: 1.9426\n",
            "Epoch 9/10\n",
            "54/54 [==============================] - 14s 259ms/step - loss: 1.8908\n",
            "Epoch 10/10\n",
            "54/54 [==============================] - 14s 259ms/step - loss: 1.8454\n",
            "after epoch: 1after epoch: 1after epoch: 1after epoch: 1after epoch: 1after epoch: 1after epoch: 1after epoch: 1after epoch: 1after epoch: 1\n",
            "Alice her Proce at she himnsty softiong, ‘Dot on ast, see ithut il you that or dithought if dook I sty and of hak the carke wood, and hew!’ All may on one. HA Mack, arker clalupped inse, Lading nereathing dookrow uld gar!\" \"Why hean dow mint, and plasce the to seag.\" I's haining one vermerty Quever suy here it itp o‘Rew freace took, she look ararentres, do see rea beathone od the Witre woGeen't wares a cauld, and liestionm!’ And ong gool it you nathed all a glean’m copsed, dim the wastere. Igquatilying al up, homent--gades,_ and a pued swelf mad. I caje, the prims, she preaieg-- he nocrwenversh the do, wo thee bittle, as the, \"She thite Quee tith the moor cwas that?\" she sadding: wher! as the said id FI’ve of ovood. The Project nice the weppeed chelmed,\" the un’t verianl wints onton. ‘I fin whouch her to herwsefforyg some!\" shome in freall not econ!\" Sleat, chea esould miit witto thancten wit hooked u lighted nerey Pargarpabber wass out It_ the with afred, and thourmongan lice!’ vistce Quise\n",
            "-------------\n",
            "Train for 54 steps\n",
            "Epoch 1/10\n",
            "54/54 [==============================] - 15s 270ms/step - loss: 1.8078\n",
            "Epoch 2/10\n",
            "54/54 [==============================] - 14s 260ms/step - loss: 1.7741\n",
            "Epoch 3/10\n",
            "54/54 [==============================] - 14s 255ms/step - loss: 1.7424\n",
            "Epoch 4/10\n",
            "54/54 [==============================] - 14s 256ms/step - loss: 1.7185\n",
            "Epoch 5/10\n",
            "54/54 [==============================] - 14s 258ms/step - loss: 1.6900\n",
            "Epoch 6/10\n",
            "54/54 [==============================] - 18s 325ms/step - loss: 1.6676\n",
            "Epoch 7/10\n",
            "54/54 [==============================] - 30s 555ms/step - loss: 1.6461\n",
            "Epoch 8/10\n",
            "54/54 [==============================] - 29s 537ms/step - loss: 1.6346\n",
            "Epoch 9/10\n",
            "54/54 [==============================] - 27s 508ms/step - loss: 1.6134\n",
            "Epoch 10/10\n",
            "54/54 [==============================] - 28s 525ms/step - loss: 1.6016\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root)._training_endpoints\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).embedding_layer.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).dense_layer.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).dense_layer.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).embedding_layer.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).dense_layer.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).dense_layer.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.bias\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
            "after epoch: 2after epoch: 2after epoch: 2after epoch: 2after epoch: 2after epoch: 2after epoch: 2after epoch: 2after epoch: 2after epoch: 2\n",
            "Alice hempy in at door one to the said eltomer deaged at and! If you canuty this kef heruce, looking to the White!\" \"Formousiens, and Parge! “Tweedlede remair impiout oft, and Jugtm to the rule. Whurdence: you call, like, _sight to talk at down! I won dinimold this Marming tumpened worx allow on only vouling olfela-le omean; onks as the this for it wonkle the butt tole. ow inse Tile (hismerse angation looked at the had, and the Knigge thee she cribe neaks at uping, and has ame d quite to glas, in then findeed, and handed hol the dider, you centiend nots oum to there tame Enth WI]NINd it onleepsiate turnled inless. You CACE!’ the Racking all all be bisply things to he wonfing my down they--turnes_ I say periger or feet firting unoon the Mours,\" said sWas main, hoid on the wonis, any don't drear, I supper not et hurtle: the push how, when Alice Justed “ITE?’ whole, not's mading it's picarion a done. Alice at with mang-fornforget were sforencerenting it it haddy think mowill herself, for can,’ \n",
            "-------------\n",
            "Train for 54 steps\n",
            "Epoch 1/10\n",
            "54/54 [==============================] - 29s 531ms/step - loss: 1.5840\n",
            "Epoch 2/10\n",
            "54/54 [==============================] - 29s 542ms/step - loss: 1.5720\n",
            "Epoch 3/10\n",
            "54/54 [==============================] - 27s 496ms/step - loss: 1.5573\n",
            "Epoch 4/10\n",
            "54/54 [==============================] - 27s 500ms/step - loss: 1.5461\n",
            "Epoch 5/10\n",
            "54/54 [==============================] - 29s 536ms/step - loss: 1.5348\n",
            "Epoch 6/10\n",
            "54/54 [==============================] - 27s 495ms/step - loss: 1.5279\n",
            "Epoch 7/10\n",
            "54/54 [==============================] - 26s 480ms/step - loss: 1.5196\n",
            "Epoch 8/10\n",
            "54/54 [==============================] - 27s 502ms/step - loss: 1.5058\n",
            "Epoch 9/10\n",
            "54/54 [==============================] - 28s 510ms/step - loss: 1.4982\n",
            "Epoch 10/10\n",
            "54/54 [==============================] - 26s 482ms/step - loss: 1.4858\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root)._training_endpoints\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).embedding_layer.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).dense_layer.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).dense_layer.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).embedding_layer.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).dense_layer.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).dense_layer.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.bias\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
            "after epoch: 3after epoch: 3after epoch: 3after epoch: 3after epoch: 3after epoch: 3after epoch: 3after epoch: 3after epoch: 3after epoch: 3\n",
            "Alice whol the Queen?’ she with this chroech, is rear_ rathinked. The Queen spoke, and away, but her ruve while At sight VERoum, and shume has Hertimall--bussont my fee!’ stisplity \"perpovilly; surtly. ‘Spallered. \"What is, horsech come fase,’ Alice. \"So she wables Alice time-till the little helf sosting repliem, without me the hands: I’d! ‘I_ creature firedher! TZen too can two turnd their her as her had face remalar tell pund was in this could frightently. \"You knoweventh of the little perse leak. I minute, with quite hore Alice was new) 1aP1.5.8.,?’ He ehand himply. You must concescling. The Macked ubper. The Found, she added the hnomore and gardly sort those puzzled as they way of hope’s lessed. Oh,\" said the robly remayes--Foor had looking. ‘Then then’s broping this wordsent on each rather to drret knew was hound,\" said Aliczlding one with YOU thinks!’ thought herself agree about see replief the sort paving it, we Properming on, againster things atch if the down at laudich for kiffing a\n",
            "-------------\n",
            "Train for 54 steps\n",
            "Epoch 1/10\n",
            "54/54 [==============================] - 28s 522ms/step - loss: 1.4828\n",
            "Epoch 2/10\n",
            "54/54 [==============================] - 26s 481ms/step - loss: 1.4777\n",
            "Epoch 3/10\n",
            "54/54 [==============================] - 26s 485ms/step - loss: 1.4678\n",
            "Epoch 4/10\n",
            "54/54 [==============================] - 27s 492ms/step - loss: 1.4614\n",
            "Epoch 5/10\n",
            "54/54 [==============================] - 25s 469ms/step - loss: 1.4566\n",
            "Epoch 6/10\n",
            "54/54 [==============================] - 29s 537ms/step - loss: 1.4503\n",
            "Epoch 7/10\n",
            "54/54 [==============================] - 27s 494ms/step - loss: 1.4451\n",
            "Epoch 8/10\n",
            "54/54 [==============================] - 26s 474ms/step - loss: 1.4388\n",
            "Epoch 9/10\n",
            "54/54 [==============================] - 25s 466ms/step - loss: 1.4322\n",
            "Epoch 10/10\n",
            "54/54 [==============================] - 28s 514ms/step - loss: 1.4303\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root)._training_endpoints\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).embedding_layer.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).dense_layer.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).dense_layer.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).embedding_layer.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).dense_layer.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).dense_layer.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.bias\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
            "after epoch: 4after epoch: 4after epoch: 4after epoch: 4after epoch: 4after epoch: 4after epoch: 4after epoch: 4after epoch: 4after epoch: 4\n",
            "Alice said To worlered her talled pouse you finking out as prove tail, \"but, and, and he was suzzled things two repeat didn’t see his spovenchere shall, \"Sill meaphing prosice, setted off two look tranagfly. Pearning on this wait very Gryphon. ‘And the little began to, this must styat make quickly madly, as answer they took the prosistction saive with cause, but shehing. She sing on a fellon wor's in a little enoug great at to such don't gee like a told this quite so to sput that carn't dear off, and get its concelf car-trapsatily off, and _I_ had look-I ThI DILIMS·CINOESE1 THOME\"--MIVRE INDITTLE” TOLLAN ONF Tmees to the Project Gutenberg-tm wardense you know.”’ However. At this a little joid your way pawn tigh very like you are time have one twight durunged willous holdining pack there was great offend by the knieth yatter is: but smote the same said to you and slowly. \"The pack. \"Cheap pructme thing this. The other whose in me lurranull why did not turn to cream! Come is so main out of one\n",
            "-------------\n",
            "Train for 54 steps\n",
            "Epoch 1/10\n",
            "54/54 [==============================] - 26s 486ms/step - loss: 1.4246\n",
            "Epoch 2/10\n",
            "54/54 [==============================] - 26s 474ms/step - loss: 1.4196\n",
            "Epoch 3/10\n",
            "54/54 [==============================] - 27s 491ms/step - loss: 1.4137\n",
            "Epoch 4/10\n",
            "54/54 [==============================] - 26s 483ms/step - loss: 1.4109\n",
            "Epoch 5/10\n",
            "54/54 [==============================] - 25s 464ms/step - loss: 1.4084\n",
            "Epoch 6/10\n",
            "54/54 [==============================] - 27s 501ms/step - loss: 1.4023\n",
            "Epoch 7/10\n",
            "54/54 [==============================] - 27s 497ms/step - loss: 1.3966\n",
            "Epoch 8/10\n",
            "54/54 [==============================] - 26s 490ms/step - loss: 1.3968\n",
            "Epoch 9/10\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 1.3925\n",
            "Epoch 10/10\n",
            "54/54 [==============================] - 26s 489ms/step - loss: 1.3871\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root)._training_endpoints\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).embedding_layer.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).dense_layer.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).dense_layer.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).embedding_layer.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).dense_layer.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).dense_layer.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.bias\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
            "after epoch: 5after epoch: 5after epoch: 5after epoch: 5after epoch: 5after epoch: 5after epoch: 5after epoch: 5after epoch: 5after epoch: 5\n",
            "Alice squeeved a hook in armainate. \"Prawn yous hour. I do you all all been opening over no one from she wondered into a work tell!’ Humpty you had rest of slater doing at all full than tigchth in startles more refuly, and she had next.\" \"Come sighed oarmfiel bey exence roverely. But he beginning uboute. Hect BILL he was shook he went on. \"I’m sounday.’ ‘Why come, she had go and back with made a Turnave the little waited. Bophowing very see a confere tose forth to that dear be a too--I began, and round Alice, jumped. Peepledarded to gen on their a LItal came, hover your way to perwour to come, Double \"Oh candwant immess cigine at she _that surptifild for having not to crosour,\" Alice copies with I can get the mands agraly of come with he time, it’s ground the song, \"they!’ she don’t,\" said the M's WOUND INDRE SAPSED, howeve.\" She cried botome only domim were of her it, and she way you couldn’t kentioms colded and that then a colle been, as she couldn’t comileacep.\" \"But making at any over th\n",
            "-------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUEXOtFyNosz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKVDU6etfxCl",
        "colab_type": "text"
      },
      "source": [
        "#**Sentiment Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVqEeYPAf19S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmbZSHrag2h_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def download_and_read(url):\n",
        "    local_file = url.split('/')[-1]\n",
        "    local_file = local_file.replace(\"%20\", \" \")\n",
        "    p = tf.keras.utils.get_file(local_file, url, \n",
        "        extract=True, cache_dir=\".\")\n",
        "    local_folder = os.path.join(\"datasets\", local_file.split('.')[0])\n",
        "    labeled_sentences = []\n",
        "    for labeled_filename in os.listdir(local_folder):\n",
        "        if labeled_filename.endswith(\"_labelled.txt\"):\n",
        "            with open(os.path.join(local_folder, labeled_filename), \"r\") as f:\n",
        "                for line in f:\n",
        "                    sentence, label = line.strip().split('\\t')\n",
        "                    labeled_sentences.append((sentence, label))\n",
        "    return labeled_sentences\n",
        "\n",
        "\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glNtcny0L7AW",
        "colab_type": "code",
        "outputId": "98d7170b-dac3-43f1-ea97-7e6217bca404",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "labeled_sentences = download_and_read(\n",
        "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/00331/sentiment%20labelled%20sentences.zip\")\n",
        "sentences = [s for (s, l) in labeled_sentences]\n",
        "labels = [int(l) for (s, l) in labeled_sentences]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://archive.ics.uci.edu/ml/machine-learning-databases/00331/sentiment%20labelled%20sentences.zip\n",
            "90112/84188 [================================] - 0s 1us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zQb1td6MPrP",
        "colab_type": "code",
        "outputId": "f818181c-6043-4c32-8e90-c0dc9930f1c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "sentences[0:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Wow... Loved this place.',\n",
              " 'Crust is not good.',\n",
              " 'Not tasty and the texture was just nasty.',\n",
              " 'Stopped by during the late May bank holiday off Rick Steve recommendation and loved it.',\n",
              " 'The selection on the menu was great and so were the prices.',\n",
              " 'Now I am getting angry and I want my damn pho.',\n",
              " \"Honeslty it didn't taste THAT fresh.)\",\n",
              " 'The potatoes were like rubber and you could tell they had been made up ahead of time being kept under a warmer.',\n",
              " 'The fries were great too.',\n",
              " 'A great touch.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggqFjvQ9NoCN",
        "colab_type": "code",
        "outputId": "3ab322d6-717b-4d9a-c45e-cb7e0a18fc95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "labels[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 0, 0, 1, 1, 0, 0, 0, 1, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWHqMnFHN3_a",
        "colab_type": "code",
        "outputId": "06195f3c-cb95-4fb6-8f3a-d1bc94a3b6b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "labeled_sentences[:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Wow... Loved this place.', '1'),\n",
              " ('Crust is not good.', '0'),\n",
              " ('Not tasty and the texture was just nasty.', '0'),\n",
              " ('Stopped by during the late May bank holiday off Rick Steve recommendation and loved it.',\n",
              "  '1'),\n",
              " ('The selection on the menu was great and so were the prices.', '1')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQlo9Ft0N6eq",
        "colab_type": "code",
        "outputId": "59135ce5-5c53-4108-cdee-14162e219afd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "vocab_size = len(tokenizer.word_counts)\n",
        "print(\"Vocabulary size: {:d}\".format(vocab_size))\n",
        "\n",
        "word2idx = tokenizer.word_index\n",
        "idx2word = {v:k for (k, v) in word2idx.items()}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size: 5271\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QrPtqy2OlDs",
        "colab_type": "code",
        "outputId": "7be4c602-1089-4117-f8cb-3a9b79d5cd34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "idx2word[2]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'and'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBSkzNerUWqq",
        "colab_type": "code",
        "outputId": "d559a9de-124e-4798-ed16-1bbe9dd482bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "word2idx['and']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVsWWeUvUYp1",
        "colab_type": "code",
        "outputId": "02687ce0-62af-457c-cc9c-17cd195afa28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(word2idx)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6fpCbplUdaE",
        "colab_type": "code",
        "outputId": "9f973213-a859-4f10-8209-175fce937e88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "seq_lengths = np.array([len(s.split()) for s in sentences])\n",
        "print([(p, np.percentile(seq_lengths, p)) for p in [75, 80, 90, 95, 99, 100]])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(75, 16.0), (80, 18.0), (90, 22.0), (95, 26.0), (99, 36.0), (100, 71.0)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hA1IZpm2wECn",
        "colab_type": "code",
        "outputId": "f025ca76-a708-4458-ed87-a15dd110f64e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "max_length = 64\n",
        "\n",
        "# create a dataset\n",
        "sentences_as_ints = tokenizer.texts_to_sequences(sentences)\n",
        "sentences_as_ints[:2]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[652, 215, 8, 38], [1486, 5, 13, 18]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJv9UyNWxd8b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences_as_ints = tf.keras.preprocessing.sequence.pad_sequences(sentences_as_ints, max_length)\n",
        "labels_as_ints = np.array(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTWtwzwlyiUw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((sentences_as_ints, labels_as_ints))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOCHR5E9zOUk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = dataset.shuffle(10000)\n",
        "test_size = len(sentences) // 3\n",
        "val_size = (len(sentences) - test_size) // 10\n",
        "test_dataset = dataset.take(test_size)\n",
        "val_dataset = dataset.skip(test_size).take(val_size)\n",
        "train_dataset = dataset.skip(test_size + val_size)\n",
        "\n",
        "batch_size = 64\n",
        "train_dataset = train_dataset.batch(batch_size)\n",
        "val_dataset = val_dataset.batch(batch_size)\n",
        "test_dataset = test_dataset.batch(batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zsn1OIXk089H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SentimentAnalysisModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, max_seqlen, **kwargs):\n",
        "    super(SentimentAnalysisModel, self).__init__(**kwargs)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, max_length)\n",
        "    self.bilstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(max_length))\n",
        "    self.dense = tf.keras.layers.Dense(64, activation=\"relu\")\n",
        "    self.out =  tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.embedding(x)\n",
        "    x = self.bilstm(x)\n",
        "    x = self.dense(x)\n",
        "    x = self.out(x)\n",
        "    return x\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fp7uM7Gl683l",
        "colab_type": "code",
        "outputId": "f6f4102f-e8ad-44af-9c4f-e0ae1f4b25ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "model = SentimentAnalysisModel(vocab_size + 1, max_length)\n",
        "model.build(input_shape=(batch_size, max_length))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sentiment_analysis_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        multiple                  337408    \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional multiple                  66048     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                multiple                  8256      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              multiple                  65        \n",
            "=================================================================\n",
            "Total params: 411,777\n",
            "Trainable params: 411,777\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMSKBJ_97WOY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# compile\n",
        "model.compile(\n",
        "    loss= \"binary_crossentropy\",\n",
        "    optimizer = \"adam\",\n",
        "    metrics = [\"accuracy\"]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3XYBPCxBqNW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_logs(data_dir):\n",
        "    logs_dir = os.path.join(data_dir, \"logs\")\n",
        "    shutil.rmtree(logs_dir, ignore_errors=True)\n",
        "    return logs_dir"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtkBH95aB_s5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dir = \"./data\"\n",
        "logs_dir = clean_logs(data_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgNbt2y17v5X",
        "colab_type": "code",
        "outputId": "9c74c869-cc5c-44bf-b4de-317e75957566",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "# train\n",
        "best_model_file = os.path.join(data_dir, \"best_model.h5\")\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(best_model_file,\n",
        "    save_weights_only=True,\n",
        "    save_best_only=True)\n",
        "tensorboard = tf.keras.callbacks.TensorBoard(log_dir=logs_dir)\n",
        "num_epochs = 10\n",
        "history = model.fit(train_dataset, epochs=num_epochs, \n",
        "    validation_data=val_dataset,\n",
        "    callbacks=[checkpoint, tensorboard])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 29 steps, validate for 4 steps\n",
            "Epoch 1/10\n",
            "29/29 [==============================] - 3s 110ms/step - loss: 0.0086 - accuracy: 0.9989 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
            "Epoch 2/10\n",
            "29/29 [==============================] - 3s 108ms/step - loss: 0.0040 - accuracy: 0.9989 - val_loss: 0.0044 - val_accuracy: 1.0000\n",
            "Epoch 3/10\n",
            "29/29 [==============================] - 3s 105ms/step - loss: 0.0072 - accuracy: 0.9994 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
            "Epoch 4/10\n",
            "29/29 [==============================] - 3s 108ms/step - loss: 0.0059 - accuracy: 0.9989 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
            "Epoch 5/10\n",
            "29/29 [==============================] - 3s 108ms/step - loss: 0.0053 - accuracy: 0.9989 - val_loss: 8.9367e-04 - val_accuracy: 1.0000\n",
            "Epoch 6/10\n",
            "29/29 [==============================] - 3s 106ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9950\n",
            "Epoch 7/10\n",
            "29/29 [==============================] - 3s 106ms/step - loss: 9.6947e-04 - accuracy: 1.0000 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "29/29 [==============================] - 3s 106ms/step - loss: 0.0046 - accuracy: 0.9994 - val_loss: 7.6629e-04 - val_accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "29/29 [==============================] - 3s 107ms/step - loss: 0.0057 - accuracy: 0.9978 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "29/29 [==============================] - 3s 106ms/step - loss: 0.0036 - accuracy: 0.9989 - val_loss: 0.0022 - val_accuracy: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LW3jCnpF9mhN",
        "colab_type": "code",
        "outputId": "03a885d3-aa7e-40b5-e255-daf5597cc956",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "!tensorboard --logdir  /content/data/logs/train"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-03-30 20:14:19.745135: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2020-03-30 20:14:19.745249: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2020-03-30 20:14:19.745267: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
            "TensorBoard 2.1.1 at http://localhost:6007/ (Press CTRL+C to quit)\n",
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmcDxJV3EZGe",
        "colab_type": "code",
        "outputId": "bb8f0651-64e5-4e5a-e9fb-bf2904750da3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install tensorboardcolab"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboardcolab in /usr/local/lib/python3.6/dist-packages (0.0.22)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNlqyysPEeBH",
        "colab_type": "code",
        "outputId": "438bd44d-303d-44c8-c34e-89470f3214fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from tensorboardcolab import TensorBoardColab, TensorBoardColabCallback\n",
        "\n",
        "tbc=TensorBoardColab()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Wait for 8 seconds...\n",
            "TensorBoard link:\n",
            "https://e42cf188.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRMbiiGcEg1r",
        "colab_type": "code",
        "outputId": "0d2de935-d495-4322-c53d-161aeee50656",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "test_loss, test_acc = model.evaluate(test_dataset)\n",
        "print(\"test loss: {:3f}, test_accuracy: {:3f} \".format(test_loss, test_acc))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0030 - accuracy: 0.9990\n",
            "test loss: 0.003010, test_accuracy: 0.999000 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eI-cLTjDHwmA",
        "colab_type": "code",
        "outputId": "fe2b4333-1fca-4f4c-ecbd-30846fc30a20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "labels, predictions = [], []\n",
        "idx2word[0] = \"PAD\"\n",
        "is_first_batch = True\n",
        "for test_batch in test_dataset:\n",
        "  inputs_b, labels_b = test_batch\n",
        "  pred_batch = model.predict(inputs_b)\n",
        "  predictions.extend([(1 if p > 0.5 else 0) for p in pred_batch])\n",
        "  labels.extend([l for l in labels_b])\n",
        "  if is_first_batch:\n",
        "    # print first batch of label, prediction and sentence\n",
        "    for rid in range(inputs_b.shape[0]):\n",
        "      words = [idx2word[idx] for idx in inputs_b[rid].numpy()]\n",
        "      words = [w for w in words if w != \"PAD\"]\n",
        "      sentence = \" \".join(words)\n",
        "      print(\"{:d}\\t{:d}\\t{:s}\".format(labels[rid], predictions[rid], sentence))\n",
        "\n",
        "    is_first_batch = False\n",
        "\n",
        "\n",
        "print(\"accuracy score: {:3f}\".format(accuracy_score(labels, predictions)))\n",
        "print(\"confusion matrix\")\n",
        "print(confusion_matrix(labels, predictions))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\t1\tthe owner used to work at nobu so this place is really similar for half the price\n",
            "1\t1\twaitress was good though\n",
            "1\t1\tthe story is also both funny and poignant at times\n",
            "0\t0\tunfortunately it was not good\n",
            "0\t0\tthis one just fails to create any real suspense\n",
            "1\t1\tback to good bbq lighter fare reasonable pricing and tell the public they are back to the old ways\n",
            "1\t1\twe were promptly greeted and seated\n",
            "1\t1\tthe grilled chicken was so tender and yellow from the saffron seasoning\n",
            "1\t1\tas for the service i thought it was good\n",
            "1\t1\tand those baby owls were adorable\n",
            "0\t0\tlobster bisque bussell sprouts risotto filet all needed salt and pepper and of course there is none at the tables\n",
            "0\t0\twhat happened next was pretty off putting\n",
            "1\t1\tif you have not seen this movie i definitely recommend it\n",
            "0\t0\tworst service to boot but that is the least of their worries\n",
            "1\t1\tit is very comfortable on the ear\n",
            "0\t0\tthe football scenes at the end were perplexing\n",
            "0\t0\tdont go here\n",
            "0\t0\ti got home to see the driest damn wings ever\n",
            "0\t0\tprobably never coming back and wouldn't recommend it\n",
            "1\t1\t10 out of 10 for both the movie and trilogy\n",
            "1\t1\tfood was great and so was the serivce\n",
            "0\t0\tmy salad had a bland vinegrette on the baby greens and hearts of palm\n",
            "0\t0\tshrimp when i unwrapped it i live only 1 2 a mile from brushfire it was literally ice cold\n",
            "1\t1\ti have a verizon lg phone and they work well together good reception and range that exceeds 20 feet line of sight\n",
            "1\t1\ti enjoyed reading this book to my children when they were little\n",
            "0\t0\tbattery life still not long enough in motorola razor v3i\n",
            "1\t1\tvivian schilling did an excellent job with the script\n",
            "1\t1\thow awesome is that\n",
            "1\t1\ti had the opportunity today to sample your amazing pizzas\n",
            "1\t1\ti own 2 of these cases and would order another\n",
            "0\t0\tthe camera on the phone may be used as a dustpan when indoors i'd rather be using a disposable then this\n",
            "0\t0\tthe very idea of it was lame take a minor character from a mediocre pg 13 film and make a complete non sequel while changing its tone to a pg rated family movie\n",
            "1\t1\tthis blueant supertooth hands free phone speaker is awesome\n",
            "0\t0\tanyways the food was definitely not filling at all and for the price you pay you should expect more\n",
            "1\t1\tthree of the most visually appealing movies i've ever seen\n",
            "1\t1\tit was just not a fun experience\n",
            "1\t1\tfor the price this was a great deal\n",
            "1\t1\tall things considered a job very well done\n",
            "0\t0\tbut whatever it was that cost them so much it didn't translate to quality that's for sure\n",
            "1\t1\trestored my phone to like new performance\n",
            "0\t0\tthis show is made for americans it is too stupid and full with hatred and clichés to be admitted elsewhere\n",
            "0\t0\ti got the car charger and not even after a week the charger was broken i went to plug it in and it started smoking\n",
            "0\t0\tvery serious spoilers this movie was a huge disappointment\n",
            "1\t1\tthe dining space is tiny but elegantly decorated and comfortable\n",
            "0\t0\tmy only problem is i thought the actor playing the villain was a low rent michael ironside\n",
            "1\t1\ti've dropped my phone more times than i can say even on concrete and my phone is still great knock on wood\n",
            "0\t0\tthis results in the phone being either stuck at max volume or mute\n",
            "0\t0\tthe worst was the salmon sashimi\n",
            "0\t0\tbuyer beware you could flush money right down the toilet\n",
            "0\t0\tit had some average acting from the main person and it was a low budget as you clearly can see\n",
            "1\t1\texcellent phone\n",
            "1\t1\thow can anyone in their right mind ask for anything more from a movie than this\n",
            "1\t1\tshot in the southern california desert using his patent faux documentary style watkins creates a film like no other\n",
            "1\t1\ti will never forget it now\n",
            "1\t1\tservice was excellent and prices are pretty reasonable considering this is vegas and located inside the crystals shopping mall by aria\n",
            "1\t1\twill be back again\n",
            "1\t1\toh the charger seems to work fine\n",
            "0\t0\thighly unprofessional and rude to a loyal patron\n",
            "1\t1\ti am so thrilled after seeing a movie like this\n",
            "0\t0\toverall i was not impressed and would not go back\n",
            "0\t0\tit was too predictable even for a chick flick\n",
            "1\t1\tpaolo sorrentino has written a wonderful story about loneliness and tony has built one of the most unforgettable characters seen in movies in recent years\n",
            "1\t1\tthe nachos are a must have\n",
            "1\t1\tyou can't beat the price on these\n",
            "accuracy score: 1.000000\n",
            "confusion matrix\n",
            "[[487   0]\n",
            " [  0 513]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLMI0X2tLC4Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWI2Xu4NmiH5",
        "colab_type": "text"
      },
      "source": [
        "#**Part of Speech Tagging**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlsyKHLpmpah",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9k7WdYcmtrK",
        "colab_type": "code",
        "outputId": "eb6a27c6-11bb-4675-f1da-03288f3d3595",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "nltk.download(\"treebank\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVEmRXCwmzxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "646wEpx8m7fo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_logs(data_dir):\n",
        "    logs_dir = os.path.join(data_dir, \"logs\")\n",
        "    shutil.rmtree(logs_dir, ignore_errors=True)\n",
        "    return logs_dir\n",
        "\n",
        "\n",
        "def download_and_read(dataset_dir, num_pairs=None):\n",
        "    sent_filename = os.path.join(dataset_dir, \"treebank-sents.txt\")\n",
        "    poss_filename = os.path.join(dataset_dir, \"treebank-poss.txt\")\n",
        "    if not(os.path.exists(sent_filename) and os.path.exists(poss_filename)):\n",
        "        import nltk    \n",
        "\n",
        "        if not os.path.exists(dataset_dir):\n",
        "            os.makedirs(dataset_dir)\n",
        "        fsents = open(sent_filename, \"w\")\n",
        "        fposs = open(poss_filename, \"w\")\n",
        "        sentences = nltk.corpus.treebank.tagged_sents()\n",
        "        for sent in sentences:\n",
        "            fsents.write(\" \".join([w for w, p in sent]) + \"\\n\")\n",
        "            fposs.write(\" \".join([p for w, p in sent]) + \"\\n\")\n",
        "\n",
        "        fsents.close()\n",
        "        fposs.close()\n",
        "    sents, poss = [], []\n",
        "    with open(sent_filename, \"r\") as fsent:\n",
        "        for idx, line in enumerate(fsent):\n",
        "            sents.append(line.strip())\n",
        "            if num_pairs is not None and idx >= num_pairs:\n",
        "                break\n",
        "    with open(poss_filename, \"r\") as fposs:\n",
        "        for idx, line in enumerate(fposs):\n",
        "            poss.append(line.strip())\n",
        "            if num_pairs is not None and idx >= num_pairs:\n",
        "                break\n",
        "    return sents, poss\n",
        "\n",
        "\n",
        "def tokenize_and_build_vocab(texts, vocab_size=None, lower=True):\n",
        "    if vocab_size is None:\n",
        "        tokenizer = tf.keras.preprocessing.text.Tokenizer(lower=lower)\n",
        "    else:\n",
        "        tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "            num_words=vocab_size+1, oov_token=\"UNK\", lower=lower)\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    if vocab_size is not None:\n",
        "        # additional workaround, see issue 8092\n",
        "        # https://github.com/keras-team/keras/issues/8092\n",
        "        tokenizer.word_index = {e:i for e, i in tokenizer.word_index.items() \n",
        "            if i <= vocab_size+1 }\n",
        "    word2idx = tokenizer.word_index\n",
        "    idx2word = {v:k for k, v in word2idx.items()}\n",
        "    return word2idx, idx2word, tokenizer\n",
        "\n",
        "\n",
        "class POSTaggingModel(tf.keras.Model):\n",
        "    def __init__(self, source_vocab_size, target_vocab_size,\n",
        "            embedding_dim, max_seqlen, rnn_output_dim, **kwargs):\n",
        "        super(POSTaggingModel, self).__init__(**kwargs)\n",
        "        self.embed = tf.keras.layers.Embedding(\n",
        "            source_vocab_size, embedding_dim, input_length=max_seqlen)\n",
        "        self.dropout = tf.keras.layers.SpatialDropout1D(0.2)\n",
        "        self.rnn = tf.keras.layers.Bidirectional(\n",
        "            tf.keras.layers.GRU(rnn_output_dim, return_sequences=True))\n",
        "        self.dense = tf.keras.layers.TimeDistributed(\n",
        "            tf.keras.layers.Dense(target_vocab_size))\n",
        "        self.activation = tf.keras.layers.Activation(\"softmax\")\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.embed(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.rnn(x)\n",
        "        x = self.dense(x)\n",
        "        x = self.activation(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def masked_accuracy():\n",
        "    def masked_accuracy_fn(ytrue, ypred):\n",
        "        ytrue = tf.keras.backend.argmax(ytrue, axis=-1)\n",
        "        ypred = tf.keras.backend.argmax(ypred, axis=-1)\n",
        " \n",
        "        mask = tf.keras.backend.cast(\n",
        "            tf.keras.backend.not_equal(ypred, 0), tf.int32)\n",
        "        matches = tf.keras.backend.cast(\n",
        "            tf.keras.backend.equal(ytrue, ypred), tf.int32) * mask\n",
        "        numer = tf.keras.backend.sum(matches)\n",
        "        denom = tf.keras.backend.maximum(tf.keras.backend.sum(mask), 1)\n",
        "        accuracy =  numer / denom\n",
        "        return accuracy\n",
        "\n",
        "    return masked_accuracy_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zewrZ-07Ur2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -r datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDeYVm2Mv1DP",
        "colab_type": "code",
        "outputId": "f4323741-2007-4631-94a2-4be2e14a262f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "NUM_PAIRS = None\n",
        "EMBEDDING_DIM = 128\n",
        "RNN_OUTPUT_DIM = 256\n",
        "BATCH_SIZE = 128\n",
        "NUM_EPOCHS = 50\n",
        "\n",
        "# set random seed\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# clean up log area\n",
        "data_dir = \"./data\"\n",
        "logs_dir = clean_logs(data_dir)\n",
        "\n",
        "# download and read source and target data into data structure\n",
        "sents, poss = download_and_read(\"./datasets\", num_pairs=NUM_PAIRS)\n",
        "assert(len(sents) == len(poss))\n",
        "print(\"# of records: {:d}\".format(len(sents)))\n",
        "\n",
        "# vocabulary sizes\n",
        "word2idx_s, idx2word_s, tokenizer_s = tokenize_and_build_vocab(\n",
        "    sents, vocab_size=9000)\n",
        "word2idx_t, idx2word_t, tokenizer_t = tokenize_and_build_vocab(\n",
        "    poss, vocab_size=38, lower=False)\n",
        "source_vocab_size = len(word2idx_s)\n",
        "target_vocab_size = len(word2idx_t)\n",
        "print(\"vocab sizes (source): {:d}, (target): {:d}\".format(\n",
        "    source_vocab_size, target_vocab_size))\n",
        "\n",
        "# # max sequence length - these should be identical on source and\n",
        "# # target so we can just analyze one of them and choose max_seqlen\n",
        "# sequence_lengths = np.array([len(s.split()) for s in sents])\n",
        "# print([(p, np.percentile(sequence_lengths, p)) \n",
        "#     for p in [75, 80, 90, 95, 99, 100]])\n",
        "# # [(75, 33.0), (80, 35.0), (90, 41.0), (95, 47.0), (99, 58.0), (100, 271.0)]\n",
        "max_seqlen = 271\n",
        "\n",
        "# create dataset\n",
        "sents_as_ints = tokenizer_s.texts_to_sequences(sents)\n",
        "sents_as_ints = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    sents_as_ints, maxlen=max_seqlen, padding=\"post\")\n",
        "poss_as_ints = tokenizer_t.texts_to_sequences(poss)\n",
        "poss_as_ints = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    poss_as_ints, maxlen=max_seqlen, padding=\"post\")\n",
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (sents_as_ints, poss_as_ints))\n",
        "idx2word_s[0], idx2word_t[0] = \"PAD\", \"PAD\"\n",
        "poss_as_catints = []\n",
        "for p in poss_as_ints:\n",
        "    poss_as_catints.append(tf.keras.utils.to_categorical(p, \n",
        "        num_classes=target_vocab_size, dtype=\"int32\"))\n",
        "poss_as_catints = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    poss_as_catints, maxlen=max_seqlen)\n",
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (sents_as_ints, poss_as_catints))\n",
        "\n",
        "# split into training, validation, and test datasets\n",
        "dataset = dataset.shuffle(10000)\n",
        "test_size = len(sents) // 3\n",
        "val_size = (len(sents) - test_size) // 10\n",
        "test_dataset = dataset.take(test_size)\n",
        "val_dataset = dataset.skip(test_size).take(val_size)\n",
        "train_dataset = dataset.skip(test_size + val_size)\n",
        "\n",
        "# create batches\n",
        "batch_size = BATCH_SIZE\n",
        "train_dataset = train_dataset.batch(batch_size)\n",
        "val_dataset = val_dataset.batch(batch_size)\n",
        "test_dataset = test_dataset.batch(batch_size)\n",
        "\n",
        "# define model\n",
        "embedding_dim = EMBEDDING_DIM\n",
        "rnn_output_dim = RNN_OUTPUT_DIM\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# of records: 3914\n",
            "vocab sizes (source): 9001, (target): 39\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYfm2w7V4Jxc",
        "colab_type": "code",
        "outputId": "275c7b83-935b-4663-958d-3207010cadae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        }
      },
      "source": [
        "model = POSTaggingModel(source_vocab_size, target_vocab_size,\n",
        "    embedding_dim, max_seqlen, rnn_output_dim)\n",
        "model.build(input_shape=(batch_size, max_seqlen))\n",
        "model.summary()\n",
        "\n",
        "model.compile(\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    optimizer=\"adam\", \n",
        "    metrics=[\"accuracy\", masked_accuracy()])\n",
        "\n",
        "# for input_b, output_b in train_dataset.take(1):\n",
        "#     pred_b = model(input_b)\n",
        "#     pred_b = tf.argmax(pred_b, axis=-1)\n",
        "# print(\"in:\", input_b.shape, \"label:\", output_b.shape, \n",
        "#     \"prediction:\", pred_b.shape)\n",
        "\n",
        "# train\n",
        "num_epochs = NUM_EPOCHS\n",
        "\n",
        "best_model_file = os.path.join(data_dir, \"best_model.h5\")\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    best_model_file, \n",
        "    save_weights_only=True,\n",
        "    save_best_only=True)\n",
        "tensorboard = tf.keras.callbacks.TensorBoard(log_dir=logs_dir)\n",
        "history = model.fit(train_dataset, \n",
        "    epochs=num_epochs,\n",
        "    validation_data=val_dataset,\n",
        "    callbacks=[checkpoint, tensorboard])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"pos_tagging_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        multiple                  1152128   \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d (SpatialDr multiple                  0         \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional multiple                  592896    \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri multiple                  20007     \n",
            "_________________________________________________________________\n",
            "activation (Activation)      multiple                  0         \n",
            "=================================================================\n",
            "Total params: 1,765,031\n",
            "Trainable params: 1,765,031\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train for 19 steps, validate for 3 steps\n",
            "Epoch 1/50\n",
            "10/19 [==============>...............] - ETA: 38s - loss: 2.5290 - accuracy: 0.8151 - masked_accuracy_fn: 0.0040WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-fc0da8f56720>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     callbacks=[checkpoint, tensorboard])\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Hy8eQWH5GC5",
        "colab_type": "code",
        "outputId": "961b925c-4f49-47d3-b9be-b50570c2881d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        }
      },
      "source": [
        "# evaluate with test\n",
        "best_model = POSTaggingModel(source_vocab_size, target_vocab_size, embedding_dim, max_seqlen, rnn_output_dim)\n",
        "best_model.build(input_shape=(batch_size, max_seqlen))\n",
        "best_model.load_weights(best_model_file)\n",
        "best_model.compile(\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    optimizer = \"adam\",\n",
        "    metrics = [\"accuracy\", masked_accuracy()]\n",
        ")\n",
        "\n",
        "test_loss, test_acc, test_masked_acc = best_model.evaluate(test_dataset)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11/11 [==============================] - 13s 1s/step - loss: 0.0601 - accuracy: 0.9816 - masked_accuracy_fn: 0.7921\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-79f7198e403c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_masked_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test loss: {:3f}, test_accuracy: {:3f}, masked_test_accuracy: {:3d}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_masked_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: Unknown format code 'd' for object of type 'float'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: Current TensorFlow version is 2.1.0. To use TF 1.x instead,\nrestart your runtime (Ctrl+M .) and run \"%tensorflow_version 1.x\" before\nyou run \"import tensorflow\".\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLF3-7Oj6WRs",
        "colab_type": "code",
        "outputId": "d2c435cd-95d3-490b-8862-ed5c8ba739a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"test loss: {:3f}, test_accuracy: {:3f}, masked_test_accuracy: {:3f}\".format(test_loss, test_acc, test_masked_acc))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test loss: 0.060080, test_accuracy: 0.981598, masked_test_accuracy: 0.792096\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeN6epxK6yXJ",
        "colab_type": "code",
        "outputId": "1a6367aa-d5c1-4cbe-e0a0-f6467bdabffd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# predict on batches\n",
        "labels, predictions = [], []\n",
        "is_first_batch = True\n",
        "accuracies = []\n",
        "\n",
        "for test_batch in test_dataset:\n",
        "    inputs_b, outputs_b = test_batch\n",
        "    preds_b = best_model.predict(inputs_b)\n",
        "    # convert from categorical to list of ints\n",
        "    preds_b = np.argmax(preds_b, axis=-1)\n",
        "    outputs_b = np.argmax(outputs_b.numpy(), axis=-1)\n",
        "    for i, (pred_l, output_l) in enumerate(zip(preds_b, outputs_b)):\n",
        "        assert(len(pred_l) == len(output_l))\n",
        "        pad_len = np.nonzero(output_l)[0][0]\n",
        "        acc = np.count_nonzero(\n",
        "            np.equal(\n",
        "                output_l[pad_len:], pred_l[pad_len:]\n",
        "            )\n",
        "        ) / len(output_l[pad_len:])\n",
        "        accuracies.append(acc)\n",
        "        if is_first_batch:\n",
        "            words = [idx2word_s[x] for x in inputs_b.numpy()[i][pad_len:]]\n",
        "            postags_l = [idx2word_t[x] for x in output_l[pad_len:] if x > 0]\n",
        "            postags_p = [idx2word_t[x] for x in pred_l[pad_len:] if x > 0]\n",
        "            print(\"labeled  : {:s}\".format(\" \".join([\"{:s}/{:s}\".format(w, p) \n",
        "                for (w, p) in zip(words, postags_l)])))\n",
        "            print(\"predicted: {:s}\".format(\" \".join([\"{:s}/{:s}\".format(w, p) \n",
        "                for (w, p) in zip(words, postags_p)])))\n",
        "            print(\" \")\n",
        "    is_first_batch = False\n",
        "\n",
        "accuracy_score = np.mean(np.array(accuracies))\n",
        "print(\"pos tagging accuracy: {:.3f}\".format(accuracy_score))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "labeled  : for/IN fiscal/JJ 1989/CD the/DT company/NN posted/VBD net/NN of/IN UNK/CD 9/CD million/NONE u/CC or/CD 2/NONE 87/DT u/NN a/IN share/IN down/CD from/CD UNK/NONE 9/CC million/CD u/NONE or/DT 3/NN 04/IN u/JJ a/CD\n",
            "predicted: for/IN fiscal/JJ 1989/CD the/DT company/NN posted/VBD net/JJ of/IN UNK/CD 9/CD million/CD u/CC or/CD 2/NONE 87/NONE u/DT a/NN share/IN down/IN from/CD UNK/CD 9/CD million/CC u/CC or/CD 3/NN 04/NN u/DT a/NN share/IN\n",
            " \n",
            "labeled  : texaco/NNP rose/VBD 3/CD 4/TO to/CD 53/CD 3/IN 8/CD as/CD 4/NNS 4/VBD million/NNS\n",
            "predicted: texaco/NNP rose/VBD 3/CD 4/TO to/CD 53/CD 3/CD 8/CD as/CD 4/CD 4/CD million/NNS shares/NNS\n",
            " \n",
            "labeled  : but/CC other/JJ people/NNS do/VBP n't/RB want/VB 1/NONE to/TO lose/VB the/DT bridges/NNS '/POS beautiful/JJ sometimes/RB historic/JJ features/NNS\n",
            "predicted: but/CC other/JJ people/NNS do/VBP n't/RB want/VB 1/NONE to/TO lose/VB the/DT bridges/NNS '/POS beautiful/JJ sometimes/RB historic/JJ features/NNS\n",
            " \n",
            "labeled  : in/IN addition/NN to/TO the/DT damages/NNS the/DT suit/NN seeks/VBZ a/DT court/NN order/NN preventing/VBG the/DT guild/NN from/IN 2/NONE punishing/VBG rnr/NONE 1/CC or/VBG retaliating/IN against/NONE rnr/NNP 1/NNP\n",
            "predicted: in/IN addition/NN to/TO the/DT damages/NNS the/DT suit/NN seeks/VBZ a/DT court/NN order/NN preventing/VBG the/DT guild/NN from/IN 2/NONE punishing/VBG rnr/NONE 1/CC or/IN retaliating/IN against/NONE rnr/NNP 1/NNP\n",
            " \n",
            "labeled  : guaranteed/NONE 1/VBN by/NONE svenska/IN handelsbanken/NNP PAD/NNP\n",
            "predicted: guaranteed/VBN 1/NONE by/NONE svenska/IN handelsbanken/NNP PAD/NNP\n",
            " \n",
            "labeled  : for/IN 10/CD years/NNS genie/NNP driskill/NNP went/VBD to/TO her/PRP neighborhood/NN bank/NN because/IN it/PRP was/VBD convenient/JJ\n",
            "predicted: for/IN 10/CD years/NNS genie/NNP driskill/NNP went/VBD to/TO her/PRP neighborhood/NN bank/NN because/IN it/PRP was/VBD convenient/JJ\n",
            " \n",
            "labeled  : abrupt/JJ departures/NNS are/VBP n't/RB unheard/JJ of/IN 1/NONE within/IN the/DT newhouse/NNP empire/NN\n",
            "predicted: abrupt/JJ departures/NNS are/VBP n't/RB unheard/JJ of/IN 1/NONE within/IN the/DT newhouse/NNP empire/NN\n",
            " \n",
            "labeled  : in/IN addition/NN buick/NNP is/VBZ a/DT relatively/RB respected/VBN nameplate/NN among/IN american/NNP express/NNP card/NN holders/NNS says/VBZ 0/NONE t/NONE 1/DT an/NNP american/NNP express/NN\n",
            "predicted: in/IN addition/NN buick/NNP is/VBZ a/DT relatively/RB respected/VBN nameplate/NN among/IN american/NNP express/NNP card/NN holders/NNS says/VBZ 0/NONE t/NONE 1/DT an/NNP american/NNP express/NN\n",
            " \n",
            "labeled  : a/DT shearson/NNP spokesman/NN said/VBD 0/NONE the/DT firm/NN is/VBZ n't/RB worried/VBN\n",
            "predicted: a/DT shearson/NNP spokesman/NN said/VBD 0/NONE the/DT firm/NN is/VBZ n't/RB worried/VBN\n",
            " \n",
            "labeled  : medicine/NNP transplant/NNP growth/NN of/IN japanese/JJ trade/NN and/CC travel/NN prompts/VBZ beth/NNP israel/NNP medical/NNP center/NNP new/NNP york/NNP to/TO set/VB up/RP a/DT bilingual/JJ medical/JJ practice/NN\n",
            "predicted: medicine/NNP transplant/NNP growth/NN of/IN japanese/JJ trade/NN and/CC travel/NN prompts/VBZ beth/NNP israel/NNP medical/NNP center/NNP new/NNP york/NNP to/NONE set/VB up/RP a/DT bilingual/JJ medical/JJ practice/NN\n",
            " \n",
            "labeled  : 1/NONE crude/JJ as/IN they/PRP were/VBD these/NONE early/DT pcs/JJ triggered/NNS explosive/VBD product/JJ development/NN in/NN desktop/IN models/NN for/NNS the/IN home/DT and/NN office/CC PAD/NN\n",
            "predicted: 1/NONE crude/JJ as/IN they/PRP were/VBD these/DT early/JJ pcs/NNS triggered/NNS explosive/VBD product/NN development/NN in/NN desktop/IN models/IN for/IN the/DT home/DT and/CC office/NN\n",
            " \n",
            "labeled  : next/JJ week/NN the/DT philippine/NNP fund/NNP 's/POS launch/NN will/MD be/VB capped/VBN 46/NONE by/IN a/DT visit/NN by/IN philippine/JJ president/NNP corazon/NNP aquino/NNP the/NNP first/JJ time/NN 0/NONE a/DT head/NN of/IN state/NN has/VBZ kicked/VBN off/RP an/DT issue/NN at/IN the/DT big/NNP board/NNP here/RB t/NONE\n",
            "predicted: next/JJ week/NN the/DT philippine/NNP fund/NNP 's/POS launch/NN will/MD be/VB capped/VBN 46/NONE by/IN a/DT visit/NN by/IN philippine/NNP president/NNP corazon/NNP aquino/NNP the/DT first/JJ time/NN 0/NONE a/DT head/NN of/IN state/NN has/VBZ kicked/VBN off/RP an/DT issue/NN at/IN the/DT big/NNP board/NNP here/RB t/NONE\n",
            " \n",
            "labeled  : kill/NONE it/VB ''/PRP he/'' says/PRP t/VBZ 1/NONE\n",
            "predicted: kill/NONE it/PRP ''/PRP he/PRP says/VBZ t/NONE\n",
            " \n",
            "labeled  : he/PRP says/VBZ 0/NONE the/DT 10/CD citizen/JJ sparked/NNS issues/IN on/NN state/NNS ballots/DT this/NN fall/VBP represent/DT the/JJS most/IN in/DT any/JJ odd/DT year/NN\n",
            "predicted: he/PRP says/VBZ 0/NONE the/DT 10/CD citizen/JJ sparked/NNS issues/IN on/IN state/NNS ballots/DT this/NN fall/IN represent/DT the/DT most/IN in/DT any/JJ odd/DT year/DT\n",
            " \n",
            "labeled  : volume/NN in/IN the/DT second/JJ section/NN was/VBD estimated/VBN 1/NONE at/IN 18/CD million/CD shares/NNS up/RB from/IN 14/CD million/CD tuesday/NNP\n",
            "predicted: volume/NN in/IN the/DT second/NNP section/NNP was/VBD estimated/VBN 1/NONE at/IN 18/CD million/CD shares/NNS up/IN from/IN 14/CD million/CD tuesday/NNP\n",
            " \n",
            "labeled  : a/DT plan/NN to/NONE bring/TO the/VB stock/DT to/NN market/TO before/NN year/IN end/NN apparently/NN was/RB upset/VBD 1/VBN by/NONE the/IN recent/DT weakness/JJ of/NN frankfurt/IN share/NNP prices/NN PAD/NNS\n",
            "predicted: a/DT plan/NN to/TO bring/TO the/VB stock/DT to/NN market/NN before/NN year/NN end/NN apparently/RB was/VBD upset/VBN 1/NONE by/IN the/IN recent/DT weakness/JJ of/NN frankfurt/IN share/NN prices/NNS\n",
            " \n",
            "labeled  : in/IN another/DT action/NN the/DT itc/NNP dismissed/VBD anti/JJ dumping/NN act/NNS complaints/VBN filed/NONE by/IN du/NNP pont/NNP co/NNP of/IN UNK/NNP del/NNP against/IN imports/NNS of/IN UNK/NN a/DT type/NN of/IN synthetic/JJ rubber/NN from/IN france/NNP and/CC west/NNP germany/NNP\n",
            "predicted: in/IN another/DT action/NN the/DT itc/NNP dismissed/VBD anti/JJ dumping/NN act/NNS complaints/NNS filed/VBN by/NONE du/NNP pont/NNP co/NNP of/IN UNK/NNP del/NNP against/IN imports/NNS of/IN UNK/NN a/DT type/NN of/IN synthetic/JJ rubber/NN from/IN france/NNP and/CC west/NNP germany/NNP\n",
            " \n",
            "labeled  : reed/NNP is/VBZ paying/VBG an/DT interim/JJ dividend/NN of/IN 4/CD 6/NN pence/RB up/CD 15/NN from/IN 4/CD pence/NN a/DT year/NN earlier/JJR\n",
            "predicted: reed/NNP is/VBZ paying/VBG an/DT interim/JJ dividend/NN of/IN 4/CD 6/NN pence/NN up/IN 15/IN from/NN 4/NN pence/NN a/NN year/NN\n",
            " \n",
            "labeled  : mr/NNP pickens/NNP made/VBD considerable/JJ political/JJ hay/NN with/IN his/PRP troubles/NNS in/IN japan/NNP\n",
            "predicted: mr/NNP pickens/NNP made/VBD considerable/JJ political/JJ hay/NN with/IN his/PRP troubles/NNS in/IN japan/NNP\n",
            " \n",
            "labeled  : since/IN then/RB a/DT team/NN of/IN about/RB 15/CD miti/NNP and/CC u/NNP s/NNP commerce/NNP department/NNS officials/VBP have/VBN crossed/DT the/NN globe/NONE 1/VBG gauging/NN consumer/NNS\n",
            "predicted: since/IN then/RB a/DT team/NN of/IN about/IN 15/CD miti/NNP and/CC u/NNP s/NNP commerce/NNP department/NNS officials/VBP have/VBN crossed/DT the/NN globe/NONE 1/VBG gauging/NN consumer/NN\n",
            " \n",
            "labeled  : fall/NN ballot/NN issues/NNS set/VBD a/DT record/NN for/IN off/JJ year/NNS\n",
            "predicted: fall/NN ballot/NN issues/NNS set/VBD a/DT record/NN for/IN off/IN year/NNS\n",
            " \n",
            "labeled  : it/PRP 's/VBZ a/DT cosmetic/JJ move/NN ''/'' said/VBD t/NONE 1/NNP jonathan/NNP s/NNP UNK/IN of/NNP UNK/NNP UNK/CC co/NNP\n",
            "predicted: it/PRP 's/VBZ a/DT cosmetic/JJ move/NN ''/'' said/VBD t/NONE 1/NNP jonathan/NNP s/NNP UNK/IN of/NNP UNK/NNP UNK/NNP\n",
            " \n",
            "labeled  : lrb/LRB fewer/JJR said/VBD 0/NONE conditions/NNS wo/MD n't/RB change/VB rrb/RRB\n",
            "predicted: lrb/LRB fewer/VB said/VBN 0/NONE conditions/NNS wo/MD n't/RB change/VB rrb/RRB\n",
            " \n",
            "labeled  : copperweld/NNP corp/NNP a/DT specialty/NN steelmaker/NN said/VBD 0/NONE 445/CD workers/NNS at/IN a/DT plant/NN in/IN shelby/NNP ohio/NNP began/VBD a/DT strike/NN after/IN the/DT united/NNP steelworkers/NNP local/NNP 3057/CD rejected/VBD a/DT new/JJ contract/NN on/IN tuesday/NNP\n",
            "predicted: copperweld/NNP corp/NNP a/DT specialty/NN steelmaker/NN said/VBD 0/NONE 445/CD workers/NNS at/IN a/DT plant/NN in/IN shelby/NNP ohio/NNP began/VBD a/DT strike/NN after/IN the/DT united/NNP steelworkers/NNP local/NNP 3057/CD rejected/VBD a/DT new/JJ contract/NN on/IN tuesday/NNP\n",
            " \n",
            "labeled  : the/DT question/NN is/VBZ whether/IN his/PRP only/JJ means/NNS of/IN defense/NN is/VBZ the/DT veto/NN\n",
            "predicted: the/DT question/NN is/VBZ whether/IN his/PRP only/JJ means/VBZ of/IN defense/NN is/VBZ the/DT veto/NN\n",
            " \n",
            "labeled  : the/DT UNK/NNP ohio/JJ based/NN company/RB also/VBD said/DT that/JJ full/NN year/IN profit/VBG from/NNS continuing/MD operations/VB will/RB be/IN far/JJ below/NN last/POS year/CD 's/CD 148/NONE\n",
            "predicted: the/DT UNK/NNP ohio/JJ based/NN company/RB also/RB said/VBD that/JJ full/NN year/NN profit/IN from/NNS continuing/NNS operations/MD will/VB be/IN far/IN below/JJ last/NN year/NN 's/CD 148/NONE million/NONE\n",
            " \n",
            "labeled  : money/NNP market/NNP deposits/NNP a/CD 6/NN\n",
            "predicted: money/NNP market/NNP deposits/NNP a/DT 6/CD\n",
            " \n",
            "labeled  : participants/NNS will/MD include/VB the/DT u/NNP s/NNP australia/NNP canada/NNP japan/NNP south/NNP korea/CC and/NNP new/NNP zealand/RB as/RB well/IN as/DT the/CD six/NNS members/IN of/DT the/NNP association/IN of/NNP southeast/NNP asian/NNPS nations/NNP thailand/NNP malaysia/NNP singapore/NNP indonesia/DT the/NNPS philippines/CC and/NNP\n",
            "predicted: participants/NNS will/MD include/VB the/DT u/NNP s/NNP australia/NNP canada/NNP japan/NNP south/NNP korea/NNP and/NNP new/NNP zealand/RB as/RB well/IN as/DT the/NNS six/NNS members/IN of/DT the/NNP association/IN of/JJ southeast/JJ asian/NNP nations/NNP thailand/NNP malaysia/NNP singapore/NNP indonesia/NNP the/NNP philippines/CC and/NNP\n",
            " \n",
            "labeled  : by/IN 1987/CD then/JJ speaker/NNP jim/NNP wright/VBD was/VBG discussing/NNS arms/NN control/IN in/NNP moscow/IN with/NNP mikhail/NNP gorbachev/CC and/RB then/VBG attempting/NONE 1/TO to/VB direct/DT the/NN president/IN through/DT an/NNS appropriations/NN rider/NONE 2/TO to/VB treat/DT the/NNPS soviets/IN as/IN though/DT the/NNP senate/VBD had/VBN ratified/NNP salt/NNP\n",
            "predicted: by/IN 1987/CD then/RB speaker/NNP jim/NNP wright/VBD was/JJ discussing/NNS arms/NN control/IN in/NNP moscow/NNP with/NNP mikhail/NNP gorbachev/CC and/RB then/RB attempting/NONE 1/TO to/VB direct/DT the/NN president/IN through/DT an/NNS appropriations/NONE rider/NONE 2/TO to/VB treat/DT the/NNS soviets/IN as/IN though/DT the/NNP senate/VBD had/VBN ratified/NNP salt/NNP\n",
            " \n",
            "labeled  : this/DT conforms/VBZ to/TO the/DT soft/JJ landing/NN '/'' scenario/NN ''/'' said/VBD t/NONE 1/NNP elliott/NNP platt/DT an/NN economist/IN at/NNP donaldson/NNP lufkin/CC jenrette/NNP securities/NNP corp/NNP\n",
            "predicted: this/DT conforms/VBZ to/TO the/DT soft/JJ landing/NN '/'' scenario/NN ''/'' said/VBD t/NONE 1/NNP elliott/NNP platt/DT an/NN economist/IN at/NNP donaldson/NNP lufkin/CC jenrette/NNP securities/NNP corp/NNP\n",
            " \n",
            "labeled  : waertsilae/NNP marine/NNP 's/POS biggest/JJS creditor/NN is/VBZ miami/JJ based/NNP carnival/NNP cruise/NNPS lines/NNP\n",
            "predicted: waertsilae/NNP marine/NNP 's/POS biggest/JJS creditor/NN is/VBZ miami/JJ based/NNP carnival/NNP cruise/NNP lines/NNP\n",
            " \n",
            "labeled  : it/PRP also/RB would/MD junk/VB an/DT UNK/JJ market/JJ based/NN system/IN for/NONE trading/VBG emissions/NNS credits/NNS among/IN UNK/NNS\n",
            "predicted: it/PRP also/RB would/MD junk/VB an/DT UNK/JJ market/NN based/NN system/IN for/NN trading/NNS emissions/NNS credits/NNS among/IN\n",
            " \n",
            "labeled  : mr/NNP klauser/NNP says/VBZ 0/NONE mitsui/NNP has/VBZ 75/CD u/NNP s/NNS subsidiaries/IN in/WDT which/PRP it/VBZ holds/CD 35/NN interest/NN or/CC more/JJR t/NONE 1/CC and/DT the/NN trading/NN company/VBZ hopes/NONE 2/TO to/VB double/DT the/NN number/IN of/PRP its/NNP u/NNS s/IN affiliates/CD\n",
            "predicted: mr/NNP klauser/NNP says/VBZ 0/NONE mitsui/NNP has/VBZ 75/NNP u/NNP s/IN subsidiaries/IN in/NONE which/PRP it/VBZ holds/CD 35/NN interest/NN or/CC more/NONE t/CC 1/CC and/DT the/NN trading/NN company/VBZ hopes/NONE 2/TO to/VB double/DT the/NN number/IN of/PRP its/IN u/NNS s/IN affiliates/IN\n",
            " \n",
            "labeled  : both/DT sides/NNS are/VBP in/IN talks/NNS 0/NONE to/NONE settle/TO the/VB dispute/DT t/NN 1/NONE\n",
            "predicted: both/DT sides/NNS are/VBP in/IN talks/NNS 0/NONE to/TO settle/VB the/VB dispute/DT t/NN 1/NONE\n",
            " \n",
            "labeled  : but/CC equitable/NNP said/VBD 0/NONE it/PRP was/VBD unable/JJ 1/NONE to/TO find/VB a/DT buyer/NN willing/JJ to/NONE pay/TO what/VB it/WP considers/PRP t/VBZ 2/NONE fair/JJ value/NN ''/'' for/IN younkers/NNP because/IN of/IN recent/JJ turmoil/NN in/IN the/DT bond/NN and/CC stock/NN markets/NNS and/CC in/IN retailing/NN\n",
            "predicted: but/CC equitable/NNP said/VBD 0/NONE it/PRP was/VBD unable/JJ 1/NONE to/TO find/VB a/DT buyer/NN willing/JJ to/NONE pay/TO what/VB it/PRP considers/PRP t/NONE 2/NONE fair/JJ value/NN ''/'' for/IN younkers/IN because/IN of/IN recent/JJ turmoil/NN in/IN the/DT bond/NN and/CC stock/CC markets/NNS and/CC in/IN retailing/NN\n",
            " \n",
            "labeled  : in/IN a/DT monthly/JJ report/NN prepared/VBN for/NONE use/IN at/NN the/IN fed/DT 's/NNP next/POS federal/JJ open/NNP market/NNP committee/NNP meeting/NNP on/NN nov/IN 14/NNP the/CD nation/DT 's/NN central/POS bank/JJ found/NN that/VBD price/IN increases/NN have/NNS moderated/VBP and/VBN economic/CC activity/JJ has/NN grown/VBZ at/VBN a/IN sluggish/DT pace/JJ in/NN recent/IN weeks/JJ PAD/NNS\n",
            "predicted: in/IN a/DT monthly/JJ report/NN prepared/VBN for/IN use/IN at/IN the/IN fed/NNP 's/POS next/JJ federal/JJ open/JJ market/NN committee/NNP meeting/NNP on/IN nov/NNP 14/DT the/DT nation/NN 's/POS central/NN bank/NN found/VBD that/IN price/NNS increases/NNS have/VBP moderated/VBP and/CC economic/JJ activity/NN has/VBZ grown/VBN at/IN a/IN sluggish/JJ pace/JJ in/IN recent/JJ weeks/NNS PAD/NNS\n",
            " \n",
            "labeled  : biscayne/NNP securities/NNP corp/NNP of/IN lauderhill/NNP fla/NNP and/CC a/DT principal/NN of/IN the/DT firm/NN alvin/NNP rosenblum/NNP of/IN plantation/NNP fla/NNP were/VBD jointly/RB fined/VBN 1/NONE 20/CD 000/NONE u/CC and/VBN given/NONE 1/JJ 10/NNS day/IN suspensions/NONE for/RB allegedly/VBG selling/NNS securities/IN at/JJ unfair/NNS\n",
            "predicted: biscayne/NNP securities/NNP corp/NNP of/IN lauderhill/NNP fla/NNP and/CC a/DT principal/NN of/IN the/DT firm/NN alvin/NNP rosenblum/NNP of/IN plantation/NNP fla/NNP were/VBD jointly/RB fined/VBN 1/NONE 20/CD 000/NONE u/CC and/NONE given/NONE 1/CD 10/JJ day/IN suspensions/IN for/RB allegedly/VBG selling/NNS securities/IN at/JJ unfair/NNS\n",
            " \n",
            "labeled  : mr/NNP nixon/NNP also/RB proposed/VBD that/IN china/NNP restore/VB its/PRP participation/NN in/IN the/DT fulbright/NNP program/NNP a/DT u/NNP s/JJ government/JJ funded/NN\n",
            "predicted: mr/NNP nixon/NNP also/RB proposed/VBD that/IN china/NNP restore/VB its/PRP participation/NN in/IN the/DT fulbright/NNP program/NNP a/DT u/NNP s/NN government/JJ funded/JJ\n",
            " \n",
            "labeled  : the/DT minimum/NN wage/NN bill/VBD worked/RP out/NONE by/IN congress/NNP and/CC bush/NNP won/VBD easy/JJ approval/NN in/IN the/DT house/NNP\n",
            "predicted: the/DT minimum/JJ wage/NN bill/NN worked/VBD out/IN by/IN congress/NNP and/NNP bush/NNP won/VBD easy/JJ approval/NN in/IN the/DT house/NNP\n",
            " \n",
            "labeled  : west/NNP texas/NNP intermediate/NNP crude/JJ for/IN december/NNP delivery/NN rose/VBD 13/CD cents/NNS a/DT barrel/NN 1/NONE to/TO settle/VB at/IN 20/CD 07/NONE\n",
            "predicted: west/NNP texas/NNP intermediate/NNP crude/JJ for/IN december/NNP delivery/NN rose/VBD 13/CD cents/NNS a/DT barrel/NN 1/NONE to/TO settle/VB at/IN 20/CD 07/NN u/NONE\n",
            " \n",
            "labeled  : domestic/JJ sales/NNS of/IN construction/NN machinery/NN such/JJ as/IN power/NN shovels/NNS and/CC UNK/NNS rose/VBD to/TO 142/CD 84/CD billion/NN yen/IN from/CD 126/CD 15/NN\n",
            "predicted: domestic/JJ sales/NNS of/IN construction/NN machinery/NN such/JJ as/IN power/NN shovels/NNS and/CC UNK/VBD rose/VBD to/TO 142/CD 84/CD billion/CD yen/IN from/CD 126/CD 15/CD\n",
            " \n",
            "labeled  : institutions/NNS mostly/RB remained/VBD on/IN the/DT sidelines/NNS because/IN of/IN uncertainty/NN regarding/VBG interest/NN rates/NNS and/CC the/DT dollar/NN\n",
            "predicted: institutions/NNS mostly/RB remained/VBD on/IN the/DT sidelines/NNS because/IN of/IN uncertainty/NN regarding/VBG interest/NN rates/NNS and/CC the/DT dollar/NN\n",
            " \n",
            "labeled  : we/PRP thought/VBD 0/NONE it/PRP was/VBD awfully/RB expensive/JJ ''/'' said/VBD t/NONE 2/NNP sterling/NNP pratt/NN wine/NN director/IN at/NNP schaefer/POS 's/IN in/NNP skokie/NNP ill/CD one/IN of/DT the/JJ top/NNS stores/IN in/JJ suburban/NNP chicago/CC but/EX there/VBP are/NNS people/IN out/RB there/IN with/RB very/JJ different/NNS opinions/IN of/NN\n",
            "predicted: we/PRP thought/VBD 0/NONE it/PRP was/VBD awfully/RB expensive/JJ ''/'' said/VBD t/NONE 2/NNP sterling/NNP pratt/NN wine/NN director/IN at/NNP schaefer/POS 's/IN in/NNP skokie/NNP ill/CD one/IN of/DT the/JJ top/NNS stores/IN in/JJ suburban/NNP chicago/CC but/NNS there/VBP are/NNS people/NNS out/RB there/IN with/JJ very/JJ different/NNS opinions/IN of/NN\n",
            " \n",
            "labeled  : sales/NNS fell/VBD to/TO 251/CD 2/CD million/NONE u/IN from/CD 278/CD 7/NONE\n",
            "predicted: sales/NNS fell/VBD to/TO 251/CD 2/CD million/CD u/IN from/CD 278/CD 7/CD\n",
            " \n",
            "labeled  : mitsui/NNP mining/NNP smelting/CC co/NNP posted/NNP a/VBD 62/DT rise/CD in/NN pretax/NN profit/IN to/JJ 5/NN 276/TO billion/CD yen/CD lrb/NN 36/LRB 9/CD million/CD u/NONE rrb/RRB in/IN its/PRP fiscal/JJ first/JJ half/NN ended/VBD sept/JJ 30/CD compared/VBN with/IN 3/CD 253/CD billion/NN yen/DT a/NN year/RBR\n",
            "predicted: mitsui/NNP mining/NNP smelting/NNP co/NNP posted/DT a/DT 62/CD rise/IN in/NN pretax/NN profit/NN to/TO 5/TO 276/TO billion/CD yen/CD lrb/CD 36/CD 9/CD million/NONE u/NONE rrb/IN in/PRP its/JJ fiscal/JJ first/JJ half/VBD ended/VBD sept/CD 30/VBN compared/VBN with/CD 3/CD 253/CD billion/NN yen/NN a/NN year/NN\n",
            " \n",
            "labeled  : preston/NNP g/NNP foster/NNP birmingham/NNP ala/NNP\n",
            "predicted: preston/NNP g/NNP foster/NNP birmingham/NNP ala/NNP\n",
            " \n",
            "labeled  : reasons/NNS for/IN the/DT UNK/NN the/DT analyst/NN said/VBD 0/NONE t/NONE 1/VBD included/DT a/NN number/IN of/JJ UNK/NNS issues/JJ such/IN as/DT a/NN right/NONE to/TO strike/VB\n",
            "predicted: reasons/NNS for/IN the/DT UNK/NN the/DT analyst/NN said/VBD 0/NONE t/NONE 1/VBD included/DT a/NN number/IN of/NNS UNK/NNS issues/JJ such/IN as/DT a/NN right/NN to/NN\n",
            " \n",
            "labeled  : cray/NNP computer/NNP has/VBZ applied/VBN 1/NONE to/TO trade/VB on/IN nasdaq/NNP\n",
            "predicted: cray/NNP computer/NNP has/VBZ applied/VBN 1/NONE to/TO trade/VB on/IN nasdaq/NNP\n",
            " \n",
            "labeled  : because/IN many/JJ of/IN these/DT subskills/NNS the/JJ symmetry/NN of/IN geometrical/JJ figures/NNS metric/JJ measurement/NN of/IN volume/NN or/CC pie/NN and/CC bar/NN graphs/NNS for/IN example/NN are/NN only/RB a/DT small/JJ part/NN of/IN the/DT total/JJ fifth/NN grade/NN curriculum/NNP mr/NNP kaminski/VBZ says/NONE 0/NONE t/DT 1/NN the/NNS preparation/MD kits/RB would/VB n't/RB replicate/JJ too/IN many/PRP if/JJ their/NN real/VBD intent/JJ was/NN general/CC instruction/RB or/JJ even/NN general/IN familiarization/NN with/NNS\n",
            "predicted: because/IN many/JJ of/IN these/DT subskills/NNS the/JJ symmetry/NN of/IN geometrical/JJ figures/NNS metric/JJ measurement/NN of/IN volume/NN or/CC pie/NN and/CC bar/NN graphs/NNS for/IN example/NN are/VBP only/RB a/DT small/JJ part/NN of/IN the/DT total/JJ fifth/NN grade/NN curriculum/NNP mr/NNP kaminski/VBZ says/NONE 0/NONE t/DT 1/DT the/NNS preparation/MD kits/RB would/RB n't/RB replicate/JJ too/JJ many/IN if/PRP their/JJ real/NN intent/VBD was/JJ general/NN instruction/RB or/RB even/JJ general/IN familiarization/IN with/NNS\n",
            " \n",
            "labeled  : i/PRP say/VBP contained/NONE dialogue/VBD ''/NN because/'' sidewalk/IN stories/NNP ''/NNP is/'' n't/VBZ really/RB silent/RB at/JJ all/IN PAD/DT\n",
            "predicted: i/PRP say/VBP contained/VBN dialogue/NN ''/'' because/IN sidewalk/NNP stories/NNP ''/'' is/VBZ n't/RB really/RB silent/RB at/IN all/DT\n",
            " \n",
            "labeled  : stock/NN index/NN arbitrage/NONE buying/VBG or/CC selling/VBG baskets/NNS of/IN stocks/NNS while/IN at/IN the/DT same/JJ time/NN 1/NONE executing/VBG offsetting/VBG trades/NNS in/IN stock/NN index/NNS futures/CC or/NNS\n",
            "predicted: stock/NN index/NN arbitrage/NN buying/VBG or/CC selling/VBG baskets/NNS of/IN stocks/IN while/IN at/IN the/DT same/JJ time/NN 1/NONE executing/NONE offsetting/VBG trades/NNS in/IN stock/NNS index/NNS futures/NNS or/NNS options/NNS\n",
            " \n",
            "labeled  : rudolph/NNP agnew/NNP 55/CD years/NNS old/JJ and/CC former/JJ chairman/NN of/IN consolidated/NNP gold/NNP fields/NNP plc/NNP was/VBD named/VBN 1/NONE a/DT nonexecutive/JJ director/NN of/IN this/DT british/JJ industrial/JJ conglomerate/NN\n",
            "predicted: rudolph/NNP agnew/NNP 55/CD years/NNS old/JJ and/CC former/JJ chairman/NN of/IN consolidated/NNP gold/NNP fields/NNP plc/NNP was/VBD named/VBN 1/NONE a/DT nonexecutive/JJ director/NN of/IN this/DT british/JJ industrial/JJ conglomerate/NN\n",
            " \n",
            "labeled  : campbell/NNP soup/NNP forced/VBD out/RP its/PRP president/NN and/CC chief/NN executive/NN r/NNP gordon/NNP mcgovern/NNP the/DT strongest/JJS indication/NN ich/NONE 1/RB yet/IN that/DT the/NNP dorrance/NN family/VBZ plans/NONE to/TO take/VB charge/NN of/IN reshaping/NONE the/VBG troubled/DT food/JJ company/NN PAD/NN\n",
            "predicted: campbell/NNP soup/NNP forced/VBD out/RP its/PRP president/NN and/CC chief/NN executive/NN r/NNP gordon/NNP mcgovern/NNP the/DT strongest/JJS indication/NN ich/NONE 1/RB yet/IN that/DT the/NNP dorrance/NNP family/NN plans/VBZ to/TO take/VB charge/VB of/IN reshaping/NONE the/DT troubled/DT food/NN company/NN\n",
            " \n",
            "labeled  : they/PRP wo/MD n't/RB buy/VB if/IN the/DT quality/NN is/VBZ not/RB there/RB ''/'' said/VBD t/NONE 1/NNP cedric/NNP martin/IN of/NNP martin/NNP wine/NNP cellar/IN in/NNP new/NNP\n",
            "predicted: they/PRP wo/MD n't/RB buy/VB if/IN the/DT quality/NN is/VBZ not/RB there/EX ''/'' said/VBD t/NONE 1/NNP cedric/NNP martin/IN of/NNP martin/NNP wine/NNP cellar/IN in/NNP new/NNP\n",
            " \n",
            "labeled  : it/PRP exp/NONE 1/VBZ remains/JJ unclear/IN whether/DT the/NN bond/NN issue/MD will/VB be/VBN rolled/NONE 2/RP\n",
            "predicted: it/PRP exp/NONE 1/VBZ remains/JJ unclear/IN whether/DT the/NN bond/NN issue/MD will/VB be/NONE rolled/NONE 2/IN\n",
            " \n",
            "labeled  : the/DT following/VBG issues/NNS were/VBD recently/RB filed/VBN 1/NONE with/IN the/DT securities/NNPS and/CC exchange/NNP commission/NNP\n",
            "predicted: the/DT following/VBG issues/NNS were/VBD recently/RB filed/VBN 1/NONE with/IN the/DT securities/NNP and/CC exchange/NNP commission/NNP\n",
            " \n",
            "labeled  : columbia/NNP wo/MD n't/RB comment/VB on/IN all/PDT the/DT speculation/NN\n",
            "predicted: columbia/NNP wo/MD n't/RB comment/VB on/IN all/DT the/DT speculation/NN\n",
            " \n",
            "labeled  : late/RB yesterday/NN georgia/NNP gulf/NNP said/VBD 0/NONE it/PRP reviewed/VBD the/DT nl/NNP proposal/NN as/RB well/RB as/IN interests/NNS from/IN third/JJ parties/NNS ''/'' regarding/VBG business/NN combinations/NNS\n",
            "predicted: late/NNP yesterday/NN georgia/NNP gulf/NNP said/VBD 0/NONE it/PRP reviewed/VBD the/DT nl/NNP proposal/NN as/IN well/RB as/IN interests/NNS from/IN third/JJ parties/NNS ''/'' regarding/VBG business/NN combinations/NNS\n",
            " \n",
            "labeled  : what/WP this/DT tells/VBZ us/PRP t/NONE 28/VBZ is/DT that/NNP u/NN s/NN trade/VBZ law/VBG is/'' working/PRP ''/VBD he/NONE\n",
            "predicted: what/WP this/DT tells/VBZ us/PRP t/NONE 28/VBZ is/IN that/DT u/NNP s/NN trade/NN law/VBZ is/VBZ working/'' ''/PRP he/VBD\n",
            " \n",
            "labeled  : first/JJ they/PRP are/VBP designed/VBN 1/NONE to/TO eliminate/VB the/DT risk/NN of/IN prepayment/NN mortgage/JJ backed/NNS securities/MD can/VB be/VBN retired/NONE 43/RB early/IN if/NN interest/NNS rates/VBP decline/CC and/JJ such/NN prepayment/VBZ forces/NNS investors/TO to/VB redeploy/PRP their/NN money/IN at/JJR lower/NNS\n",
            "predicted: first/RB they/PRP are/VBP designed/VBN 1/NONE to/TO eliminate/VB the/DT risk/NN of/IN prepayment/NN mortgage/JJ backed/NNS securities/MD can/VB be/VBN retired/VBN 43/RB early/IN if/NN interest/NNS rates/NNS decline/CC and/JJ such/NN prepayment/NNS forces/NNS investors/TO to/VB redeploy/PRP their/NN money/IN at/NNS lower/NNS\n",
            " \n",
            "labeled  : in/IN japan/NNP the/DT benchmark/NN no/NN 111/CD 4/CD 6/NN issue/NN due/JJ UNK/CD ended/VBD on/IN brokers/NNS screens/NNS unchanged/JJ at/IN 95/CD 09/NONE 1/TO to/VB yield/CD 5/NN\n",
            "predicted: in/IN japan/NNP the/DT benchmark/NN no/DT 111/CD 4/CD 6/NN issue/NN due/JJ UNK/VBD ended/IN on/NNS brokers/NNS screens/NNS unchanged/IN at/CD 95/CD 09/CD 1/TO to/VB yield/CD\n",
            " \n",
            "labeled  : terms/NNS were/VBD n't/RB disclosed/VBN 1/NONE\n",
            "predicted: terms/NNS were/VBD n't/RB disclosed/VBN 1/NONE\n",
            " \n",
            "labeled  : but/CC in/IN london/NNP and/CC tokyo/NNP where/WRB computer/JJ driven/NN trading/RB now/VBZ plays/DT a/JJ small/CC but/VBG growing/NN role/NONE t/NNS 1/VBP traders/NONE say/DT 0/NN a/IN number/NNS of/VBP\n",
            "predicted: but/CC in/IN london/NNP and/CC tokyo/NNP where/WRB computer/JJ driven/NN trading/NN now/RB plays/DT a/JJ small/CC but/CC growing/VBG role/NONE t/NONE 1/NNS traders/VBP say/DT 0/DT a/NN number/NNS of/NNS\n",
            " \n",
            "labeled  : under/IN the/DT stars/NNS and/CC moons/NNS of/IN the/DT renovated/VBN indiana/NNP roof/NNP ballroom/NN nine/CD of/IN the/DT hottest/JJS chefs/NNS in/IN town/NN fed/VBD them/PRP indiana/NNP duckling/NN mousseline/NN lobster/NN consomme/NN veal/NN mignon/NN and/CC chocolate/JJ terrine/NN with/IN a/DT raspberry/NN sauce/NN\n",
            "predicted: under/IN the/DT stars/NNS and/CC moons/NNS of/IN the/DT renovated/VBN indiana/NNP roof/NNP ballroom/NN nine/CD of/IN the/DT hottest/JJS chefs/NNS in/IN town/NN fed/VBD them/PRP indiana/NNP duckling/NN mousseline/NN lobster/NN consomme/NN veal/NN mignon/NN and/CC chocolate/JJ terrine/NN with/IN a/DT raspberry/NN sauce/NN\n",
            " \n",
            "labeled  : london/NNP share/NN prices/NNS were/VBD bolstered/VBN 1/NONE largely/RB by/IN continued/VBN gains/NNS on/IN wall/NNP street/NNP and/CC technical/JJ factors/NNS UNK/VBG demand/NN for/IN london/NNP 's/POS blue/JJ chip/NNS\n",
            "predicted: london/NNP share/NN prices/NNS were/VBD bolstered/VBN 1/NONE largely/RB by/IN continued/VBN gains/NNS on/IN wall/NNP street/NNP and/CC technical/JJ factors/NNS UNK/VBZ demand/NN for/IN london/NNP 's/POS blue/JJ chip/NNS stocks/NNS\n",
            " \n",
            "labeled  : 10/CD billion/CD u/NONE of/IN 30/JJ year/NNS bonds/NONE 0/NONE t/TO 1/VB to/VBN be/NONE auctioned/NNP 110/CC thursday/TO and/VB to/NNP mature/CD aug/CD\n",
            "predicted: 10/CD billion/CD u/NONE of/IN 30/JJ year/NNS bonds/NONE 0/NONE t/NONE 1/NONE to/VB be/NONE auctioned/NNP 110/CC thursday/CC and/TO to/NNP mature/CD aug/CD\n",
            " \n",
            "labeled  : maxwell/NNP r/NNP d/NNP vos/NNP brooklyn/NNP\n",
            "predicted: maxwell/NNP r/NNP d/NNP vos/NNP brooklyn/NNP\n",
            " \n",
            "labeled  : the/DT treasury/NNP said/VBD 0/NONE the/DT refunding/NN is/VBZ contingent/NN upon/IN congressional/JJ and/CC presidential/JJ passage/NN of/IN an/DT increase/NN in/IN the/DT federal/JJ debt/NN ceiling/NN\n",
            "predicted: the/DT treasury/NNP said/VBD 0/NONE the/DT refunding/NN is/VBZ contingent/NN upon/IN congressional/JJ and/CC presidential/JJ passage/NN of/IN an/DT increase/NN in/IN the/DT federal/JJ debt/NN ceiling/NN\n",
            " \n",
            "labeled  : in/IN the/DT u/NNP s/JJ over/NN the/JJ counter/NN market/NNS american/IN depositary/NNP shares/DT for/VBG reuters/CD each/NNS representing/IN three/DT shares/NNP in/NN the/VBD london/JJ market/IN closed/CD unchanged/NONE\n",
            "predicted: in/IN the/DT u/NNP s/NN over/JJ the/JJ counter/IN market/JJ american/IN depositary/NNS shares/IN for/NNP reuters/CD each/NNS representing/NNS three/NNS shares/IN in/DT the/NNP london/NNP market/VBD closed/IN unchanged/IN at/CD\n",
            " \n",
            "labeled  : the/DT california/NNP supreme/NNP court/NNP last/JJ year/NN UNK/VBD direction/NN 1/NONE to/TO make/VB it/PRP exp/NONE 2/RB much/JJR harder/NONE to/TO win/VB des/NNP cases/NNS because/IN the/DT justices/NNS saw/VBD how/WRB all/PDT the/DT pharmaceutical/JJ litigation/NN has/VBZ UNK/VBN the/DT introduction/NN of/IN new/JJ drugs/NNS t/NONE\n",
            "predicted: the/DT california/NNP supreme/NNP court/NNP last/JJ year/NN UNK/NN direction/NN 1/NONE to/TO make/VB it/PRP exp/NONE 2/RB much/RB harder/NONE to/TO win/VB des/NNP cases/NNS because/IN the/DT justices/NNS saw/VBD how/WRB all/DT the/DT pharmaceutical/JJ litigation/NN has/VBZ UNK/VBN the/DT introduction/NN of/IN new/JJ drugs/NNS t/NONE\n",
            " \n",
            "labeled  : in/IN a/DT letter/NN to/TO georgia/NNP gulf/NNP president/NNP jerry/NNP r/NNP satrum/NNP mr/NNP martin/NNP asked/VBD georgia/NNP gulf/NNP 1/NONE to/TO answer/VB its/PRP offer/NN by/IN tuesday/NNP\n",
            "predicted: in/IN a/DT letter/NN to/TO georgia/NNP gulf/NNP president/NNP jerry/NNP r/NNP satrum/NNP mr/NNP martin/NNP asked/VBD georgia/NNP gulf/NNP 1/NONE to/TO answer/VB its/PRP offer/NN by/IN tuesday/NNP\n",
            " \n",
            "labeled  : meanwhile/RB the/DT national/NNP association/NNP of/IN purchasing/NNP management/NNP said/VBD 0/NONE its/PRP latest/JJS survey/NN indicated/VBD that/IN the/DT manufacturing/NN economy/NN contracted/VBD in/IN october/NNP for/IN the/DT sixth/JJ consecutive/JJ month/NN\n",
            "predicted: meanwhile/RB the/DT national/NNP association/NNP of/IN purchasing/NNP management/NNP said/VBD 0/NONE its/PRP latest/JJS survey/NN indicated/VBD that/DT the/DT manufacturing/NN economy/NN contracted/VBD in/IN october/IN for/IN the/DT sixth/JJ consecutive/JJ month/NN\n",
            " \n",
            "labeled  : program/NN trading/NN critics/NNS also/RB want/VBP the/DT federal/NNP reserve/NNP board/NNP rather/RB than/IN the/DT futures/NNS industry/NN to/TO set/VB such/JJ margins/NNS\n",
            "predicted: program/NN trading/NN critics/NNS also/RB want/VBP the/DT federal/NNP reserve/NNP board/NNP rather/RB than/IN the/DT futures/NNS industry/NN to/TO set/VB such/JJ margins/NNS\n",
            " \n",
            "labeled  : dd/NNP acquisition/NNP corp/NNP a/DT partnership/NN of/IN unicorp/NNP canada/NNP corp/NNP 's/POS UNK/NNP capital/NNP group/NNP and/CC cara/NNP operations/NNPS ltd/NNP extended/VBD to/TO nov/NNP 20/CD its/PRP 45/JJ a/NN share/IN offer/DT for/NNP all/NNPS dunkin'/NNP UNK/NNS inc/JJ\n",
            "predicted: dd/NNP acquisition/NNP corp/NNP a/DT partnership/NN of/IN unicorp/NNP canada/NNP corp/NNP 's/POS UNK/NNP capital/NN group/NNP and/CC cara/NNP operations/NNP ltd/NNP extended/VBD to/NNP nov/CD 20/CD its/CD 45/DT a/NN share/NN offer/IN for/NNP all/NNP dunkin'/NNP UNK/NNP inc/NNS shares/JJ\n",
            " \n",
            "labeled  : shearson/NNP is/VBZ 62/JJ owned/IN by/NNP american/NNP express/NNP\n",
            "predicted: shearson/NNP is/VBZ 62/VBN owned/VBN by/IN american/NNP express/NNP co/NNP\n",
            " \n",
            "labeled  : diaper/NN services/NNS make/VBP a/DT comeback/NN amid/IN growing/VBG environmental/JJ concerns/NNS\n",
            "predicted: diaper/NN services/NNS make/VBP a/DT comeback/NN amid/IN growing/VBG environmental/JJ concerns/NNS\n",
            " \n",
            "labeled  : new/JJ managers/NNS would/MD think/VB a/DT little/RB more/JJR like/IN wall/NNP street/NNP ''/'' mr/NNP mcmillin/NNP added/VBD t/NONE\n",
            "predicted: new/JJ managers/NNS would/MD think/VB a/DT little/RB more/JJR like/IN wall/NNP street/NNP ''/'' mr/NNP mcmillin/NNP added/VBD t/NONE\n",
            " \n",
            "labeled  : in/IN september/NNP the/DT company/NN said/VBD 0/NONE it/PRP was/VBD seeking/VBG offers/NNS for/IN its/PRP five/CD radio/NN stations/NNS in/IN order/NN 1/NONE to/TO concentrate/VB on/IN its/PRP programming/NN business/NN\n",
            "predicted: in/IN september/NNP the/DT company/NN said/VBD 0/NONE it/PRP was/VBD seeking/VBG offers/NNS for/IN its/PRP five/CD radio/NN stations/NNS in/IN order/NN 1/NONE to/TO concentrate/VB on/IN its/PRP programming/NN business/NN\n",
            " \n",
            "labeled  : under/IN terms/NNS of/IN the/DT agreement/NN shareholders/NNS other/JJ than/IN the/DT underwoods/NNPS will/MD receive/VB 3/CD 500/NONE u/DT a/NN share/IN at/NN closing/WDT which/NONE t/VBZ 2/VBN is/NONE expected/IN 1/NNP\n",
            "predicted: under/IN terms/NNS of/IN the/DT agreement/NN shareholders/NNS other/JJ than/IN the/DT underwoods/NNP will/MD receive/VB 3/CD 500/CD u/DT a/NN share/IN at/NONE closing/NONE which/NONE t/NONE 2/VBZ is/VBN expected/NONE 1/IN\n",
            " \n",
            "labeled  : the/DT sec/NNP documents/NNS describe/VBP those/DT chips/NNS which/WDT t/NONE 1/VBP are/VBN made/NONE 22/IN of/NN gallium/NN arsenide/IN as/NONE 2/VBG being/RB so/JJ fragile/CC and/JJ minute/NONE 0/PRP they/MD will/VB require/JJ special/JJ robotic/NN handling/NN\n",
            "predicted: the/DT sec/NNP documents/NNS describe/VBP those/DT chips/NNS which/WDT t/NONE 1/VBP are/VBN made/NONE 22/IN of/NN gallium/NN arsenide/IN as/NONE 2/NONE being/VBG so/JJ fragile/CC and/NN minute/NONE 0/PRP they/MD will/VB require/JJ special/JJ robotic/NN handling/NN\n",
            " \n",
            "labeled  : she/PRP won/VBD grant/NN money/NN for/IN the/DT school/NN advised/VBD cheerleaders/NNS ran/VBD the/DT pep/NN club/NN proposed/VBD and/CC taught/VBD a/DT new/JJ cultural/JJ literacy/NN ''/'' class/NN in/IN western/JJ civilization/NN and/CC was/VBD chosen/VBN 1/NONE by/IN the/DT school/NN pta/NNP as/IN teacher/NN of/IN the/DT year/NN ''/''\n",
            "predicted: she/PRP won/VBD grant/NN money/NN for/IN the/DT school/NN advised/VBD cheerleaders/NNS ran/VBD the/DT pep/NN club/NN proposed/VBD and/CC taught/VBD a/DT new/JJ cultural/JJ literacy/NN ''/'' class/NN in/IN western/JJ civilization/NN and/CC was/VBD chosen/VBN 1/NONE by/IN the/DT school/NN pta/NNP as/IN teacher/DT of/IN the/DT year/NN ''/''\n",
            " \n",
            "labeled  : it/PRP said/VBD 0/NONE it/PRP needs/VBZ 1/NONE to/TO make/VB the/DT payment/NN by/IN dec/NNP 1/CD 1/NONE to/TO avoid/VB a/DT default/NN that/WDT t/NONE 2/MD could/VB lead/TO to/DT an/NN acceleration/IN of/DT the/NN\n",
            "predicted: it/PRP said/VBD 0/NONE it/PRP needs/VBZ 1/NONE to/TO make/VB the/DT payment/DT by/IN dec/NNP 1/NONE 1/NONE to/TO avoid/VB a/DT default/DT that/WDT t/NONE 2/MD could/VB lead/TO to/DT an/NN acceleration/IN of/DT the/NN\n",
            " \n",
            "labeled  : the/DT group/NN says/VBZ 0/NONE standardized/JJ achievement/NN test/NN scores/NNS are/VBP greatly/RB inflated/VBN because/IN teachers/NNS often/RB teach/VBP the/DT test/NN ''/'' as/IN mrs/NNP yeargin/NNP did/VBD although/NONE most/IN are/JJS never/VBP caught/RB 1/VBN PAD/NONE\n",
            "predicted: the/DT group/NN says/VBZ 0/NONE standardized/JJ achievement/NN test/NN scores/NNS are/VBP greatly/RB inflated/VBN because/IN teachers/NNS often/RB teach/VBP the/DT test/NN ''/'' as/IN mrs/NNP yeargin/NNP did/VBD although/IN most/JJS are/VBP never/RB caught/VBN 1/NONE PAD/NONE\n",
            " \n",
            "labeled  : w/NNP n/NNP whelen/CC co/NNP of/IN georgetown/NNP del/NNP and/CC its/PRP president/NN william/NNP n/NNP whelen/NNP jr/NNP also/RB of/IN georgetown/NNP were/VBD barred/VBN 1/NONE from/IN 2/NONE transacting/VBG principal/JJ trades/NNS for/IN 90/CD days/NNS and/CC were/VBD jointly/RB fined/VBN 1/NONE 15/CD 000/NONE\n",
            "predicted: w/NNP n/NNP whelen/NNP co/NNP of/IN georgetown/NNP del/NNP and/CC its/PRP president/NN william/NNP n/NNP whelen/NNP jr/NNP also/RB of/IN georgetown/NNP were/VBD barred/VBN 1/NONE from/IN 2/NONE transacting/VBG principal/JJ trades/NNS for/IN 90/CD days/NNS and/CC were/VBD jointly/RB fined/VBN 1/NONE 15/CD 000/NONE\n",
            " \n",
            "labeled  : in/IN this/DT instance/NN industry/NN observers/NNS say/VBP 0/NONE t/NONE 1/PRP he/VBZ is/VBG entering/JJ uncharted/NNS\n",
            "predicted: in/IN this/DT instance/NN industry/NN observers/NNS say/VBP 0/NONE t/NONE 1/PRP he/VBZ is/VBG entering/JJ uncharted/NNS\n",
            " \n",
            "labeled  : workers/NNS dumped/VBD large/JJ burlap/NN sacks/NNS of/IN the/DT imported/VBN material/NN into/IN a/DT huge/JJ bin/NN poured/VBD in/RP cotton/NN and/CC acetate/NN fibers/NNS and/CC mechanically/RB mixed/VBD the/DT dry/JJ fibers/NNS in/IN a/DT process/NN used/VBN to/NONE make/NONE filters/TO PAD/VB PAD/NNS\n",
            "predicted: workers/NNS dumped/VBD large/JJ burlap/NN sacks/NNS of/IN the/DT imported/VBN material/NN into/IN a/DT huge/JJ bin/NN poured/VBD in/IN cotton/NN and/CC acetate/NN fibers/NNS and/CC mechanically/RB mixed/VBD the/DT dry/JJ fibers/NNS in/IN a/DT process/NN used/VBN to/NONE make/TO filters/VB\n",
            " \n",
            "labeled  : but/CC it/PRP exp/NONE 1/VBZ is/RB n't/JJ clear/RB yet/IN whether/DT the/JJ central/NN bank/MD will/VB make/PDT such/DT a/NN\n",
            "predicted: but/CC it/PRP exp/NONE 1/VBZ is/RB n't/RB clear/RB yet/IN whether/DT the/JJ central/NN bank/MD will/VB make/VB such/DT a/NN\n",
            " \n",
            "labeled  : i/PRP sense/VBP that/IN some/DT people/NNS are/VBP UNK/JJ 2/NONE to/TO UNK/VB their/PRP UNK/NNS out/RP in/IN any/DT aggressive/JJ way/NN until/IN after/IN the/DT figures/NNS come/VBP out/IN ''/'' said/VBD t/NONE 1/NNP richard/NNP eakle/NN president/IN of/NNP eakle/NNPS associates/NNP fair/NNP\n",
            "predicted: i/PRP sense/VBP that/IN some/DT people/NNS are/VBP UNK/VBN 2/NONE to/TO UNK/VB their/PRP UNK/NNS out/IN in/IN any/DT aggressive/JJ way/NN until/IN after/IN the/DT figures/NNS come/VBP out/RB ''/'' said/VBD t/NONE 1/NNP richard/NNP eakle/NN president/IN of/NNP eakle/NNP associates/NNP fair/NNP\n",
            " \n",
            "labeled  : a/DT series/NN of/IN 5/CD 000/CC or/RB so/NNS changes/VBZ is/DT a/NN peal/'' ''/CC and/VBZ takes/RB about/CD three/NNS\n",
            "predicted: a/DT series/NN of/IN 5/CD 000/CC or/CC so/NNS changes/VBZ is/DT a/NN peal/'' ''/CC and/CC takes/IN about/CD three/NNS\n",
            " \n",
            "labeled  : they/PRP argue/VBP that/IN u/NNP s/NNS investors/RB often/MD can/VB buy/JJ american/JJ depositary/NNS receipts/IN on/DT the/JJ big/NNS stocks/IN in/JJ many/NNS funds/DT these/JJ so/NNPS called/VBP adrs/NNS represent/IN shares/JJ of/NNS foreign/VBN companies/NONE traded/IN in/DT the/NNP\n",
            "predicted: they/PRP argue/VBP that/IN u/NNP s/NNS investors/RB often/MD can/VB buy/JJ american/NNS depositary/NNS receipts/IN on/DT the/JJ big/NNS stocks/IN in/JJ many/NNS funds/DT these/JJ so/NNS called/NNS adrs/NNS represent/IN shares/IN of/JJ foreign/NNS companies/IN traded/IN in/DT\n",
            " \n",
            "labeled  : government/NN officials/NNS especially/RB in/IN japan/NNP probably/RB would/MD resist/VB any/DT onslaught/NN of/IN program/NN trading/NN by/IN players/NNS trying/VBG to/NONE shrug/TO off/VB the/RP u/DT s/NNP furor/NN over/IN their/PRP activities/NNS and/CC marching/VBG abroad/RB with/IN their/PRP business/NN\n",
            "predicted: government/NN officials/NNS especially/RB in/IN japan/NNP probably/RB would/MD resist/VB any/DT onslaught/NN of/IN program/NN trading/NN by/IN players/NNS trying/VBG to/NONE shrug/TO off/VB the/DT u/DT s/DT furor/NN over/IN their/PRP activities/NNS and/CC marching/VBG abroad/RB with/IN their/PRP business/NN\n",
            " \n",
            "labeled  : 1/NONE asked/VBD 2/NONE whether/IN the/DT bidding/NN flap/NN would/MD hurt/VB u/JJ s/NNS japan/NNP relations/NNP mr/VBD yamamoto/DT said/MD this/VB will/DT be/NN a/NN minus/''\n",
            "predicted: 1/NONE asked/VBD 2/NONE whether/IN the/DT bidding/NN flap/NN would/MD hurt/VB u/NNP s/NNP japan/NNP relations/NNP mr/NNP yamamoto/VBD said/MD this/MD will/VB be/DT a/NN minus/'' factor/''\n",
            " \n",
            "labeled  : he/PRP was/VBD on/IN the/DT board/NN of/IN an/DT insurance/NN company/NN with/IN financial/JJ problems/NNS but/CC he/PRP insists/VBZ 0/NONE he/PRP made/VBD no/DT secret/NN of/IN it/PRP\n",
            "predicted: he/PRP was/VBD on/IN the/DT board/NN of/IN an/DT insurance/NN company/NN with/IN financial/JJ problems/NNS but/CC he/PRP insists/VBZ 0/NONE he/PRP made/VBD no/DT secret/NN of/IN it/PRP\n",
            " \n",
            "labeled  : ralston/NNP attributed/VBD its/PRP fourth/NN quarter/NN slump/RB partly/TO to/JJR higher/NNS costs/IN of/NNS UNK/IN in/DT the/NN pet/NN food/NN business/RB as/RB well/IN as/JJ competitive/NNS pressures/WDT which/NONE t/VBD 1/JJR required/NN higher/NN\n",
            "predicted: ralston/NNP attributed/VBD its/PRP fourth/JJ quarter/NN slump/RB partly/RB to/TO higher/JJR costs/NNS of/IN UNK/IN in/IN the/DT pet/NN food/NN business/NN as/IN well/RB as/JJ competitive/JJ pressures/NNS which/WDT t/NONE 1/RB required/JJR higher/JJR advertising/NN\n",
            " \n",
            "labeled  : u/NNP s/NNS investors/MD should/VB have/DT a/JJR greater/NN opportunity/IN at/JJ direct/NN investment/'' ''/IN in/NNP\n",
            "predicted: u/NNP s/NNS investors/MD should/VB have/DT a/DT greater/NN opportunity/IN at/IN direct/NN investment/NN ''/IN in/NNP\n",
            " \n",
            "labeled  : they/PRP like/VBP 1/NONE to/TO talk/VB about/IN 2/NONE having/VBG the/DT new/JJ red/NNP rock/NNP terrace/NNP lcb/LRB one/CD of/IN diamond/NNP creek/NNP 's/POS cabernets/NNPS rcb/RRB or/CC the/DT dunn/NNP 1985/CD cabernet/NNP or/CC the/DT petrus/NNP\n",
            "predicted: they/PRP like/IN 1/NONE to/TO talk/VB about/IN 2/NONE having/VBG the/DT new/JJ red/NNP rock/NNP terrace/NNP lcb/LRB one/CD of/IN diamond/NNP creek/NNP 's/POS cabernets/NNPS rcb/LRB or/CC the/DT dunn/NNP 1985/CD cabernet/NNP or/CC the/DT petrus/NNP\n",
            " \n",
            "labeled  : in/IN a/DT few/JJ weeks/NNS many/JJ barges/NNS probably/RB wo/MD n't/RB be/VB able/JJ 2/NONE to/TO operate/VB fully/RB loaded/VBN south/RB of/IN st/NNP louis/NNP because/IN the/DT u/NNP s/NNP army/NNP corps/IN of/NNPS engineers/VBZ is/VBG beginning/NONE 3/TO to/VB reduce/DT the/NN flow/IN of/DT the/NNP missouri/NNP river/WDT which/NONE t/VBZ 1/IN UNK/DT into/NNP the/NNP\n",
            "predicted: in/IN a/DT few/JJ weeks/NNS many/JJ barges/NNS probably/RB wo/MD n't/RB be/VB able/JJ 2/NONE to/TO operate/VB fully/RB loaded/VBN south/RB of/IN st/NNP louis/NNP because/IN the/DT u/NNP s/NNP army/NNP corps/IN of/NNP engineers/VBZ is/NONE beginning/NONE 3/TO to/VB reduce/DT the/NN flow/IN of/NNP the/NNP missouri/NNP river/NONE which/NONE t/IN 1/IN UNK/IN into/NNP the/NNP mississippi/NNP\n",
            " \n",
            "labeled  : as/IN a/DT foster/NNP corporate/NNP parent/NNP you/PRP will/MD experience/VB the/DT same/JJ joy/NN felt/VBD by/NONE robert/IN bass/NNP lewis/NNP ranieri/NNP william/NNP simon/NNP and/NNP others/CC who/NNS t/WP 207/NONE find/VBP ways/NNS 0/NONE to/NONE help/TO troubled/VB savings/VBN institutions/NNS and/NNS their/CC employees/PRP help/NNS themselves/VB t/PRP 1/NONE\n",
            "predicted: as/IN a/DT foster/NNP corporate/NNP parent/NNP you/PRP will/MD experience/VB the/DT same/JJ joy/NN felt/VBD by/NONE robert/IN bass/NNP lewis/NNP ranieri/NNP william/NNP simon/NNP and/CC others/NNP who/WP t/NONE 207/NONE find/VBP ways/NNS 0/NONE to/TO help/VB troubled/VB savings/NNS institutions/NNS and/CC their/NNS employees/NNS help/PRP themselves/PRP t/NONE 1/NONE\n",
            " \n",
            "labeled  : program/NN trading/NN is/VBZ a/DT racket/NN ''/'' complains/VBZ 0/NONE t/NONE 1/NNP edward/NNP egnuss/DT a/NNP white/NNP plains/NNP n/NN y/CC investor/NNS and/NNS electronics/NN sales/CC executive/PRP and/VBZ it/RB 's/TO not/DT to/NN the/IN benefit/DT of/JJ the/NN small/DT investor/VBZ that/IN 's/RB for/''\n",
            "predicted: program/NN trading/NN is/VBZ a/DT racket/NN ''/'' complains/VBZ 0/NONE t/NONE 1/NNP edward/NNP egnuss/DT a/NNP white/NNP plains/NNP n/NNP y/CC investor/CC and/NNS electronics/NNS sales/NNS executive/CC and/PRP it/VBZ 's/RB not/TO to/DT the/NN benefit/IN of/DT the/JJ small/NN investor/IN that/NN 's/IN for/JJ\n",
            " \n",
            "labeled  : that/DT represents/VBZ a/DT very/RB thin/JJ excess/JJ ''/'' return/NN certainly/RB far/RB less/JJR than/IN what/WP most/RBS fundamental/JJ stock/NN pickers/NNS claim/VBP 1/NONE to/TO seek/VB t/NONE 2/IN as/PRP their/NN performance/NN\n",
            "predicted: that/DT represents/VBZ a/DT very/RB thin/JJ excess/JJ ''/'' return/NN certainly/RB far/RB less/JJR than/IN what/WP most/JJS fundamental/JJ stock/NN pickers/NNS claim/VBP 1/NONE to/TO seek/VB t/NONE 2/PRP as/PRP their/NN performance/NN\n",
            " \n",
            "labeled  : freeport/NNP mcmoran/NNP inc/VBD said/NONE 0/PRP it/MD will/VB UNK/PRP its/NNP freeport/NNP mcmoran/NNPS energy/NNP partners/NN ltd/IN partnership/DT into/RB a/VBN publicly/NN traded/IN company/DT through/NN the/IN exchange/NNS of/IN units/DT of/NN the/IN partnership/JJ for/NNS\n",
            "predicted: freeport/NNP mcmoran/NNP inc/NNP said/NONE 0/PRP it/PRP will/VB UNK/PRP its/NNP freeport/NNP mcmoran/NNP energy/NNP partners/NNP ltd/IN partnership/IN into/DT a/VBN publicly/VBN traded/IN company/IN through/DT the/NNP exchange/IN of/NNS units/IN of/DT the/NN partnership/IN for/JJ\n",
            " \n",
            "labeled  : also/RB more/JJR people/NNS said/VBD 0/NONE conditions/NNS will/MD UNK/VB in/IN the/DT period/NN\n",
            "predicted: also/RB more/JJR people/NNS said/VBD 0/NONE conditions/NNS will/MD UNK/VB in/IN the/DT period/NN\n",
            " \n",
            "labeled  : dr/NNP novello/NNP is/VBZ deputy/NN director/NN of/IN the/DT national/NNP institute/NNP of/IN child/NNP health/NNP and/CC human/NNP development/NNP\n",
            "predicted: dr/NNP novello/NNP is/VBZ deputy/NN director/NN of/IN the/DT national/NNP institute/NNP of/IN child/NNP health/NNP and/CC human/NNP development/NNP\n",
            " \n",
            "labeled  : robert/NNP j/NNP danzig/NNP vice/NN president/NN rnr/NONE 1/CC and/JJ general/NN manager/NONE rnr/IN 1/NNP of/NNP hearst/VBD newspapers/RP stood/IN up/DT in/NN the/POS paper/NN 's/NN newsroom/CC yesterday/VBD and/IN announced/DT that/NNS no/VBD buyers/VBN had/RB stepped/CC forward/IN and/DT that/NN the/MD paper/VB would/NONE fold/VBG 2/JJR putting/IN more/CD than/JJ 730/NNS full/IN time/IN employees/NN\n",
            "predicted: robert/NNP j/NNP danzig/NNP vice/NN president/NN rnr/NONE 1/CC and/JJ general/NN manager/NONE rnr/IN 1/IN of/NNP hearst/VBD newspapers/RB stood/IN up/IN in/DT the/NN paper/NN 's/NN newsroom/NN yesterday/NN and/VBD announced/DT that/DT no/NNS buyers/VBD had/VBD stepped/CC forward/CC and/DT that/DT the/NN paper/MD would/VB fold/IN 2/IN putting/IN more/IN than/JJ 730/JJ full/JJ time/IN employees/IN\n",
            " \n",
            "labeled  : diaper/NN shortages/NNS this/DT summer/NN limited/VBD growth/NN at/IN stork/NNP diaper/NNP services/NNPS springfield/NNP mass/NNP where/WRB business/NN is/VBZ up/IN 25/CD in/NN t/IN 1/NONE\n",
            "predicted: diaper/NN shortages/NNS this/DT summer/NN limited/VBD growth/NN at/IN stork/NNP diaper/NNP services/NNPS springfield/NNP mass/NNP where/WRB business/NN is/VBZ up/RB 25/CD in/IN t/NONE\n",
            " \n",
            "labeled  : too/RB much/JJ money/NN ich/NONE 1/VBZ is/IN at/NN stake/IN for/NN program/NNS traders/TO to/VB give/IN\n",
            "predicted: too/RB much/JJ money/NN ich/NONE 1/VBZ is/IN at/NN stake/IN for/NN program/NNS traders/TO to/VB give/VB\n",
            " \n",
            "labeled  : but/CC with/IN the/DT index/NN proving/VBG somewhat/RB better/JJR than/IN expected/NONE and/VBN the/CC widely/DT anticipated/RB report/VBN on/NN october/IN employment/NNP scheduled/NN 1/VBN to/NONE UNK/TO tomorrow/VB stock/NN prices/NN UNK/NNS only/VBD modestly/RB in/RB response/IN to/NN the/TO report/DT and/NN then/CC UNK/RB PAD/VBD\n",
            "predicted: but/CC with/IN the/DT index/NN proving/VBG somewhat/RB better/JJR than/IN expected/NONE and/CC the/DT widely/DT anticipated/VBN report/NN on/IN october/NNP employment/NNP scheduled/VBN 1/NONE to/TO UNK/VB tomorrow/VB stock/NN prices/NNS UNK/NNS only/RB modestly/RB in/VB response/DT to/TO the/DT report/NN and/NN then/RB UNK/NONE\n",
            " \n",
            "labeled  : that/DT proposal/NN had/VBD been/VBN UNK/VBN 1/NONE by/IN environmentalists/NNS but/CC UNK/VBN 1/NONE by/IN utilities/NNS because/IN they/PRP feared/VBD 0/NONE it/PRP would/MD limit/VB their/PRP growth/NN\n",
            "predicted: that/IN proposal/NN had/VBD been/VBN UNK/VBN 1/NONE by/IN environmentalists/NNS but/CC UNK/VBN 1/NONE by/IN utilities/NNS because/IN they/PRP feared/VBD 0/NONE it/PRP would/MD limit/VB their/PRP growth/NN\n",
            " \n",
            "labeled  : the/DT senior/NN of/IN the/DT three/CD executives/NNS who/WP t/NONE 1/MD will/VB assume/NNP mr/NNP reupke/POS 's/NNS duties/VBZ is/NNP UNK/NNP UNK/CD 58/NN finance/NN director/CC and/DT a/NNP reuters/NN board/NN\n",
            "predicted: the/DT senior/JJ of/IN the/DT three/CD executives/NNS who/WP t/NONE 1/MD will/VB assume/NNP mr/NNP reupke/NNP 's/NNS duties/VBZ is/VBZ UNK/NNP UNK/NN 58/NN finance/NN director/CC and/NNP a/NNP reuters/NNP board/NN\n",
            " \n",
            "labeled  : the/DT ginnie/NNP mae/NNP 9/CD securities/NN were/NNS yielding/VBD 9/VBG 32/CD to/NN a/TO 12/DT year/JJ average/NN life/NN\n",
            "predicted: the/DT ginnie/NNP mae/NNP 9/VBD securities/VBD were/VBD yielding/CD 9/TO 32/TO to/DT a/NN 12/NN year/NN average/NN\n",
            " \n",
            "labeled  : mr/NNP hahn/NNP agrees/VBZ that/IN he/PRP has/VBZ a/DT retentive/JJ ''/'' memory/NN but/CC friends/NNS say/VBP 0/NONE that/DT 's/VBZ an/DT understatement/NN\n",
            "predicted: mr/NNP hahn/NNP agrees/VBZ that/IN he/PRP has/VBZ a/DT retentive/JJ ''/'' memory/NN but/CC friends/NNS say/VBP 0/NONE that/IN 's/POS an/DT understatement/NN\n",
            " \n",
            "labeled  : yesterday/NN carnival/NNP said/VBD 0/NONE a/DT new/JJ company/NN ich/NONE 1/VBZ has/VBN been/VBN formed/NONE 2/IN in/NNP finland/WDT that/NONE t/MD 3/VB will/RP carry/NNP on/POS waertsilae/NN 's/NNS\n",
            "predicted: yesterday/NN carnival/NNP said/VBD 0/NONE a/DT new/JJ company/NN ich/NONE 1/VBZ has/VBN been/VBN formed/IN 2/IN in/NNP finland/WDT that/NONE t/MD 3/MD will/VB carry/IN on/NNP waertsilae/POS 's/NNS\n",
            " \n",
            "labeled  : the/DT companies/NNS are/VBP giving/VBG four/JJ day/NNS vacations/IN for/CD two/TO to/NNP buick/NNS buyers/WP who/NONE t/VBP 51/DT charge/CC all/NN or/IN part/PRP of/IN their/NNS down/IN payments/DT on/NNP the/NNP american/JJ express/NN\n",
            "predicted: the/DT companies/NNS are/VBP giving/VBG four/CD day/NNS vacations/IN for/CD two/TO to/NNP buick/NNS buyers/NNS who/NONE t/NONE 51/NN charge/DT all/CC or/IN part/IN of/PRP their/IN down/IN payments/IN on/DT the/NNP american/NNP express/JJ\n",
            " \n",
            "labeled  : the/DT chicago/NNP merc/NNP plans/VBZ an/DT additional/JJ circuit/NN breaker/NN ''/'' 0/NONE to/NONE stem/TO sharp/VB UNK/JJ in/NNS the/IN market/DT t/NN 1/NONE\n",
            "predicted: the/DT chicago/NNP merc/NNP plans/VBZ an/DT additional/JJ circuit/NN breaker/NN ''/'' 0/NONE to/TO stem/TO sharp/VB UNK/NNS in/IN the/IN market/NN t/NONE\n",
            " \n",
            "labeled  : the/DT company/NN currently/RB offers/VBZ a/DT word/NN processing/NN package/IN for/JJ personal/NNS computers/VBN called/NONE legend/NNP\n",
            "predicted: the/DT company/NN currently/RB offers/VBZ a/DT word/NN processing/NN package/IN for/JJ personal/NNS computers/NONE called/NONE legend/NNP\n",
            " \n",
            "labeled  : bailey/NNP controls/NNP based/VBN in/NONE wickliffe/IN ohio/NNP makes/NNP computerized/VBZ industrial/JJ controls/JJ systems/NNS PAD/NNS\n",
            "predicted: bailey/NNP controls/NNP based/VBN in/NONE wickliffe/IN ohio/NNP makes/NNP computerized/VBZ industrial/JJ controls/NNS systems/NNS\n",
            " \n",
            "labeled  : 1/NONE currently/RB a/DT 300/CD million/JJ a/NN year/CD business/NN 900/NN telephone/VBZ service/VBN is/NONE expected/TO 1/VB to/CD hit/CD 500/NONE million/JJ u/NN next/CC year/IN and/CD near/CD 2/NONE billion/IN u/CD by/IN 1992/NNS as/IN uses/DT for/NN the/VBP service/NONE continue/TO 3/VB to/VBZ expand/NONE says/NONE 0/NNP t/NNP 2/IN joel/NNP gross/NNP of/CC donaldson/NNP lufkin/NNP\n",
            "predicted: 1/NONE currently/RB a/DT 300/CD million/CD a/NN year/NN business/NN 900/NN telephone/NN service/VBZ is/VBN expected/NONE 1/TO to/CD hit/CD 500/CD million/NONE u/JJ next/NN year/CC and/CC near/CD 2/CD billion/NONE u/IN by/IN 1992/IN as/IN uses/IN for/DT the/NN service/NONE continue/NONE 3/TO to/VB expand/NONE says/NONE 0/NONE t/NNP 2/NNP joel/NNP gross/NNP of/NNP donaldson/NNP lufkin/NNP\n",
            " \n",
            "labeled  : a/DT computer/NN using/VBG the/DT more/JJ advanced/NNP intel/NNP corp/CD 386/NN UNK/IN with/CD four/NNS UNK/IN of/NN memory/CC and/DT a/JJ 100/JJ megabyte/NN hard/RB disk/VBZ now/IN sells/CD for/NONE 5/RB UNK/IN u/CD down/NONE\n",
            "predicted: a/DT computer/NN using/VBG the/DT more/DT advanced/NN intel/NNP corp/CD 386/CD UNK/IN with/CD four/CD UNK/IN of/NN memory/NN and/DT a/JJ 100/JJ megabyte/NN hard/NN disk/RB now/RB sells/IN for/CD 5/CD UNK/VBN u/IN down/IN from/CD\n",
            " \n",
            "labeled  : temple/NNP however/RB harshly/RB criticized/VBD sea/NNP containers/NNPS '/POS plan/NN yesterday/NN 1/NONE characterizing/VBG it/PRP as/IN a/DT highly/RB conditional/JJ device/NN designed/VBN 2/NONE to/NONE entrench/TO management/VB confuse/NN shareholders/VB and/NNS prevent/CC them/VB from/PRP 3/IN accepting/NONE our/VBG superior/PRP cash/JJ offer/NN ''/NN PAD/''\n",
            "predicted: temple/NNP however/RB harshly/RB criticized/VBD sea/NNP containers/NNPS '/POS plan/NN yesterday/NN 1/NONE characterizing/VBG it/PRP as/IN a/DT highly/RB conditional/JJ device/NN designed/VBN 2/NONE to/TO entrench/TO management/VB confuse/VB shareholders/CC and/CC prevent/PRP them/PRP from/PRP 3/NONE accepting/PRP our/PRP superior/JJ cash/NN offer/NN ''/''\n",
            " \n",
            "labeled  : lsi/NNP logic/NNP corp/NNP reported/VBD a/DT surprise/NN 35/CD 7/CD million/NONE u/JJ third/JJ quarter/NN net/VBG loss/DT including/JJ a/NN special/NN restructuring/WDT charge/NONE that/VBZ t/DT 145/VBG reflects/JJ a/NN continuing/IN industry/NN wide/NN\n",
            "predicted: lsi/NNP logic/NNP corp/NNP reported/VBD a/DT surprise/NN 35/CD 7/CD million/NONE u/NN third/JJ quarter/JJ net/NN loss/DT including/DT a/JJ special/NN restructuring/NN charge/NONE that/NONE t/DT 145/VBZ reflects/DT a/NN continuing/JJ industry/NN wide/NN\n",
            " \n",
            "labeled  : mrs/NNP yeargin/NNP says/VBZ that/IN she/PRP also/RB wanted/VBD 1/NONE to/TO help/VB 2/NONE lift/VB greenville/NNP high/NNP school/NNP 's/POS overall/JJ test/NN scores/NNS usually/RB near/IN the/DT bottom/NN of/IN 14/CD district/NN high/JJ schools/NNS in/IN rankings/NNS carried/VBN annually/NONE by/RB local/IN newspapers/JJ PAD/NNS\n",
            "predicted: mrs/NNP yeargin/NNP says/VBZ that/IN she/PRP also/RB wanted/VBD 1/NONE to/TO help/VB 2/VB lift/VB greenville/NNP high/NNP school/NNP 's/POS overall/JJ test/NN scores/NNS usually/RB near/IN the/DT bottom/NN of/IN 14/CD district/NN high/JJ schools/NNS in/IN rankings/NNS carried/VBN annually/RB by/IN local/IN newspapers/NNS\n",
            " \n",
            "labeled  : the/DT u/NNP s/NNP chamber/IN of/NNP commerce/NONE 1/RB still/VBN opposed/TO to/DT any/NN mininum/NN wage/VBD increase/NONE said/DT 0/NN the/NN compromise/NONE plan/TO to/VB lift/DT the/NN wage/NN floor/CD 27/NN in/IN two/CD stages/NNS between/IN april/NNP 1990/CD and/CC april/NNP 1991/CD will/MD be/VB impossible/JJ for/IN many/JJ employers/NNS to/TO accommodate/VB and/CC will/MD result/VB in/IN the/DT elimination/NN of/IN jobs/NNS for/IN american/JJ workers/NNS and/CC higher/JJR prices/NNS for/IN american/JJ consumers/NNS\n",
            "predicted: the/DT u/NNP s/NNP chamber/IN of/NNP commerce/NNP 1/RB still/RB opposed/TO to/DT any/NN mininum/NN wage/NN increase/NONE said/NONE 0/DT the/NN compromise/NN plan/NONE to/VB lift/DT the/DT wage/NN floor/NN 27/CD in/IN two/CD stages/NNS between/IN april/NNP 1990/CD and/CC april/NNP 1991/CD will/MD be/VB impossible/JJ for/IN many/JJ employers/NNS to/TO accommodate/VB and/CC will/MD result/VB in/IN the/DT elimination/NN of/IN jobs/NNS for/IN american/JJ workers/NNS and/CC higher/JJR prices/NNS for/IN american/JJ consumers/NNS\n",
            " \n",
            "labeled  : they/PRP have/VBP n't/RB forgotten/VBN the/DT leap/NN in/IN share/NN prices/NNS last/JJ dec/NNP 7/CD when/WRB the/DT first/JJ bout/NN of/IN foreign/JJ led/NN index/NN arbitrage/VBD drove/NNS stocks/RB skyward/IN in/DT the/JJ last/JJ half/IN hour/NN of/NONE trading/NONE t/JJ 1/NNS startling/WP regulators/NONE who/VBD t/NONE 254/PRP thought/VBD 0/VBN they/JJ had/NNS written/NONE enough/NONE rules/TO 0/VB t/PDT 2/DT to/NN\n",
            "predicted: they/PRP have/VBP n't/RB forgotten/VBN the/DT leap/NN in/IN share/NN prices/NNS last/JJ dec/NNP 7/CD when/WRB the/DT first/JJ bout/NN of/IN foreign/JJ led/JJ index/JJ arbitrage/NNS drove/NNS stocks/RB skyward/IN in/DT the/JJ last/JJ half/NN hour/IN of/NN trading/NONE t/NNS 1/NNS startling/NNS regulators/NONE who/NONE t/NONE 254/VBD thought/VBD 0/PRP they/VBD had/VBN written/VBN enough/NONE rules/NONE 0/NONE t/TO 2/TO to/VB\n",
            " \n",
            "labeled  : mrs/NNP yeargin/NNP admits/VBZ 0/NONE she/PRP made/VBD a/DT big/JJ mistake/NN but/CC insists/VBZ 0/NONE her/PRP motives/NNS were/VBD correct/JJ\n",
            "predicted: mrs/NNP yeargin/NNP admits/VBZ 0/NONE she/PRP made/VBD a/DT big/JJ mistake/NN but/CC insists/VBZ 0/NONE her/PRP motives/NNS were/VBD correct/JJ\n",
            " \n",
            "labeled  : it/PRP found/VBD that/IN of/IN the/DT 73/CD who/NN t/WP 51/NONE import/VBP 10/CD t/NN 1/NONE said/VBD 0/NONE they/PRP imported/VBD more/RBR in/IN october/NNP and/CC 12/CD t/NN 1/NONE said/VBD 0/NONE they/PRP imported/VBD less/JJR than/IN the/DT previous/JJ month/NN\n",
            "predicted: it/PRP found/VBD that/IN of/IN the/DT 73/CD who/WP t/NONE 51/CD import/NN 10/NONE t/NONE 1/VBD said/VBD 0/PRP they/VBD imported/VBN more/IN in/IN october/NNP and/CC 12/NONE t/NONE 1/NONE said/NONE 0/NONE they/VBD imported/VBD less/JJR than/IN the/DT previous/JJ month/NN\n",
            " \n",
            "labeled  : the/DT reduced/VBN dividend/NN is/VBZ payable/JJ jan/NNP 2/CD to/TO stock/NN of/IN record/NN dec/NNP 15/CD\n",
            "predicted: the/DT reduced/VBN dividend/VBZ is/VBZ payable/JJ jan/NNP 2/CD to/TO stock/VB of/IN record/NN dec/NNP 15/CD\n",
            " \n",
            "labeled  : while/IN 1/NONE neither/DT admitting/VBG nor/CC denying/VBG wrongdoing/NN triton/NNP and/CC mr/NNP chase/NNP consented/VBD to/TO findings/NNS of/IN violations/NNS in/IN connection/NN with/IN limited/NN partnership/NNS\n",
            "predicted: while/IN 1/NONE neither/DT admitting/VBG nor/CC denying/VBG wrongdoing/NN triton/NNP and/CC mr/NNP chase/NNP consented/VBD to/TO findings/NNS of/IN violations/NNS in/IN connection/NN with/IN limited/JJ partnership/NNS sales/NNS\n",
            " \n",
            "labeled  : dan/NNP e/NNP nelms/NNP valley/NNP federal/NNP 's/POS president/NN and/CC chief/JJ executive/NN officer/NN said/VBD 0/NONE the/DT one/JJ time/NN charge/RB substantially/VBZ eliminates/JJ future/NNS losses/VBN associated/NONE with/IN the/DT unit/NN\n",
            "predicted: dan/NNP e/NNP nelms/NNP valley/NNP federal/NNP 's/POS president/NN and/CC chief/NN executive/NN officer/NN said/VBD 0/NONE the/DT one/NN time/NN charge/NN substantially/RB eliminates/JJ future/NNS losses/NNS associated/VBN with/NONE the/DT unit/NN\n",
            " \n",
            "pos tagging accuracy: 0.981\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfrfGb7rasyR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3MSSD_VPa-6",
        "colab_type": "text"
      },
      "source": [
        "#**Sequence to Sequence without Attention**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOCuN13FPhKm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import re\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import unicodedata "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2IDuOUbP3xG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gS2jhrWyQAoV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_up_logs(data_dir):\n",
        "  checkpoint_dir = os.path.join(data_dir, \"checkpoints\")\n",
        "  if os.path.exists(checkpoint_dir):\n",
        "    shutil.rmtree(checkpoint_dir, ignore_errors=True)\n",
        "    os.makedir(checkpoint_dir)\n",
        "  return checkpoint_dir"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkEfW4rjQm-e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_sentences(sent):\n",
        "  sent = \"\".join([c for c in unicodedata.normalize(\"NFD\", sent) if unicodedata.category(c) != \"Mn\"])\n",
        "  sent = re.sub(r\"([!.?])\", r\" \\1\", sent)\n",
        "  sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n",
        "  sent = sent.lower()\n",
        "  return sent"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-8G8NCLRp67",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def download_and_read():\n",
        "  en_sents, fr_sents_in, fr_sents_out = [], [], []\n",
        "  local_file = os.path.join(\"datasets\", \"fra.txt\")\n",
        "  with open(local_file, \"r\") as fin:\n",
        "    for i, line in enumerate(fin):\n",
        "      en_sent, fr_sent = line.strip().split('\\t')\n",
        "      en_sent = [w for w in preprocess_sentence(en_sent).split()]\n",
        "      fr_sent = preprocess_sentences(fr_sent)\n",
        "      fr_sent_in = [w for w in (\"BOS \" + fr_sent).split()]\n",
        "      fr_sent_out = [w for w in (fr_sent + \"EOS\").split()]\n",
        "      en_sents.append(en_sent)\n",
        "      fr_sents_in.append(fr_sent_in)\n",
        "      fr_sents_out.append(fr_sent_out)\n",
        "      if i >= num_sent_pairs - 1:\n",
        "        break\n",
        "  return en_sents, fr_sents_in, fr_sents_out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lG0i_MZ1T_1C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, num_timesteps, embedding_dim, encoder_dim, **kwargs):\n",
        "      super(Encoder, self).__init__(**kwargs)\n",
        "      self.encoder_dim = encoder_dim\n",
        "      self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=num_timesteps)\n",
        "      self.rnn = tf.keras.layers.GRU(encoder_dim, return_sequences=False, return_state=True)\n",
        "  def call(self, x, state):\n",
        "    x = self.embedding(x)\n",
        "    x, state = self.rnn(x, initial_state=state)\n",
        "    return x, state\n",
        "\n",
        "  def init_state(self, batch_size):\n",
        "    return tf.zeros((batch_size, self.encoder_dim))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhxQLGrmZcBR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, num_timesteps, decoder_dim, **kwargs):\n",
        "    super(Decoder, self).__init__(**kwargs)\n",
        "    self.decoder_dim = decoder_dim\n",
        "    self.embedding = tf.keras.layers.Embedding(\n",
        "        vocab_size, embedding_dim, input_length = num_timesteps)\n",
        "    self.rnn = tf.keras.layers.GRU(\n",
        "        decoder_dim, return_sequences=True, return_state=True\n",
        "    )\n",
        "    self.dense = tf.kera.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, x, state):\n",
        "    x = self.embedding(x)\n",
        "    x, state = self.rnn(x, state)\n",
        "    x = self.dense(x)\n",
        "    return x, state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQDSbxekbkVO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_fn(ytrue, ypred):\n",
        "  scce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "  mask = tf.math.logical_not(tf.math.equal(ytrue, 0))\n",
        "  mask = tf.cast(mask, dtype=tf.int64)\n",
        "  loss = scce(ytrue, ypred, sample_weight=mask)\n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTrxBEcrcoeI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function()\n",
        "def train_step(encoder_in, decoder_in, decoder_out, encoder_state):\n",
        "  with tf.GradientTape() as tape:\n",
        "    encoder_out, encoder_state = encoder(encoder_in, encoder_state)\n",
        "    decoder_state = encoder_state\n",
        "    decoder_pred, decoder_state = decoder(decoder_in, decoder_state)\n",
        "    loss = loss_fn(decoder_out, decoder_pred)\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "  return loss \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9xFv658e52g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(encoder, decoder, batch_size, sents_en, data_en, sents_fr_out, word2idx_fr, idx2word_fr):\n",
        "  random_id = np.random.choice(len(sents_en))\n",
        "  print(\"input    : \", \" \".join(sents_en[random_id]))\n",
        "  print(\"label    : \", \" \".join(sents_fr_out[random_id]))\n",
        "\n",
        "  encoder_in = tf.expand_dims(data_en[random_id], axis=0)\n",
        "  decoder_out = tf.expand_dims(sents_fr_out[random_id], axis=0)\n",
        "\n",
        "  encoder_state = encoder.init_state(1)\n",
        "  encoder_out, encoder_state = encoder(encoder_in, encoder_state)\n",
        "  decoder_state = encoder_state\n",
        "\n",
        "  decoder_in = tf.expand_dims(\n",
        "      tf.constant([word2idx_fr[\"BOS\"]]), axis=0)\n",
        "  \n",
        "  pred_sent_fr = []\n",
        "\n",
        "  while True:\n",
        "    decoder_pred = decoder_state = decoder(decoder_in, decoder_state)\n",
        "    decoder_pred = tf.argmax(decoder_pred, axis=-1)\n",
        "    pred_word = idx2word_fr[decoder_pred.numpy()[0][0]]\n",
        "    pred_sent_fr.append(pred_word)\n",
        "    if pred_word == \"EOS\":\n",
        "      break\n",
        "    decoder_in = decoder_pred\n",
        "\n",
        "  print(\"predicted: \", \" \".join(pred_send_f))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4q24XDUGige9",
        "colab_type": "code",
        "outputId": "76f9cac2-cc52-4d4f-a857-3bc64d6298ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def evaluate_bleu_score(encoder, decoder, test_dataset, word2idx_fr, idx2word_fr):\n",
        "  bleu_scores = []\n",
        "  smooth_fn = SmoothingFunction()\n",
        "  for encoder_in, decoder_in, decoder_out in test_dataset:\n",
        "    encoder_state = encoder.init_state(batch_size)\n",
        "    encoder_out, encoder_state = encoder(encoder_in, encoder_state)\n",
        "    decoder_state = encoder_state\n",
        "    decoder_pred, decoder_state = decoder(decoder_in, decoder_state)\n",
        "\n",
        "    # compute argmax\n",
        "    decoder_out = decoder_out.numpy()\n",
        "    decoder_pred = tf.argmax(decoder_pred, axis=-1).numpy()\n",
        "\n",
        "    for i in range(decoder.shape[0]):\n",
        "      ref_sent = [idx2word_fr[j] for j in decoder_out[i].tolist() if j > 0]\n",
        "      hyp_sent = [idx2word_fr[j] for j in decoder_pred[i].tolist() if j > 0]\n",
        "      # remove tailing EOS\n",
        "      ref_sent = ref_sent[0:-1]\n",
        "      hyp_sent = hyp_sent[0:-1]\n",
        "      blue_score = sentence_bleu([ref_sent], hyp_sent, smoothing_function=smooth_fn.method1)\n",
        "      blue_scores.append(bleu_score)\n",
        "\n",
        "\n",
        "    return np.mean(np.array(bleu_scores))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR! Session/line number was not unique in database. History logging moved to new session 59\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9eDEMefMy9r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}