{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCcghGhPQivb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "106abe1e-047c-497a-d56c-5bdecc1b8685"
      },
      "source": [
        "!pip install -q tensorflow==2.1.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 421.8MB 36kB/s \n",
            "\u001b[K     |████████████████████████████████| 3.9MB 37.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 450kB 46.6MB/s \n",
            "\u001b[?25h  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZqScrzCQxH_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import cProfile"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiV2Q_eBk71B",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UISTYx33RzGE",
        "colab_type": "code",
        "outputId": "d9f9d8f6-2559-4151-911f-ac1dcc4e4349",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.executing_eagerly()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgRWHNiyR2sw",
        "colab_type": "code",
        "outputId": "2c093ce8-636d-4b54-d203-c084f131f083",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x = [[2.0]]\n",
        "m = tf.matmul(x, x)\n",
        "print(\"hello {}\".format(m))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hello [[4.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QME-NSWMSAIP",
        "colab_type": "code",
        "outputId": "b249891d-1320-4283-ba8e-87aec3f1c1e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "a = tf.constant([[1, 2], [3, 4]])\n",
        "print(\"Hello {}\".format(a))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello [[1 2]\n",
            " [3 4]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCymJafLSrMw",
        "colab_type": "code",
        "outputId": "cbc096a7-49d4-4211-89ec-e409d8356448",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR! Session/line number was not unique in database. History logging moved to new session 59\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTgg16s0XOwF",
        "colab_type": "code",
        "outputId": "62acbac5-60f4-491a-9c24-0a25cc9e97cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Network training parameters\n",
        "EPOCHS = 200\n",
        "BATCH_SIZE = 128\n",
        "VERBOSE = 1\n",
        "\n",
        "NB_CLASSES = 10 # number of outputs = number of digits\n",
        "N_HIDDEN = 128\n",
        "VALIDATION_SPLIT = 0.2 # how much TRAIN is reserved for VALIDATION\n",
        "\n",
        "# loading MNIST dataset\n",
        "# verify\n",
        "# the split between train and test is 60,000, and 10,000 respectly\n",
        "\n",
        "mnist = keras.datasets.mnist\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "\n",
        "# X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
        "RESHAPED = 784\n",
        "# \n",
        "X_train = X_train.reshape(60000, RESHAPED)\n",
        "X_test = X_test.reshape(10000, RESHAPED)\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "\n",
        "# normalize in [0,1]\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "print(X_train.shape[0], 'train samples')\n",
        "print(X_test.shape[0], 'test samples')\n",
        "\n",
        "#one-hot\n",
        "Y_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)\n",
        "Y_test = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)\n",
        "\n",
        "# build the model\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(keras.layers.Dense(NB_CLASSES, input_shape=(RESHAPED,), name='dense_layer', activation='softmax'))\n",
        "\n",
        "# summary of the model\n",
        "model.summary()\n",
        "\n",
        "# compiling the model\n",
        "model.compile(optimizer='SGD', loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# training the model\n",
        "model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
        "\n",
        "# evaluate the model\n",
        "test_loss, test_acc = model.evaluate(X_test, Y_test)\n",
        "print('Test accuracy: ', test_acc)\n",
        "\n",
        "# making prediction\n",
        "predictions = model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000 train samples\n",
            "10000 test samples\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_layer (Dense)          (None, 10)                7850      \n",
            "=================================================================\n",
            "Total params: 7,850\n",
            "Trainable params: 7,850\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/200\n",
            "48000/48000 [==============================] - 1s 24us/sample - loss: 1.3662 - accuracy: 0.6748 - val_loss: 0.8938 - val_accuracy: 0.8230\n",
            "Epoch 2/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.7927 - accuracy: 0.8276 - val_loss: 0.6578 - val_accuracy: 0.8577\n",
            "Epoch 3/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.6436 - accuracy: 0.8510 - val_loss: 0.5625 - val_accuracy: 0.8699\n",
            "Epoch 4/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.5713 - accuracy: 0.8621 - val_loss: 0.5095 - val_accuracy: 0.8780\n",
            "Epoch 5/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.5271 - accuracy: 0.8683 - val_loss: 0.4756 - val_accuracy: 0.8824\n",
            "Epoch 6/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.4967 - accuracy: 0.8734 - val_loss: 0.4512 - val_accuracy: 0.8857\n",
            "Epoch 7/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.4743 - accuracy: 0.8770 - val_loss: 0.4332 - val_accuracy: 0.8898\n",
            "Epoch 8/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.4569 - accuracy: 0.8801 - val_loss: 0.4188 - val_accuracy: 0.8917\n",
            "Epoch 9/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.4429 - accuracy: 0.8829 - val_loss: 0.4073 - val_accuracy: 0.8944\n",
            "Epoch 10/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.4313 - accuracy: 0.8849 - val_loss: 0.3976 - val_accuracy: 0.8963\n",
            "Epoch 11/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.4215 - accuracy: 0.8874 - val_loss: 0.3895 - val_accuracy: 0.8976\n",
            "Epoch 12/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.4131 - accuracy: 0.8891 - val_loss: 0.3827 - val_accuracy: 0.8991\n",
            "Epoch 13/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.4057 - accuracy: 0.8903 - val_loss: 0.3766 - val_accuracy: 0.9003\n",
            "Epoch 14/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3993 - accuracy: 0.8917 - val_loss: 0.3711 - val_accuracy: 0.9015\n",
            "Epoch 15/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.3934 - accuracy: 0.8927 - val_loss: 0.3666 - val_accuracy: 0.9023\n",
            "Epoch 16/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.3882 - accuracy: 0.8936 - val_loss: 0.3622 - val_accuracy: 0.9031\n",
            "Epoch 17/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3836 - accuracy: 0.8948 - val_loss: 0.3582 - val_accuracy: 0.9038\n",
            "Epoch 18/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3792 - accuracy: 0.8956 - val_loss: 0.3546 - val_accuracy: 0.9043\n",
            "Epoch 19/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3754 - accuracy: 0.8964 - val_loss: 0.3515 - val_accuracy: 0.9051\n",
            "Epoch 20/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3718 - accuracy: 0.8972 - val_loss: 0.3485 - val_accuracy: 0.9058\n",
            "Epoch 21/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3684 - accuracy: 0.8983 - val_loss: 0.3458 - val_accuracy: 0.9061\n",
            "Epoch 22/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3653 - accuracy: 0.8989 - val_loss: 0.3431 - val_accuracy: 0.9058\n",
            "Epoch 23/200\n",
            "48000/48000 [==============================] - 1s 20us/sample - loss: 0.3624 - accuracy: 0.8996 - val_loss: 0.3409 - val_accuracy: 0.9068\n",
            "Epoch 24/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3597 - accuracy: 0.9002 - val_loss: 0.3386 - val_accuracy: 0.9073\n",
            "Epoch 25/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3571 - accuracy: 0.9007 - val_loss: 0.3366 - val_accuracy: 0.9081\n",
            "Epoch 26/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3548 - accuracy: 0.9013 - val_loss: 0.3346 - val_accuracy: 0.9088\n",
            "Epoch 27/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3525 - accuracy: 0.9020 - val_loss: 0.3328 - val_accuracy: 0.9089\n",
            "Epoch 28/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3504 - accuracy: 0.9022 - val_loss: 0.3310 - val_accuracy: 0.9098\n",
            "Epoch 29/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3484 - accuracy: 0.9029 - val_loss: 0.3294 - val_accuracy: 0.9097\n",
            "Epoch 30/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3465 - accuracy: 0.9035 - val_loss: 0.3279 - val_accuracy: 0.9099\n",
            "Epoch 31/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3446 - accuracy: 0.9036 - val_loss: 0.3263 - val_accuracy: 0.9103\n",
            "Epoch 32/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3429 - accuracy: 0.9042 - val_loss: 0.3249 - val_accuracy: 0.9106\n",
            "Epoch 33/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.3413 - accuracy: 0.9047 - val_loss: 0.3236 - val_accuracy: 0.9108\n",
            "Epoch 34/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3397 - accuracy: 0.9054 - val_loss: 0.3223 - val_accuracy: 0.9117\n",
            "Epoch 35/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3382 - accuracy: 0.9053 - val_loss: 0.3211 - val_accuracy: 0.9120\n",
            "Epoch 36/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3367 - accuracy: 0.9059 - val_loss: 0.3200 - val_accuracy: 0.9121\n",
            "Epoch 37/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.3353 - accuracy: 0.9059 - val_loss: 0.3188 - val_accuracy: 0.9130\n",
            "Epoch 38/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3340 - accuracy: 0.9064 - val_loss: 0.3178 - val_accuracy: 0.9126\n",
            "Epoch 39/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.3327 - accuracy: 0.9070 - val_loss: 0.3167 - val_accuracy: 0.9132\n",
            "Epoch 40/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3315 - accuracy: 0.9074 - val_loss: 0.3159 - val_accuracy: 0.9135\n",
            "Epoch 41/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3303 - accuracy: 0.9075 - val_loss: 0.3149 - val_accuracy: 0.9138\n",
            "Epoch 42/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3291 - accuracy: 0.9079 - val_loss: 0.3139 - val_accuracy: 0.9144\n",
            "Epoch 43/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3280 - accuracy: 0.9083 - val_loss: 0.3131 - val_accuracy: 0.9146\n",
            "Epoch 44/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3270 - accuracy: 0.9087 - val_loss: 0.3122 - val_accuracy: 0.9148\n",
            "Epoch 45/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3259 - accuracy: 0.9091 - val_loss: 0.3113 - val_accuracy: 0.9149\n",
            "Epoch 46/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3249 - accuracy: 0.9089 - val_loss: 0.3106 - val_accuracy: 0.9147\n",
            "Epoch 47/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3240 - accuracy: 0.9096 - val_loss: 0.3099 - val_accuracy: 0.9151\n",
            "Epoch 48/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3231 - accuracy: 0.9098 - val_loss: 0.3091 - val_accuracy: 0.9153\n",
            "Epoch 49/200\n",
            "48000/48000 [==============================] - 1s 20us/sample - loss: 0.3221 - accuracy: 0.9098 - val_loss: 0.3083 - val_accuracy: 0.9150\n",
            "Epoch 50/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3212 - accuracy: 0.9101 - val_loss: 0.3078 - val_accuracy: 0.9153\n",
            "Epoch 51/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3204 - accuracy: 0.9106 - val_loss: 0.3070 - val_accuracy: 0.9155\n",
            "Epoch 52/200\n",
            "48000/48000 [==============================] - 1s 20us/sample - loss: 0.3196 - accuracy: 0.9108 - val_loss: 0.3063 - val_accuracy: 0.9158\n",
            "Epoch 53/200\n",
            "48000/48000 [==============================] - 1s 20us/sample - loss: 0.3188 - accuracy: 0.9110 - val_loss: 0.3056 - val_accuracy: 0.9162\n",
            "Epoch 54/200\n",
            "48000/48000 [==============================] - 1s 20us/sample - loss: 0.3180 - accuracy: 0.9112 - val_loss: 0.3051 - val_accuracy: 0.9161\n",
            "Epoch 55/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3172 - accuracy: 0.9117 - val_loss: 0.3044 - val_accuracy: 0.9162\n",
            "Epoch 56/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3165 - accuracy: 0.9118 - val_loss: 0.3039 - val_accuracy: 0.9168\n",
            "Epoch 57/200\n",
            "48000/48000 [==============================] - 1s 27us/sample - loss: 0.3157 - accuracy: 0.9119 - val_loss: 0.3034 - val_accuracy: 0.9170\n",
            "Epoch 58/200\n",
            "48000/48000 [==============================] - 1s 27us/sample - loss: 0.3150 - accuracy: 0.9123 - val_loss: 0.3028 - val_accuracy: 0.9159\n",
            "Epoch 59/200\n",
            "48000/48000 [==============================] - 1s 20us/sample - loss: 0.3143 - accuracy: 0.9124 - val_loss: 0.3022 - val_accuracy: 0.9168\n",
            "Epoch 60/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3136 - accuracy: 0.9127 - val_loss: 0.3017 - val_accuracy: 0.9165\n",
            "Epoch 61/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3130 - accuracy: 0.9126 - val_loss: 0.3012 - val_accuracy: 0.9168\n",
            "Epoch 62/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3123 - accuracy: 0.9131 - val_loss: 0.3008 - val_accuracy: 0.9167\n",
            "Epoch 63/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3117 - accuracy: 0.9131 - val_loss: 0.3003 - val_accuracy: 0.9171\n",
            "Epoch 64/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.3111 - accuracy: 0.9134 - val_loss: 0.2998 - val_accuracy: 0.9171\n",
            "Epoch 65/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3105 - accuracy: 0.9137 - val_loss: 0.2993 - val_accuracy: 0.9171\n",
            "Epoch 66/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3099 - accuracy: 0.9139 - val_loss: 0.2988 - val_accuracy: 0.9168\n",
            "Epoch 67/200\n",
            "48000/48000 [==============================] - 1s 20us/sample - loss: 0.3094 - accuracy: 0.9139 - val_loss: 0.2984 - val_accuracy: 0.9170\n",
            "Epoch 68/200\n",
            "48000/48000 [==============================] - 1s 21us/sample - loss: 0.3088 - accuracy: 0.9139 - val_loss: 0.2980 - val_accuracy: 0.9174\n",
            "Epoch 69/200\n",
            "48000/48000 [==============================] - 1s 22us/sample - loss: 0.3083 - accuracy: 0.9141 - val_loss: 0.2976 - val_accuracy: 0.9172\n",
            "Epoch 70/200\n",
            "48000/48000 [==============================] - 1s 21us/sample - loss: 0.3077 - accuracy: 0.9144 - val_loss: 0.2972 - val_accuracy: 0.9175\n",
            "Epoch 71/200\n",
            "48000/48000 [==============================] - 1s 22us/sample - loss: 0.3072 - accuracy: 0.9146 - val_loss: 0.2969 - val_accuracy: 0.9174\n",
            "Epoch 72/200\n",
            "48000/48000 [==============================] - 1s 21us/sample - loss: 0.3067 - accuracy: 0.9148 - val_loss: 0.2964 - val_accuracy: 0.9176\n",
            "Epoch 73/200\n",
            "48000/48000 [==============================] - 1s 21us/sample - loss: 0.3062 - accuracy: 0.9148 - val_loss: 0.2960 - val_accuracy: 0.9173\n",
            "Epoch 74/200\n",
            "48000/48000 [==============================] - 1s 21us/sample - loss: 0.3057 - accuracy: 0.9149 - val_loss: 0.2956 - val_accuracy: 0.9177\n",
            "Epoch 75/200\n",
            "48000/48000 [==============================] - 1s 21us/sample - loss: 0.3052 - accuracy: 0.9147 - val_loss: 0.2953 - val_accuracy: 0.9177\n",
            "Epoch 76/200\n",
            "48000/48000 [==============================] - 1s 21us/sample - loss: 0.3047 - accuracy: 0.9151 - val_loss: 0.2949 - val_accuracy: 0.9178\n",
            "Epoch 77/200\n",
            "48000/48000 [==============================] - 1s 21us/sample - loss: 0.3042 - accuracy: 0.9155 - val_loss: 0.2946 - val_accuracy: 0.9182\n",
            "Epoch 78/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3038 - accuracy: 0.9156 - val_loss: 0.2942 - val_accuracy: 0.9181\n",
            "Epoch 79/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3033 - accuracy: 0.9155 - val_loss: 0.2939 - val_accuracy: 0.9185\n",
            "Epoch 80/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.3029 - accuracy: 0.9158 - val_loss: 0.2936 - val_accuracy: 0.9183\n",
            "Epoch 81/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3025 - accuracy: 0.9160 - val_loss: 0.2933 - val_accuracy: 0.9181\n",
            "Epoch 82/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.3020 - accuracy: 0.9160 - val_loss: 0.2930 - val_accuracy: 0.9187\n",
            "Epoch 83/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3016 - accuracy: 0.9160 - val_loss: 0.2927 - val_accuracy: 0.9185\n",
            "Epoch 84/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.3012 - accuracy: 0.9162 - val_loss: 0.2924 - val_accuracy: 0.9181\n",
            "Epoch 85/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3008 - accuracy: 0.9163 - val_loss: 0.2920 - val_accuracy: 0.9184\n",
            "Epoch 86/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3004 - accuracy: 0.9165 - val_loss: 0.2917 - val_accuracy: 0.9185\n",
            "Epoch 87/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3000 - accuracy: 0.9162 - val_loss: 0.2915 - val_accuracy: 0.9186\n",
            "Epoch 88/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2996 - accuracy: 0.9165 - val_loss: 0.2912 - val_accuracy: 0.9189\n",
            "Epoch 89/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2992 - accuracy: 0.9170 - val_loss: 0.2911 - val_accuracy: 0.9188\n",
            "Epoch 90/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2989 - accuracy: 0.9168 - val_loss: 0.2907 - val_accuracy: 0.9189\n",
            "Epoch 91/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2985 - accuracy: 0.9169 - val_loss: 0.2904 - val_accuracy: 0.9193\n",
            "Epoch 92/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2981 - accuracy: 0.9169 - val_loss: 0.2902 - val_accuracy: 0.9190\n",
            "Epoch 93/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2978 - accuracy: 0.9170 - val_loss: 0.2899 - val_accuracy: 0.9196\n",
            "Epoch 94/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2974 - accuracy: 0.9171 - val_loss: 0.2896 - val_accuracy: 0.9194\n",
            "Epoch 95/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2971 - accuracy: 0.9175 - val_loss: 0.2894 - val_accuracy: 0.9195\n",
            "Epoch 96/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2967 - accuracy: 0.9176 - val_loss: 0.2891 - val_accuracy: 0.9193\n",
            "Epoch 97/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2964 - accuracy: 0.9175 - val_loss: 0.2889 - val_accuracy: 0.9196\n",
            "Epoch 98/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2961 - accuracy: 0.9174 - val_loss: 0.2886 - val_accuracy: 0.9194\n",
            "Epoch 99/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2958 - accuracy: 0.9178 - val_loss: 0.2884 - val_accuracy: 0.9197\n",
            "Epoch 100/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2954 - accuracy: 0.9178 - val_loss: 0.2882 - val_accuracy: 0.9195\n",
            "Epoch 101/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2951 - accuracy: 0.9177 - val_loss: 0.2880 - val_accuracy: 0.9195\n",
            "Epoch 102/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2948 - accuracy: 0.9178 - val_loss: 0.2877 - val_accuracy: 0.9201\n",
            "Epoch 103/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2945 - accuracy: 0.9186 - val_loss: 0.2876 - val_accuracy: 0.9191\n",
            "Epoch 104/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2942 - accuracy: 0.9181 - val_loss: 0.2874 - val_accuracy: 0.9193\n",
            "Epoch 105/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2938 - accuracy: 0.9183 - val_loss: 0.2873 - val_accuracy: 0.9199\n",
            "Epoch 106/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2936 - accuracy: 0.9181 - val_loss: 0.2869 - val_accuracy: 0.9199\n",
            "Epoch 107/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2933 - accuracy: 0.9185 - val_loss: 0.2867 - val_accuracy: 0.9200\n",
            "Epoch 108/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2930 - accuracy: 0.9184 - val_loss: 0.2865 - val_accuracy: 0.9197\n",
            "Epoch 109/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2927 - accuracy: 0.9186 - val_loss: 0.2864 - val_accuracy: 0.9194\n",
            "Epoch 110/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2924 - accuracy: 0.9185 - val_loss: 0.2862 - val_accuracy: 0.9200\n",
            "Epoch 111/200\n",
            "48000/48000 [==============================] - 1s 24us/sample - loss: 0.2922 - accuracy: 0.9186 - val_loss: 0.2859 - val_accuracy: 0.9201\n",
            "Epoch 112/200\n",
            "48000/48000 [==============================] - 1s 28us/sample - loss: 0.2919 - accuracy: 0.9186 - val_loss: 0.2857 - val_accuracy: 0.9198\n",
            "Epoch 113/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2917 - accuracy: 0.9187 - val_loss: 0.2855 - val_accuracy: 0.9197\n",
            "Epoch 114/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2914 - accuracy: 0.9189 - val_loss: 0.2854 - val_accuracy: 0.9198\n",
            "Epoch 115/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2911 - accuracy: 0.9192 - val_loss: 0.2852 - val_accuracy: 0.9204\n",
            "Epoch 116/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2909 - accuracy: 0.9192 - val_loss: 0.2851 - val_accuracy: 0.9202\n",
            "Epoch 117/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2906 - accuracy: 0.9187 - val_loss: 0.2848 - val_accuracy: 0.9202\n",
            "Epoch 118/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2904 - accuracy: 0.9190 - val_loss: 0.2846 - val_accuracy: 0.9202\n",
            "Epoch 119/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2901 - accuracy: 0.9191 - val_loss: 0.2845 - val_accuracy: 0.9206\n",
            "Epoch 120/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2898 - accuracy: 0.9193 - val_loss: 0.2843 - val_accuracy: 0.9205\n",
            "Epoch 121/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2896 - accuracy: 0.9195 - val_loss: 0.2842 - val_accuracy: 0.9203\n",
            "Epoch 122/200\n",
            "48000/48000 [==============================] - 1s 20us/sample - loss: 0.2893 - accuracy: 0.9195 - val_loss: 0.2840 - val_accuracy: 0.9207\n",
            "Epoch 123/200\n",
            "48000/48000 [==============================] - 1s 21us/sample - loss: 0.2892 - accuracy: 0.9192 - val_loss: 0.2838 - val_accuracy: 0.9207\n",
            "Epoch 124/200\n",
            "48000/48000 [==============================] - 1s 22us/sample - loss: 0.2889 - accuracy: 0.9196 - val_loss: 0.2837 - val_accuracy: 0.9208\n",
            "Epoch 125/200\n",
            "48000/48000 [==============================] - 1s 22us/sample - loss: 0.2887 - accuracy: 0.9197 - val_loss: 0.2836 - val_accuracy: 0.9208\n",
            "Epoch 126/200\n",
            "48000/48000 [==============================] - 1s 22us/sample - loss: 0.2884 - accuracy: 0.9198 - val_loss: 0.2835 - val_accuracy: 0.9209\n",
            "Epoch 127/200\n",
            "48000/48000 [==============================] - 1s 23us/sample - loss: 0.2882 - accuracy: 0.9197 - val_loss: 0.2832 - val_accuracy: 0.9210\n",
            "Epoch 128/200\n",
            "48000/48000 [==============================] - 1s 22us/sample - loss: 0.2880 - accuracy: 0.9197 - val_loss: 0.2830 - val_accuracy: 0.9209\n",
            "Epoch 129/200\n",
            "48000/48000 [==============================] - 1s 20us/sample - loss: 0.2878 - accuracy: 0.9197 - val_loss: 0.2829 - val_accuracy: 0.9211\n",
            "Epoch 130/200\n",
            "48000/48000 [==============================] - 1s 20us/sample - loss: 0.2876 - accuracy: 0.9201 - val_loss: 0.2828 - val_accuracy: 0.9209\n",
            "Epoch 131/200\n",
            "48000/48000 [==============================] - 1s 20us/sample - loss: 0.2873 - accuracy: 0.9201 - val_loss: 0.2826 - val_accuracy: 0.9210\n",
            "Epoch 132/200\n",
            "48000/48000 [==============================] - 1s 20us/sample - loss: 0.2871 - accuracy: 0.9201 - val_loss: 0.2824 - val_accuracy: 0.9208\n",
            "Epoch 133/200\n",
            "48000/48000 [==============================] - 1s 20us/sample - loss: 0.2869 - accuracy: 0.9201 - val_loss: 0.2823 - val_accuracy: 0.9216\n",
            "Epoch 134/200\n",
            "48000/48000 [==============================] - 1s 20us/sample - loss: 0.2867 - accuracy: 0.9203 - val_loss: 0.2822 - val_accuracy: 0.9212\n",
            "Epoch 135/200\n",
            "48000/48000 [==============================] - 1s 20us/sample - loss: 0.2865 - accuracy: 0.9200 - val_loss: 0.2821 - val_accuracy: 0.9212\n",
            "Epoch 136/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2863 - accuracy: 0.9203 - val_loss: 0.2819 - val_accuracy: 0.9212\n",
            "Epoch 137/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2861 - accuracy: 0.9201 - val_loss: 0.2817 - val_accuracy: 0.9212\n",
            "Epoch 138/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2859 - accuracy: 0.9205 - val_loss: 0.2816 - val_accuracy: 0.9214\n",
            "Epoch 139/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2857 - accuracy: 0.9205 - val_loss: 0.2815 - val_accuracy: 0.9215\n",
            "Epoch 140/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2855 - accuracy: 0.9204 - val_loss: 0.2814 - val_accuracy: 0.9213\n",
            "Epoch 141/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2853 - accuracy: 0.9204 - val_loss: 0.2812 - val_accuracy: 0.9216\n",
            "Epoch 142/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2851 - accuracy: 0.9206 - val_loss: 0.2811 - val_accuracy: 0.9212\n",
            "Epoch 143/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2849 - accuracy: 0.9205 - val_loss: 0.2810 - val_accuracy: 0.9214\n",
            "Epoch 144/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2847 - accuracy: 0.9208 - val_loss: 0.2808 - val_accuracy: 0.9217\n",
            "Epoch 145/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2845 - accuracy: 0.9206 - val_loss: 0.2807 - val_accuracy: 0.9213\n",
            "Epoch 146/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2843 - accuracy: 0.9207 - val_loss: 0.2807 - val_accuracy: 0.9213\n",
            "Epoch 147/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2841 - accuracy: 0.9209 - val_loss: 0.2805 - val_accuracy: 0.9216\n",
            "Epoch 148/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2839 - accuracy: 0.9209 - val_loss: 0.2804 - val_accuracy: 0.9218\n",
            "Epoch 149/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2838 - accuracy: 0.9210 - val_loss: 0.2803 - val_accuracy: 0.9222\n",
            "Epoch 150/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2836 - accuracy: 0.9210 - val_loss: 0.2802 - val_accuracy: 0.9217\n",
            "Epoch 151/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2834 - accuracy: 0.9212 - val_loss: 0.2801 - val_accuracy: 0.9218\n",
            "Epoch 152/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2832 - accuracy: 0.9211 - val_loss: 0.2800 - val_accuracy: 0.9215\n",
            "Epoch 153/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2830 - accuracy: 0.9212 - val_loss: 0.2798 - val_accuracy: 0.9218\n",
            "Epoch 154/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2829 - accuracy: 0.9212 - val_loss: 0.2797 - val_accuracy: 0.9223\n",
            "Epoch 155/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2827 - accuracy: 0.9214 - val_loss: 0.2796 - val_accuracy: 0.9221\n",
            "Epoch 156/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2825 - accuracy: 0.9211 - val_loss: 0.2795 - val_accuracy: 0.9223\n",
            "Epoch 157/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2824 - accuracy: 0.9214 - val_loss: 0.2794 - val_accuracy: 0.9220\n",
            "Epoch 158/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2822 - accuracy: 0.9218 - val_loss: 0.2793 - val_accuracy: 0.9227\n",
            "Epoch 159/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2820 - accuracy: 0.9213 - val_loss: 0.2791 - val_accuracy: 0.9228\n",
            "Epoch 160/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2818 - accuracy: 0.9214 - val_loss: 0.2792 - val_accuracy: 0.9220\n",
            "Epoch 161/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2817 - accuracy: 0.9216 - val_loss: 0.2790 - val_accuracy: 0.9228\n",
            "Epoch 162/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2815 - accuracy: 0.9218 - val_loss: 0.2789 - val_accuracy: 0.9222\n",
            "Epoch 163/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2814 - accuracy: 0.9216 - val_loss: 0.2788 - val_accuracy: 0.9223\n",
            "Epoch 164/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2812 - accuracy: 0.9217 - val_loss: 0.2786 - val_accuracy: 0.9226\n",
            "Epoch 165/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2810 - accuracy: 0.9216 - val_loss: 0.2786 - val_accuracy: 0.9218\n",
            "Epoch 166/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2809 - accuracy: 0.9219 - val_loss: 0.2785 - val_accuracy: 0.9229\n",
            "Epoch 167/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2808 - accuracy: 0.9219 - val_loss: 0.2783 - val_accuracy: 0.9224\n",
            "Epoch 168/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2806 - accuracy: 0.9217 - val_loss: 0.2783 - val_accuracy: 0.9224\n",
            "Epoch 169/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2805 - accuracy: 0.9218 - val_loss: 0.2781 - val_accuracy: 0.9226\n",
            "Epoch 170/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2803 - accuracy: 0.9221 - val_loss: 0.2781 - val_accuracy: 0.9226\n",
            "Epoch 171/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2802 - accuracy: 0.9218 - val_loss: 0.2780 - val_accuracy: 0.9229\n",
            "Epoch 172/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2800 - accuracy: 0.9218 - val_loss: 0.2780 - val_accuracy: 0.9227\n",
            "Epoch 173/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2799 - accuracy: 0.9221 - val_loss: 0.2779 - val_accuracy: 0.9232\n",
            "Epoch 174/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2797 - accuracy: 0.9221 - val_loss: 0.2777 - val_accuracy: 0.9232\n",
            "Epoch 175/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2795 - accuracy: 0.9223 - val_loss: 0.2777 - val_accuracy: 0.9229\n",
            "Epoch 176/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2794 - accuracy: 0.9221 - val_loss: 0.2776 - val_accuracy: 0.9235\n",
            "Epoch 177/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2793 - accuracy: 0.9224 - val_loss: 0.2776 - val_accuracy: 0.9228\n",
            "Epoch 178/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2791 - accuracy: 0.9220 - val_loss: 0.2774 - val_accuracy: 0.9230\n",
            "Epoch 179/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2790 - accuracy: 0.9225 - val_loss: 0.2773 - val_accuracy: 0.9234\n",
            "Epoch 180/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2788 - accuracy: 0.9224 - val_loss: 0.2773 - val_accuracy: 0.9227\n",
            "Epoch 181/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2787 - accuracy: 0.9222 - val_loss: 0.2772 - val_accuracy: 0.9228\n",
            "Epoch 182/200\n",
            "48000/48000 [==============================] - 1s 20us/sample - loss: 0.2786 - accuracy: 0.9222 - val_loss: 0.2771 - val_accuracy: 0.9226\n",
            "Epoch 183/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2784 - accuracy: 0.9222 - val_loss: 0.2770 - val_accuracy: 0.9234\n",
            "Epoch 184/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2783 - accuracy: 0.9225 - val_loss: 0.2768 - val_accuracy: 0.9233\n",
            "Epoch 185/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2782 - accuracy: 0.9223 - val_loss: 0.2768 - val_accuracy: 0.9230\n",
            "Epoch 186/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2780 - accuracy: 0.9224 - val_loss: 0.2767 - val_accuracy: 0.9230\n",
            "Epoch 187/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2779 - accuracy: 0.9222 - val_loss: 0.2767 - val_accuracy: 0.9233\n",
            "Epoch 188/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2778 - accuracy: 0.9222 - val_loss: 0.2766 - val_accuracy: 0.9230\n",
            "Epoch 189/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2776 - accuracy: 0.9226 - val_loss: 0.2765 - val_accuracy: 0.9236\n",
            "Epoch 190/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2775 - accuracy: 0.9226 - val_loss: 0.2764 - val_accuracy: 0.9232\n",
            "Epoch 191/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2774 - accuracy: 0.9226 - val_loss: 0.2763 - val_accuracy: 0.9230\n",
            "Epoch 192/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2772 - accuracy: 0.9227 - val_loss: 0.2763 - val_accuracy: 0.9233\n",
            "Epoch 193/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2771 - accuracy: 0.9226 - val_loss: 0.2763 - val_accuracy: 0.9225\n",
            "Epoch 194/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2770 - accuracy: 0.9227 - val_loss: 0.2760 - val_accuracy: 0.9234\n",
            "Epoch 195/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2769 - accuracy: 0.9230 - val_loss: 0.2760 - val_accuracy: 0.9233\n",
            "Epoch 196/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2767 - accuracy: 0.9232 - val_loss: 0.2760 - val_accuracy: 0.9234\n",
            "Epoch 197/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2766 - accuracy: 0.9229 - val_loss: 0.2759 - val_accuracy: 0.9237\n",
            "Epoch 198/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2765 - accuracy: 0.9230 - val_loss: 0.2759 - val_accuracy: 0.9234\n",
            "Epoch 199/200\n",
            "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2764 - accuracy: 0.9231 - val_loss: 0.2758 - val_accuracy: 0.9234\n",
            "Epoch 200/200\n",
            "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2762 - accuracy: 0.9230 - val_loss: 0.2757 - val_accuracy: 0.9236\n",
            "10000/10000 [==============================] - 0s 32us/sample - loss: 0.2773 - accuracy: 0.9218\n",
            "Test accuracy:  0.9218\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hu512tXAin6k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.set_printoptions(edgeitems=784)\n",
        "np.core.arrayprint._line_width = 180"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SQnh1k8c5HO",
        "colab_type": "code",
        "outputId": "d0f8f6f5-377c-4628-b584-5c8bbc06abb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(\"Hello {}\".format(X_train[:1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello [[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.01176471 0.07058824 0.07058824 0.07058824\n",
            "  0.49411765 0.53333336 0.6862745  0.10196079 0.6509804  1.\n",
            "  0.96862745 0.49803922 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.11764706 0.14117648 0.36862746 0.6039216\n",
            "  0.6666667  0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
            "  0.88235295 0.6745098  0.99215686 0.9490196  0.7647059  0.2509804\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.19215687\n",
            "  0.93333334 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
            "  0.99215686 0.99215686 0.99215686 0.9843137  0.3647059  0.32156864\n",
            "  0.32156864 0.21960784 0.15294118 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.07058824 0.85882354 0.99215686\n",
            "  0.99215686 0.99215686 0.99215686 0.99215686 0.7764706  0.7137255\n",
            "  0.96862745 0.94509804 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.3137255  0.6117647  0.41960785 0.99215686\n",
            "  0.99215686 0.8039216  0.04313726 0.         0.16862746 0.6039216\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.05490196 0.00392157 0.6039216  0.99215686 0.3529412\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.54509807 0.99215686 0.74509805 0.00784314 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.04313726\n",
            "  0.74509805 0.99215686 0.27450982 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.13725491 0.94509804\n",
            "  0.88235295 0.627451   0.42352942 0.00392157 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.31764707 0.9411765  0.99215686\n",
            "  0.99215686 0.46666667 0.09803922 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.1764706  0.7294118  0.99215686 0.99215686\n",
            "  0.5882353  0.10588235 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.0627451  0.3647059  0.9882353  0.99215686 0.73333335\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.9764706  0.99215686 0.9764706  0.2509804  0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.18039216 0.50980395 0.7176471  0.99215686\n",
            "  0.99215686 0.8117647  0.00784314 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.15294118 0.5803922\n",
            "  0.8980392  0.99215686 0.99215686 0.99215686 0.98039216 0.7137255\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.09411765 0.44705883 0.8666667  0.99215686 0.99215686 0.99215686\n",
            "  0.99215686 0.7882353  0.30588236 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.09019608 0.25882354 0.8352941  0.99215686\n",
            "  0.99215686 0.99215686 0.99215686 0.7764706  0.31764707 0.00784314\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.07058824 0.67058825\n",
            "  0.85882354 0.99215686 0.99215686 0.99215686 0.99215686 0.7647059\n",
            "  0.3137255  0.03529412 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.21568628 0.6745098  0.8862745  0.99215686 0.99215686 0.99215686\n",
            "  0.99215686 0.95686275 0.52156866 0.04313726 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.53333336 0.99215686\n",
            "  0.99215686 0.99215686 0.83137256 0.5294118  0.5176471  0.0627451\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wl5HaN9wgkA3",
        "colab_type": "code",
        "outputId": "340184a8-99ea-49ad-d7b2-360ab8ff71c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xV5R1wc9g1Uj",
        "colab_type": "code",
        "outputId": "92b1bdec-b0ed-4d42-9451-7ba379b512cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Y_train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etYOhyoxnxcf",
        "colab_type": "code",
        "outputId": "4e3943ca-a149-4a7b-c1d3-7a2ef4fb9f56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Hello {}\".format(Y_train[:1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A199SHgVn5as",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQVFfd6Yk9EC",
        "colab_type": "text"
      },
      "source": [
        "#MNIST Version 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJpbRZqxlAov",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8VTcaahlJH4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# network and training\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 128\n",
        "VERBOSE = 1\n",
        "NB_CLASSES = 10 # number of outputs = number of digits\n",
        "N_HIDDEN = 128\n",
        "VALIDATION_SPLIT = 0.2 # how much TRAIN is reserved for VALIDATION"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxri8jdmoszv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loading MNIST dataset\n",
        "# verify \n",
        "# the split between train and test is 60,000, and 10,000 respectly\n",
        "# one-hot is automatically applied\n",
        "mnist = keras.datasets.mnist\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "\n",
        "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784 \n",
        "RESHAPED = 784\n",
        "#\n",
        "X_train = X_train.reshape(60000, RESHAPED)\n",
        "X_test = X_test.reshape(10000, RESHAPED)\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNek6Vo-qg4h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#normalize in [0,1]\n",
        "X_train, X_test = X_train / 255.0, X_test / 255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCLh4lqSrLTo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#one-hot\n",
        "Y_trainOH = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)\n",
        "Y_testOH = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ionONjFOr7vb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#build the model\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(keras.layers.Dense(N_HIDDEN, input_shape=(RESHAPED,), name='dense_layer', activation='relu'))\n",
        "model.add(keras.layers.Dense(N_HIDDEN, name='dense_layer_2', activation='relu'))\n",
        "model.add(keras.layers.Dense(NB_CLASSES, name='dense_layer_3', activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmRXuSIktAgi",
        "colab_type": "code",
        "outputId": "2ff366ee-7707-4c40-d93a-84d00054cd2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "# summary of the model\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_layer (Dense)          (None, 128)               100480    \n",
            "_________________________________________________________________\n",
            "dense_layer_2 (Dense)        (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_layer_3 (Dense)        (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 118,282\n",
            "Trainable params: 118,282\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RI60rVlrtM_-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# compiling the model\n",
        "model.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reI0cqoItnZc",
        "colab_type": "code",
        "outputId": "c5fe9430-c95d-479a-f4af-b79d181e97bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#training the model\n",
        "model.fit(X_train, Y_trainOH, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=VERBOSE, validation_split=VALIDATION_SPLIT)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/50\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 1.4278 - accuracy: 0.6425 - val_loss: 0.7276 - val_accuracy: 0.8374\n",
            "Epoch 2/50\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.5856 - accuracy: 0.8530 - val_loss: 0.4436 - val_accuracy: 0.8878\n",
            "Epoch 3/50\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.4309 - accuracy: 0.8840 - val_loss: 0.3657 - val_accuracy: 0.9017\n",
            "Epoch 4/50\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.3738 - accuracy: 0.8962 - val_loss: 0.3319 - val_accuracy: 0.9068\n",
            "Epoch 5/50\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.3419 - accuracy: 0.9036 - val_loss: 0.3075 - val_accuracy: 0.9144\n",
            "Epoch 6/50\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.3202 - accuracy: 0.9088 - val_loss: 0.2917 - val_accuracy: 0.9169\n",
            "Epoch 7/50\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.3035 - accuracy: 0.9129 - val_loss: 0.2797 - val_accuracy: 0.9211\n",
            "Epoch 8/50\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.2897 - accuracy: 0.9169 - val_loss: 0.2679 - val_accuracy: 0.9232\n",
            "Epoch 9/50\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.2779 - accuracy: 0.9202 - val_loss: 0.2584 - val_accuracy: 0.9277\n",
            "Epoch 10/50\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.2675 - accuracy: 0.9229 - val_loss: 0.2496 - val_accuracy: 0.9291\n",
            "Epoch 11/50\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.2577 - accuracy: 0.9261 - val_loss: 0.2427 - val_accuracy: 0.9308\n",
            "Epoch 12/50\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.2491 - accuracy: 0.9289 - val_loss: 0.2353 - val_accuracy: 0.9348\n",
            "Epoch 13/50\n",
            "48000/48000 [==============================] - 3s 72us/sample - loss: 0.2406 - accuracy: 0.9313 - val_loss: 0.2279 - val_accuracy: 0.9352\n",
            "Epoch 14/50\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.2330 - accuracy: 0.9333 - val_loss: 0.2222 - val_accuracy: 0.9383\n",
            "Epoch 15/50\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.2257 - accuracy: 0.9352 - val_loss: 0.2169 - val_accuracy: 0.9391\n",
            "Epoch 16/50\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.2188 - accuracy: 0.9373 - val_loss: 0.2105 - val_accuracy: 0.9414\n",
            "Epoch 17/50\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.2124 - accuracy: 0.9390 - val_loss: 0.2053 - val_accuracy: 0.9434\n",
            "Epoch 18/50\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.2064 - accuracy: 0.9410 - val_loss: 0.2014 - val_accuracy: 0.9445\n",
            "Epoch 19/50\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.2007 - accuracy: 0.9422 - val_loss: 0.1964 - val_accuracy: 0.9460\n",
            "Epoch 20/50\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1952 - accuracy: 0.9438 - val_loss: 0.1926 - val_accuracy: 0.9467\n",
            "Epoch 21/50\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1903 - accuracy: 0.9449 - val_loss: 0.1873 - val_accuracy: 0.9487\n",
            "Epoch 22/50\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1852 - accuracy: 0.9468 - val_loss: 0.1850 - val_accuracy: 0.9493\n",
            "Epoch 23/50\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1807 - accuracy: 0.9479 - val_loss: 0.1809 - val_accuracy: 0.9500\n",
            "Epoch 24/50\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1762 - accuracy: 0.9494 - val_loss: 0.1770 - val_accuracy: 0.9520\n",
            "Epoch 25/50\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1719 - accuracy: 0.9507 - val_loss: 0.1736 - val_accuracy: 0.9529\n",
            "Epoch 26/50\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1680 - accuracy: 0.9519 - val_loss: 0.1704 - val_accuracy: 0.9532\n",
            "Epoch 27/50\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1641 - accuracy: 0.9532 - val_loss: 0.1673 - val_accuracy: 0.9548\n",
            "Epoch 28/50\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1604 - accuracy: 0.9540 - val_loss: 0.1650 - val_accuracy: 0.9554\n",
            "Epoch 29/50\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1568 - accuracy: 0.9554 - val_loss: 0.1618 - val_accuracy: 0.9572\n",
            "Epoch 30/50\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.1533 - accuracy: 0.9560 - val_loss: 0.1610 - val_accuracy: 0.9563\n",
            "Epoch 31/50\n",
            "48000/48000 [==============================] - 2s 35us/sample - loss: 0.1502 - accuracy: 0.9571 - val_loss: 0.1577 - val_accuracy: 0.9574\n",
            "Epoch 32/50\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1473 - accuracy: 0.9579 - val_loss: 0.1553 - val_accuracy: 0.9569\n",
            "Epoch 33/50\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1440 - accuracy: 0.9589 - val_loss: 0.1527 - val_accuracy: 0.9578\n",
            "Epoch 34/50\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1412 - accuracy: 0.9594 - val_loss: 0.1507 - val_accuracy: 0.9592\n",
            "Epoch 35/50\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1384 - accuracy: 0.9604 - val_loss: 0.1489 - val_accuracy: 0.9590\n",
            "Epoch 36/50\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1357 - accuracy: 0.9609 - val_loss: 0.1470 - val_accuracy: 0.9603\n",
            "Epoch 37/50\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1330 - accuracy: 0.9617 - val_loss: 0.1448 - val_accuracy: 0.9605\n",
            "Epoch 38/50\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1307 - accuracy: 0.9625 - val_loss: 0.1424 - val_accuracy: 0.9617\n",
            "Epoch 39/50\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1280 - accuracy: 0.9630 - val_loss: 0.1410 - val_accuracy: 0.9617\n",
            "Epoch 40/50\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1258 - accuracy: 0.9639 - val_loss: 0.1400 - val_accuracy: 0.9616\n",
            "Epoch 41/50\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.1233 - accuracy: 0.9645 - val_loss: 0.1396 - val_accuracy: 0.9622\n",
            "Epoch 42/50\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1214 - accuracy: 0.9654 - val_loss: 0.1363 - val_accuracy: 0.9631\n",
            "Epoch 43/50\n",
            "48000/48000 [==============================] - 2s 36us/sample - loss: 0.1191 - accuracy: 0.9658 - val_loss: 0.1349 - val_accuracy: 0.9628\n",
            "Epoch 44/50\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1171 - accuracy: 0.9664 - val_loss: 0.1338 - val_accuracy: 0.9638\n",
            "Epoch 45/50\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1151 - accuracy: 0.9670 - val_loss: 0.1316 - val_accuracy: 0.9639\n",
            "Epoch 46/50\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1131 - accuracy: 0.9681 - val_loss: 0.1307 - val_accuracy: 0.9647\n",
            "Epoch 47/50\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1111 - accuracy: 0.9683 - val_loss: 0.1303 - val_accuracy: 0.9639\n",
            "Epoch 48/50\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.1092 - accuracy: 0.9689 - val_loss: 0.1277 - val_accuracy: 0.9645\n",
            "Epoch 49/50\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.1074 - accuracy: 0.9692 - val_loss: 0.1267 - val_accuracy: 0.9650\n",
            "Epoch 50/50\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.1056 - accuracy: 0.9702 - val_loss: 0.1256 - val_accuracy: 0.9653\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc8a44e5b00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwjsOEMnt6Tx",
        "colab_type": "code",
        "outputId": "fdacfc95-76b5-4e71-a14b-7e32a7b10de7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# evaluate the model\n",
        "test_loss, test_acc = model.evaluate(X_test, Y_testOH)\n",
        "print('Test accuracy: ', test_acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 0s 44us/sample - loss: 0.1221 - accuracy: 0.9629\n",
            "Test accuracy:  0.9629\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C40ozLaBxlOy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make predictions\n",
        "predictions = model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvbDQdfWyAwO",
        "colab_type": "code",
        "outputId": "8b465879-3e77-40ea-cad5-dd947c46802f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "predictions"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3.55323791e-05, 8.15232738e-07, 3.53845709e-04, ...,\n",
              "        9.96411502e-01, 1.15705125e-05, 1.44277015e-04],\n",
              "       [9.99028853e-05, 1.53693720e-03, 9.94953811e-01, ...,\n",
              "        2.73126544e-09, 5.61448629e-04, 4.81672369e-10],\n",
              "       [2.97014849e-05, 9.88136649e-01, 3.32636270e-03, ...,\n",
              "        3.41135100e-03, 1.98516925e-03, 1.71995838e-04],\n",
              "       ...,\n",
              "       [5.88528408e-08, 1.62485740e-08, 5.12673886e-08, ...,\n",
              "        8.72184137e-06, 9.28286099e-05, 1.02263107e-03],\n",
              "       [8.90673107e-07, 2.11297447e-06, 7.10856796e-08, ...,\n",
              "        4.33009930e-08, 9.87833482e-04, 1.17527314e-07],\n",
              "       [1.19455153e-05, 1.33590063e-08, 2.07812391e-05, ...,\n",
              "        3.69319658e-10, 5.17953083e-07, 2.69914953e-08]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eumWGjvwGxms",
        "colab_type": "text"
      },
      "source": [
        "#**MNIST Version 3 Dropout Regularization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbJYvb62yDSu",
        "colab_type": "code",
        "outputId": "3fa6eac3-3a84-4ce3-d3e0-cfef2f73b639",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR! Session/line number was not unique in database. History logging moved to new session 61\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3IGvA-IIBgv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# network and training\n",
        "EPOCHS = 200\n",
        "BATCH_SIZE = 128\n",
        "VERBOSE = 1\n",
        "NB_CLASSES = 10 # number of outputs = number of digits\n",
        "N_HIDDEN = 128\n",
        "VALIDATION_SPLIT = 0.2 # how much TRAIN is reserved for VALIDATION\n",
        "DROPOUT = 0.3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oa0dKJ8LINuo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loading MNIST dataset\n",
        "# verify \n",
        "# the split between train and test is 60,000, and 10,000 respectly\n",
        "# one-hot is automatically applied\n",
        "mnist = keras.datasets.mnist\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "\n",
        "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784 \n",
        "RESHAPED = 784\n",
        "#\n",
        "X_train = X_train.reshape(60000, RESHAPED)\n",
        "X_test = X_test.reshape(10000, RESHAPED)\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORyZpx0WIcal",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#normalize in [0,1]\n",
        "X_train, X_test = X_train / 255.0, X_test / 255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqEGjKIrIkLW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#one-hot\n",
        "Y_trainOH = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)\n",
        "Y_testOH = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N55SyLHXIp1W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#build the model\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(keras.layers.Dense(N_HIDDEN, input_shape=(RESHAPED,), name='dense_layer', activation='relu'))\n",
        "model.add(keras.layers.Dropout(DROPOUT))\n",
        "model.add(keras.layers.Dense(N_HIDDEN, name='dense_layer_2', activation='relu'))\n",
        "model.add(keras.layers.Dense(NB_CLASSES, name='dense_layer_3', activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3v3DVc-kI54F",
        "colab_type": "code",
        "outputId": "dc0c6768-9806-446a-fd4c-89a1cf6c774a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "# summary of the model\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_layer (Dense)          (None, 128)               100480    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_layer_2 (Dense)        (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_layer_3 (Dense)        (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 118,282\n",
            "Trainable params: 118,282\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dTKOMdGI8-F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# compiling the model\n",
        "model.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLg75ka7JFO0",
        "colab_type": "code",
        "outputId": "33fa0d97-74a0-4f66-ffbf-39b6dbb7b0c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#training the model\n",
        "model.fit(X_train, Y_trainOH, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=VERBOSE, validation_split=VALIDATION_SPLIT)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 1.5301 - accuracy: 0.5557 - val_loss: 0.7890 - val_accuracy: 0.8284\n",
            "Epoch 2/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.7728 - accuracy: 0.7747 - val_loss: 0.4987 - val_accuracy: 0.8758\n",
            "Epoch 3/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.5847 - accuracy: 0.8290 - val_loss: 0.4055 - val_accuracy: 0.8942\n",
            "Epoch 4/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.5036 - accuracy: 0.8538 - val_loss: 0.3591 - val_accuracy: 0.9022\n",
            "Epoch 5/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.4536 - accuracy: 0.8678 - val_loss: 0.3274 - val_accuracy: 0.9095\n",
            "Epoch 6/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.4190 - accuracy: 0.8776 - val_loss: 0.3068 - val_accuracy: 0.9151\n",
            "Epoch 7/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.3909 - accuracy: 0.8859 - val_loss: 0.2889 - val_accuracy: 0.9180\n",
            "Epoch 8/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.3691 - accuracy: 0.8940 - val_loss: 0.2745 - val_accuracy: 0.9205\n",
            "Epoch 9/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.3501 - accuracy: 0.8987 - val_loss: 0.2623 - val_accuracy: 0.9243\n",
            "Epoch 10/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.3305 - accuracy: 0.9027 - val_loss: 0.2507 - val_accuracy: 0.9280\n",
            "Epoch 11/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.3202 - accuracy: 0.9060 - val_loss: 0.2413 - val_accuracy: 0.9297\n",
            "Epoch 12/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.3059 - accuracy: 0.9116 - val_loss: 0.2317 - val_accuracy: 0.9331\n",
            "Epoch 13/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.2956 - accuracy: 0.9148 - val_loss: 0.2247 - val_accuracy: 0.9352\n",
            "Epoch 14/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.2873 - accuracy: 0.9167 - val_loss: 0.2168 - val_accuracy: 0.9375\n",
            "Epoch 15/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.2793 - accuracy: 0.9194 - val_loss: 0.2114 - val_accuracy: 0.9393\n",
            "Epoch 16/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.2698 - accuracy: 0.9214 - val_loss: 0.2036 - val_accuracy: 0.9423\n",
            "Epoch 17/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.2634 - accuracy: 0.9236 - val_loss: 0.1989 - val_accuracy: 0.9442\n",
            "Epoch 18/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.2542 - accuracy: 0.9265 - val_loss: 0.1923 - val_accuracy: 0.9454\n",
            "Epoch 19/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.2471 - accuracy: 0.9279 - val_loss: 0.1874 - val_accuracy: 0.9471\n",
            "Epoch 20/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.2408 - accuracy: 0.9306 - val_loss: 0.1837 - val_accuracy: 0.9477\n",
            "Epoch 21/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.2368 - accuracy: 0.9304 - val_loss: 0.1790 - val_accuracy: 0.9492\n",
            "Epoch 22/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.2296 - accuracy: 0.9329 - val_loss: 0.1747 - val_accuracy: 0.9507\n",
            "Epoch 23/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.2237 - accuracy: 0.9345 - val_loss: 0.1718 - val_accuracy: 0.9510\n",
            "Epoch 24/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.2207 - accuracy: 0.9347 - val_loss: 0.1675 - val_accuracy: 0.9524\n",
            "Epoch 25/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.2170 - accuracy: 0.9354 - val_loss: 0.1641 - val_accuracy: 0.9532\n",
            "Epoch 26/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.2109 - accuracy: 0.9387 - val_loss: 0.1608 - val_accuracy: 0.9543\n",
            "Epoch 27/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.2091 - accuracy: 0.9388 - val_loss: 0.1580 - val_accuracy: 0.9540\n",
            "Epoch 28/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.2049 - accuracy: 0.9394 - val_loss: 0.1554 - val_accuracy: 0.9548\n",
            "Epoch 29/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.2012 - accuracy: 0.9412 - val_loss: 0.1522 - val_accuracy: 0.9567\n",
            "Epoch 30/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.1971 - accuracy: 0.9428 - val_loss: 0.1498 - val_accuracy: 0.9571\n",
            "Epoch 31/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.1928 - accuracy: 0.9435 - val_loss: 0.1482 - val_accuracy: 0.9565\n",
            "Epoch 32/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.1897 - accuracy: 0.9443 - val_loss: 0.1455 - val_accuracy: 0.9579\n",
            "Epoch 33/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.1852 - accuracy: 0.9451 - val_loss: 0.1428 - val_accuracy: 0.9588\n",
            "Epoch 34/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.1820 - accuracy: 0.9474 - val_loss: 0.1419 - val_accuracy: 0.9589\n",
            "Epoch 35/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.1801 - accuracy: 0.9470 - val_loss: 0.1401 - val_accuracy: 0.9596\n",
            "Epoch 36/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.1790 - accuracy: 0.9476 - val_loss: 0.1378 - val_accuracy: 0.9602\n",
            "Epoch 37/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.1747 - accuracy: 0.9490 - val_loss: 0.1360 - val_accuracy: 0.9603\n",
            "Epoch 38/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.1713 - accuracy: 0.9495 - val_loss: 0.1346 - val_accuracy: 0.9607\n",
            "Epoch 39/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.1684 - accuracy: 0.9505 - val_loss: 0.1323 - val_accuracy: 0.9609\n",
            "Epoch 40/200\n",
            "48000/48000 [==============================] - 2s 42us/sample - loss: 0.1670 - accuracy: 0.9502 - val_loss: 0.1313 - val_accuracy: 0.9617\n",
            "Epoch 41/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.1646 - accuracy: 0.9520 - val_loss: 0.1300 - val_accuracy: 0.9617\n",
            "Epoch 42/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.1631 - accuracy: 0.9519 - val_loss: 0.1281 - val_accuracy: 0.9624\n",
            "Epoch 43/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.1591 - accuracy: 0.9532 - val_loss: 0.1266 - val_accuracy: 0.9628\n",
            "Epoch 44/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.1601 - accuracy: 0.9527 - val_loss: 0.1254 - val_accuracy: 0.9627\n",
            "Epoch 45/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.1552 - accuracy: 0.9553 - val_loss: 0.1247 - val_accuracy: 0.9628\n",
            "Epoch 46/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.1536 - accuracy: 0.9553 - val_loss: 0.1228 - val_accuracy: 0.9638\n",
            "Epoch 47/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.1508 - accuracy: 0.9553 - val_loss: 0.1223 - val_accuracy: 0.9638\n",
            "Epoch 48/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.1519 - accuracy: 0.9547 - val_loss: 0.1209 - val_accuracy: 0.9644\n",
            "Epoch 49/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.1495 - accuracy: 0.9553 - val_loss: 0.1197 - val_accuracy: 0.9646\n",
            "Epoch 50/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.1476 - accuracy: 0.9568 - val_loss: 0.1189 - val_accuracy: 0.9645\n",
            "Epoch 51/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.1435 - accuracy: 0.9576 - val_loss: 0.1179 - val_accuracy: 0.9650\n",
            "Epoch 52/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.1456 - accuracy: 0.9573 - val_loss: 0.1169 - val_accuracy: 0.9653\n",
            "Epoch 53/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.1438 - accuracy: 0.9567 - val_loss: 0.1161 - val_accuracy: 0.9657\n",
            "Epoch 54/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.1420 - accuracy: 0.9577 - val_loss: 0.1149 - val_accuracy: 0.9658\n",
            "Epoch 55/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.1400 - accuracy: 0.9593 - val_loss: 0.1138 - val_accuracy: 0.9658\n",
            "Epoch 56/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.1370 - accuracy: 0.9596 - val_loss: 0.1128 - val_accuracy: 0.9667\n",
            "Epoch 57/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.1347 - accuracy: 0.9607 - val_loss: 0.1126 - val_accuracy: 0.9664\n",
            "Epoch 58/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.1384 - accuracy: 0.9591 - val_loss: 0.1108 - val_accuracy: 0.9677\n",
            "Epoch 59/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.1331 - accuracy: 0.9604 - val_loss: 0.1104 - val_accuracy: 0.9669\n",
            "Epoch 60/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.1310 - accuracy: 0.9615 - val_loss: 0.1093 - val_accuracy: 0.9678\n",
            "Epoch 61/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.1302 - accuracy: 0.9621 - val_loss: 0.1089 - val_accuracy: 0.9683\n",
            "Epoch 62/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.1306 - accuracy: 0.9619 - val_loss: 0.1078 - val_accuracy: 0.9692\n",
            "Epoch 63/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.1294 - accuracy: 0.9620 - val_loss: 0.1072 - val_accuracy: 0.9684\n",
            "Epoch 64/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.1303 - accuracy: 0.9617 - val_loss: 0.1062 - val_accuracy: 0.9691\n",
            "Epoch 65/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.1251 - accuracy: 0.9629 - val_loss: 0.1056 - val_accuracy: 0.9690\n",
            "Epoch 66/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.1255 - accuracy: 0.9626 - val_loss: 0.1050 - val_accuracy: 0.9688\n",
            "Epoch 67/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.1245 - accuracy: 0.9634 - val_loss: 0.1045 - val_accuracy: 0.9691\n",
            "Epoch 68/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.1229 - accuracy: 0.9639 - val_loss: 0.1040 - val_accuracy: 0.9694\n",
            "Epoch 69/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.1207 - accuracy: 0.9641 - val_loss: 0.1032 - val_accuracy: 0.9695\n",
            "Epoch 70/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1220 - accuracy: 0.9639 - val_loss: 0.1029 - val_accuracy: 0.9697\n",
            "Epoch 71/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.1193 - accuracy: 0.9644 - val_loss: 0.1017 - val_accuracy: 0.9703\n",
            "Epoch 72/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.1162 - accuracy: 0.9653 - val_loss: 0.1015 - val_accuracy: 0.9707\n",
            "Epoch 73/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.1170 - accuracy: 0.9655 - val_loss: 0.1009 - val_accuracy: 0.9697\n",
            "Epoch 74/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.1170 - accuracy: 0.9656 - val_loss: 0.1003 - val_accuracy: 0.9703\n",
            "Epoch 75/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.1173 - accuracy: 0.9653 - val_loss: 0.0998 - val_accuracy: 0.9707\n",
            "Epoch 76/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.1137 - accuracy: 0.9661 - val_loss: 0.0994 - val_accuracy: 0.9708\n",
            "Epoch 77/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.1130 - accuracy: 0.9673 - val_loss: 0.0986 - val_accuracy: 0.9704\n",
            "Epoch 78/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.1118 - accuracy: 0.9662 - val_loss: 0.0981 - val_accuracy: 0.9710\n",
            "Epoch 79/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.1112 - accuracy: 0.9670 - val_loss: 0.0978 - val_accuracy: 0.9712\n",
            "Epoch 80/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.1118 - accuracy: 0.9672 - val_loss: 0.0974 - val_accuracy: 0.9713\n",
            "Epoch 81/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.1089 - accuracy: 0.9672 - val_loss: 0.0968 - val_accuracy: 0.9715\n",
            "Epoch 82/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.1081 - accuracy: 0.9673 - val_loss: 0.0964 - val_accuracy: 0.9717\n",
            "Epoch 83/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.1077 - accuracy: 0.9678 - val_loss: 0.0960 - val_accuracy: 0.9716\n",
            "Epoch 84/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.1072 - accuracy: 0.9681 - val_loss: 0.0956 - val_accuracy: 0.9709\n",
            "Epoch 85/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.1083 - accuracy: 0.9674 - val_loss: 0.0956 - val_accuracy: 0.9720\n",
            "Epoch 86/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.1025 - accuracy: 0.9692 - val_loss: 0.0951 - val_accuracy: 0.9719\n",
            "Epoch 87/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.1056 - accuracy: 0.9685 - val_loss: 0.0946 - val_accuracy: 0.9714\n",
            "Epoch 88/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.1049 - accuracy: 0.9684 - val_loss: 0.0942 - val_accuracy: 0.9719\n",
            "Epoch 89/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.1029 - accuracy: 0.9697 - val_loss: 0.0939 - val_accuracy: 0.9713\n",
            "Epoch 90/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.1021 - accuracy: 0.9696 - val_loss: 0.0936 - val_accuracy: 0.9723\n",
            "Epoch 91/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.1007 - accuracy: 0.9696 - val_loss: 0.0928 - val_accuracy: 0.9722\n",
            "Epoch 92/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.1025 - accuracy: 0.9695 - val_loss: 0.0924 - val_accuracy: 0.9721\n",
            "Epoch 93/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0994 - accuracy: 0.9703 - val_loss: 0.0929 - val_accuracy: 0.9727\n",
            "Epoch 94/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.1015 - accuracy: 0.9693 - val_loss: 0.0921 - val_accuracy: 0.9731\n",
            "Epoch 95/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.1011 - accuracy: 0.9695 - val_loss: 0.0914 - val_accuracy: 0.9728\n",
            "Epoch 96/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0974 - accuracy: 0.9715 - val_loss: 0.0912 - val_accuracy: 0.9727\n",
            "Epoch 97/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0984 - accuracy: 0.9703 - val_loss: 0.0906 - val_accuracy: 0.9736\n",
            "Epoch 98/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0971 - accuracy: 0.9712 - val_loss: 0.0901 - val_accuracy: 0.9733\n",
            "Epoch 99/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0963 - accuracy: 0.9704 - val_loss: 0.0902 - val_accuracy: 0.9732\n",
            "Epoch 100/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0932 - accuracy: 0.9720 - val_loss: 0.0908 - val_accuracy: 0.9733\n",
            "Epoch 101/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0952 - accuracy: 0.9710 - val_loss: 0.0894 - val_accuracy: 0.9739\n",
            "Epoch 102/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0934 - accuracy: 0.9716 - val_loss: 0.0893 - val_accuracy: 0.9737\n",
            "Epoch 103/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0936 - accuracy: 0.9719 - val_loss: 0.0890 - val_accuracy: 0.9737\n",
            "Epoch 104/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0925 - accuracy: 0.9726 - val_loss: 0.0893 - val_accuracy: 0.9739\n",
            "Epoch 105/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0950 - accuracy: 0.9711 - val_loss: 0.0888 - val_accuracy: 0.9734\n",
            "Epoch 106/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0917 - accuracy: 0.9722 - val_loss: 0.0886 - val_accuracy: 0.9736\n",
            "Epoch 107/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0913 - accuracy: 0.9729 - val_loss: 0.0885 - val_accuracy: 0.9741\n",
            "Epoch 108/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0910 - accuracy: 0.9726 - val_loss: 0.0880 - val_accuracy: 0.9740\n",
            "Epoch 109/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0888 - accuracy: 0.9732 - val_loss: 0.0878 - val_accuracy: 0.9732\n",
            "Epoch 110/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0888 - accuracy: 0.9734 - val_loss: 0.0875 - val_accuracy: 0.9737\n",
            "Epoch 111/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0891 - accuracy: 0.9735 - val_loss: 0.0871 - val_accuracy: 0.9737\n",
            "Epoch 112/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0886 - accuracy: 0.9724 - val_loss: 0.0866 - val_accuracy: 0.9746\n",
            "Epoch 113/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0882 - accuracy: 0.9730 - val_loss: 0.0861 - val_accuracy: 0.9747\n",
            "Epoch 114/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0873 - accuracy: 0.9739 - val_loss: 0.0860 - val_accuracy: 0.9749\n",
            "Epoch 115/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0874 - accuracy: 0.9729 - val_loss: 0.0867 - val_accuracy: 0.9740\n",
            "Epoch 116/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0852 - accuracy: 0.9741 - val_loss: 0.0860 - val_accuracy: 0.9744\n",
            "Epoch 117/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0847 - accuracy: 0.9751 - val_loss: 0.0858 - val_accuracy: 0.9743\n",
            "Epoch 118/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0853 - accuracy: 0.9749 - val_loss: 0.0859 - val_accuracy: 0.9743\n",
            "Epoch 119/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0851 - accuracy: 0.9733 - val_loss: 0.0851 - val_accuracy: 0.9747\n",
            "Epoch 120/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0843 - accuracy: 0.9739 - val_loss: 0.0853 - val_accuracy: 0.9747\n",
            "Epoch 121/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0832 - accuracy: 0.9757 - val_loss: 0.0856 - val_accuracy: 0.9745\n",
            "Epoch 122/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.0847 - accuracy: 0.9745 - val_loss: 0.0853 - val_accuracy: 0.9748\n",
            "Epoch 123/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0825 - accuracy: 0.9750 - val_loss: 0.0850 - val_accuracy: 0.9750\n",
            "Epoch 124/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0808 - accuracy: 0.9754 - val_loss: 0.0852 - val_accuracy: 0.9747\n",
            "Epoch 125/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0809 - accuracy: 0.9755 - val_loss: 0.0849 - val_accuracy: 0.9748\n",
            "Epoch 126/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0804 - accuracy: 0.9761 - val_loss: 0.0845 - val_accuracy: 0.9750\n",
            "Epoch 127/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.0797 - accuracy: 0.9756 - val_loss: 0.0843 - val_accuracy: 0.9752\n",
            "Epoch 128/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0820 - accuracy: 0.9752 - val_loss: 0.0837 - val_accuracy: 0.9755\n",
            "Epoch 129/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0805 - accuracy: 0.9754 - val_loss: 0.0837 - val_accuracy: 0.9754\n",
            "Epoch 130/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0779 - accuracy: 0.9764 - val_loss: 0.0843 - val_accuracy: 0.9756\n",
            "Epoch 131/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0786 - accuracy: 0.9765 - val_loss: 0.0836 - val_accuracy: 0.9749\n",
            "Epoch 132/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0774 - accuracy: 0.9761 - val_loss: 0.0837 - val_accuracy: 0.9747\n",
            "Epoch 133/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.0772 - accuracy: 0.9768 - val_loss: 0.0831 - val_accuracy: 0.9750\n",
            "Epoch 134/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0772 - accuracy: 0.9761 - val_loss: 0.0836 - val_accuracy: 0.9754\n",
            "Epoch 135/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0766 - accuracy: 0.9764 - val_loss: 0.0832 - val_accuracy: 0.9753\n",
            "Epoch 136/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0777 - accuracy: 0.9762 - val_loss: 0.0830 - val_accuracy: 0.9753\n",
            "Epoch 137/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0774 - accuracy: 0.9760 - val_loss: 0.0826 - val_accuracy: 0.9750\n",
            "Epoch 138/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0758 - accuracy: 0.9769 - val_loss: 0.0830 - val_accuracy: 0.9751\n",
            "Epoch 139/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0757 - accuracy: 0.9771 - val_loss: 0.0821 - val_accuracy: 0.9753\n",
            "Epoch 140/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0753 - accuracy: 0.9773 - val_loss: 0.0821 - val_accuracy: 0.9758\n",
            "Epoch 141/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0742 - accuracy: 0.9776 - val_loss: 0.0820 - val_accuracy: 0.9759\n",
            "Epoch 142/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.0724 - accuracy: 0.9777 - val_loss: 0.0820 - val_accuracy: 0.9754\n",
            "Epoch 143/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0729 - accuracy: 0.9781 - val_loss: 0.0818 - val_accuracy: 0.9752\n",
            "Epoch 144/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0737 - accuracy: 0.9770 - val_loss: 0.0819 - val_accuracy: 0.9753\n",
            "Epoch 145/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0711 - accuracy: 0.9789 - val_loss: 0.0819 - val_accuracy: 0.9755\n",
            "Epoch 146/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0736 - accuracy: 0.9772 - val_loss: 0.0819 - val_accuracy: 0.9758\n",
            "Epoch 147/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0729 - accuracy: 0.9775 - val_loss: 0.0817 - val_accuracy: 0.9761\n",
            "Epoch 148/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0741 - accuracy: 0.9776 - val_loss: 0.0812 - val_accuracy: 0.9757\n",
            "Epoch 149/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0709 - accuracy: 0.9782 - val_loss: 0.0812 - val_accuracy: 0.9759\n",
            "Epoch 150/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0713 - accuracy: 0.9778 - val_loss: 0.0808 - val_accuracy: 0.9756\n",
            "Epoch 151/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0721 - accuracy: 0.9776 - val_loss: 0.0807 - val_accuracy: 0.9762\n",
            "Epoch 152/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0694 - accuracy: 0.9789 - val_loss: 0.0811 - val_accuracy: 0.9762\n",
            "Epoch 153/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0697 - accuracy: 0.9783 - val_loss: 0.0806 - val_accuracy: 0.9754\n",
            "Epoch 154/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0691 - accuracy: 0.9787 - val_loss: 0.0808 - val_accuracy: 0.9758\n",
            "Epoch 155/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0699 - accuracy: 0.9786 - val_loss: 0.0807 - val_accuracy: 0.9759\n",
            "Epoch 156/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0695 - accuracy: 0.9786 - val_loss: 0.0806 - val_accuracy: 0.9760\n",
            "Epoch 157/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0674 - accuracy: 0.9799 - val_loss: 0.0804 - val_accuracy: 0.9763\n",
            "Epoch 158/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0700 - accuracy: 0.9789 - val_loss: 0.0804 - val_accuracy: 0.9767\n",
            "Epoch 159/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0688 - accuracy: 0.9789 - val_loss: 0.0801 - val_accuracy: 0.9764\n",
            "Epoch 160/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0671 - accuracy: 0.9793 - val_loss: 0.0800 - val_accuracy: 0.9762\n",
            "Epoch 161/200\n",
            "48000/48000 [==============================] - 2s 41us/sample - loss: 0.0680 - accuracy: 0.9793 - val_loss: 0.0799 - val_accuracy: 0.9763\n",
            "Epoch 162/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0674 - accuracy: 0.9793 - val_loss: 0.0795 - val_accuracy: 0.9766\n",
            "Epoch 163/200\n",
            "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0671 - accuracy: 0.9789 - val_loss: 0.0797 - val_accuracy: 0.9764\n",
            "Epoch 164/200\n",
            "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0672 - accuracy: 0.9791 - val_loss: 0.0800 - val_accuracy: 0.9763\n",
            "Epoch 165/200\n",
            "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0650 - accuracy: 0.9801 - val_loss: 0.0798 - val_accuracy: 0.9766\n",
            "Epoch 166/200\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.0640 - accuracy: 0.9795 - val_loss: 0.0797 - val_accuracy: 0.9768\n",
            "Epoch 167/200\n",
            "48000/48000 [==============================] - 4s 75us/sample - loss: 0.0645 - accuracy: 0.9802 - val_loss: 0.0798 - val_accuracy: 0.9761\n",
            "Epoch 168/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0660 - accuracy: 0.9800 - val_loss: 0.0793 - val_accuracy: 0.9763\n",
            "Epoch 169/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0647 - accuracy: 0.9798 - val_loss: 0.0786 - val_accuracy: 0.9772\n",
            "Epoch 170/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0645 - accuracy: 0.9803 - val_loss: 0.0791 - val_accuracy: 0.9766\n",
            "Epoch 171/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0639 - accuracy: 0.9802 - val_loss: 0.0795 - val_accuracy: 0.9763\n",
            "Epoch 172/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0638 - accuracy: 0.9800 - val_loss: 0.0786 - val_accuracy: 0.9768\n",
            "Epoch 173/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0628 - accuracy: 0.9801 - val_loss: 0.0797 - val_accuracy: 0.9773\n",
            "Epoch 174/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0643 - accuracy: 0.9801 - val_loss: 0.0792 - val_accuracy: 0.9768\n",
            "Epoch 175/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0626 - accuracy: 0.9805 - val_loss: 0.0788 - val_accuracy: 0.9778\n",
            "Epoch 176/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0643 - accuracy: 0.9803 - val_loss: 0.0781 - val_accuracy: 0.9766\n",
            "Epoch 177/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0618 - accuracy: 0.9799 - val_loss: 0.0783 - val_accuracy: 0.9772\n",
            "Epoch 178/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0635 - accuracy: 0.9803 - val_loss: 0.0786 - val_accuracy: 0.9771\n",
            "Epoch 179/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0611 - accuracy: 0.9805 - val_loss: 0.0784 - val_accuracy: 0.9774\n",
            "Epoch 180/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0615 - accuracy: 0.9809 - val_loss: 0.0781 - val_accuracy: 0.9773\n",
            "Epoch 181/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0610 - accuracy: 0.9811 - val_loss: 0.0789 - val_accuracy: 0.9776\n",
            "Epoch 182/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0604 - accuracy: 0.9818 - val_loss: 0.0782 - val_accuracy: 0.9768\n",
            "Epoch 183/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0593 - accuracy: 0.9820 - val_loss: 0.0790 - val_accuracy: 0.9770\n",
            "Epoch 184/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0610 - accuracy: 0.9811 - val_loss: 0.0783 - val_accuracy: 0.9772\n",
            "Epoch 185/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0604 - accuracy: 0.9816 - val_loss: 0.0784 - val_accuracy: 0.9770\n",
            "Epoch 186/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.0600 - accuracy: 0.9818 - val_loss: 0.0782 - val_accuracy: 0.9778\n",
            "Epoch 187/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0596 - accuracy: 0.9816 - val_loss: 0.0783 - val_accuracy: 0.9777\n",
            "Epoch 188/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0588 - accuracy: 0.9820 - val_loss: 0.0785 - val_accuracy: 0.9772\n",
            "Epoch 189/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0601 - accuracy: 0.9816 - val_loss: 0.0779 - val_accuracy: 0.9773\n",
            "Epoch 190/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0592 - accuracy: 0.9819 - val_loss: 0.0781 - val_accuracy: 0.9774\n",
            "Epoch 191/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0590 - accuracy: 0.9815 - val_loss: 0.0784 - val_accuracy: 0.9774\n",
            "Epoch 192/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0577 - accuracy: 0.9823 - val_loss: 0.0781 - val_accuracy: 0.9773\n",
            "Epoch 193/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.0589 - accuracy: 0.9823 - val_loss: 0.0782 - val_accuracy: 0.9773\n",
            "Epoch 194/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0578 - accuracy: 0.9819 - val_loss: 0.0781 - val_accuracy: 0.9772\n",
            "Epoch 195/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0572 - accuracy: 0.9824 - val_loss: 0.0781 - val_accuracy: 0.9773\n",
            "Epoch 196/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0562 - accuracy: 0.9826 - val_loss: 0.0777 - val_accuracy: 0.9777\n",
            "Epoch 197/200\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0560 - accuracy: 0.9830 - val_loss: 0.0774 - val_accuracy: 0.9778\n",
            "Epoch 198/200\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0571 - accuracy: 0.9826 - val_loss: 0.0776 - val_accuracy: 0.9778\n",
            "Epoch 199/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.0567 - accuracy: 0.9825 - val_loss: 0.0778 - val_accuracy: 0.9777\n",
            "Epoch 200/200\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.0569 - accuracy: 0.9824 - val_loss: 0.0775 - val_accuracy: 0.9779\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc8a22fd048>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_h5m16OJL0V",
        "colab_type": "code",
        "outputId": "a8919654-c784-4020-a3e8-c52c37bee6eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# evaluate the model\n",
        "test_loss, test_acc = model.evaluate(X_test, Y_testOH)\n",
        "print('Test accuracy: ', test_acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 0s 46us/sample - loss: 0.0737 - accuracy: 0.9773\n",
            "Test accuracy:  0.9773\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWaodFLXJoRo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make predictions\n",
        "predictions = model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3cNxoy4Jsrm",
        "colab_type": "code",
        "outputId": "2a250f23-dbed-43f8-8b9b-fb68b04fb334",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "predictions"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3.40219032e-07, 1.47596095e-08, 2.39010501e-06, ...,\n",
              "        9.99951005e-01, 4.96505503e-09, 4.94277765e-06],\n",
              "       [4.22175503e-07, 3.55415541e-05, 9.99937296e-01, ...,\n",
              "        1.16563481e-10, 2.17283446e-06, 2.17305579e-12],\n",
              "       [2.07913288e-07, 9.99230623e-01, 1.15808354e-04, ...,\n",
              "        5.20906236e-04, 1.00044148e-04, 4.68133862e-07],\n",
              "       ...,\n",
              "       [7.14193010e-11, 5.82849824e-10, 2.61261873e-10, ...,\n",
              "        8.75356091e-06, 7.85256944e-08, 9.69236771e-06],\n",
              "       [1.03119868e-08, 2.83898544e-10, 4.60824851e-11, ...,\n",
              "        7.61138796e-09, 9.63260463e-05, 1.67404479e-10],\n",
              "       [8.73824092e-07, 1.42860834e-10, 1.86762165e-07, ...,\n",
              "        3.28755738e-11, 2.90824320e-09, 2.13695561e-10]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MD9gNGlYNrm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTvS-IiJYakb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtxCAnMkYbxX",
        "colab_type": "text"
      },
      "source": [
        "#**Sentiment Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCBoIsiMYgLr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models, preprocessing\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4tJ1R0QY1QT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_len = 200\n",
        "n_words = 10000\n",
        "dim_embeddings = 256\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 500"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3mnwYlWaD8f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data():\n",
        "  # load data\n",
        "  (X_train, y_train), (X_test, y_test) = datasets.imdb.load_data(num_words=n_words)\n",
        "  # Pad sequences with max_len\n",
        "  X_train = preprocessing.sequence.pad_sequences(X_train, maxlen=max_len)\n",
        "  X_test = preprocessing.sequence.pad_sequences(X_test, maxlen=max_len)\n",
        "  return (X_train, y_train), (X_test, y_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EU-LRa0VdN9Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = models.Sequential()\n",
        "  # Input - Embedding Layer\n",
        "  # the model will take as input integer matrix of size (batch, input_length)\n",
        "  # the model will output dimension (input_length, dim_embedding)\n",
        "  # the largest integer in the input should be no larger\n",
        "  # than n_words (vocabulary size)\n",
        "  model.add(layers.Embedding(n_words, dim_embeddings, input_length=max_len))\n",
        "  model.add(layers.Dropout(0.3))\n",
        "\n",
        "  # takes the maximum value of either feature vector from each of the n_words features\n",
        "  model.add(layers.GlobalMaxPooling1D())\n",
        "  model.add(layers.Dense(128, activation='relu'))\n",
        "  model.add(layers.Dropout(0.5))\n",
        "  model.add(layers.Dense(1, activation='sigmoid'))\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZr3qwzae5MT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3928e197-54b5-4c16-a539-8fa243eadb03"
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = load_data()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-F2LjLafMTu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "a71ebe25-8039-454b-fda4-be6a81c6f53c"
      },
      "source": [
        "print(X_train[:2])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[   5   25  100   43  838  112   50  670    2    9   35  480  284    5\n",
            "   150    4  172  112  167    2  336  385   39    4  172 4536 1111   17\n",
            "   546   38   13  447    4  192   50   16    6  147 2025   19   14   22\n",
            "     4 1920 4613  469    4   22   71   87   12   16   43  530   38   76\n",
            "    15   13 1247    4   22   17  515   17   12   16  626   18    2    5\n",
            "    62  386   12    8  316    8  106    5    4 2223 5244   16  480   66\n",
            "  3785   33    4  130   12   16   38  619    5   25  124   51   36  135\n",
            "    48   25 1415   33    6   22   12  215   28   77   52    5   14  407\n",
            "    16   82    2    8    4  107  117 5952   15  256    4    2    7 3766\n",
            "     5  723   36   71   43  530  476   26  400  317   46    7    4    2\n",
            "  1029   13  104   88    4  381   15  297   98   32 2071   56   26  141\n",
            "     6  194 7486   18    4  226   22   21  134  476   26  480    5  144\n",
            "    30 5535   18   51   36   28  224   92   25  104    4  226   65   16\n",
            "    38 1334   88   12   16  283    5   16 4472  113  103   32   15   16\n",
            "  5345   19  178   32]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    1  194 1153\n",
            "   194 8255   78  228    5    6 1463 4369 5012  134   26    4  715    8\n",
            "   118 1634   14  394   20   13  119  954  189  102    5  207  110 3103\n",
            "    21   14   69  188    8   30   23    7    4  249  126   93    4  114\n",
            "     9 2300 1523    5  647    4  116    9   35 8163    4  229    9  340\n",
            "  1322    4  118    9    4  130 4901   19    4 1002    5   89   29  952\n",
            "    46   37    4  455    9   45   43   38 1543 1905  398    4 1649   26\n",
            "  6853    5  163   11 3215    2    4 1153    9  194  775    7 8255    2\n",
            "   349 2637  148  605    2 8003   15  123  125   68    2 6853   15  349\n",
            "   165 4362   98    5    4  228    9   43    2 1157   15  299  120    5\n",
            "   120  174   11  220  175  136   50    9 4373  228 8255    5    2  656\n",
            "   245 2350    5    4 9837  131  152  491   18    2   32 7464 1212   14\n",
            "     9    6  371   78   22  625   64 1382    9    8  168  145   23    4\n",
            "  1690   15   16    4 1355    5   28    6   52  154  462   33   89   78\n",
            "   285   16  145   95]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEEfR8YUfU8e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b09f7615-ada9-426d-95e4-56f5fd569e74"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 200)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OgmxRVxfcad",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1bef02c7-306a-41c1-85f3-4715be822cb4"
      },
      "source": [
        "14*13+4"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "186"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BP2EGbHJfkx9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "39cc69f0-743c-4bc9-dacf-0e1392d03edd"
      },
      "source": [
        "print(y_train[:2])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbixxghjf50s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bdd47e4b-cac1-4f73-8265-2a4f77972a6f"
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2tERSXaf9nr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4nXPEGZgWea",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "1e896c86-b0f9-4af8-cb2a-263ff3829e45"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 200, 256)          2560000   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 200, 256)          0         \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 2,593,025\n",
            "Trainable params: 2,593,025\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzNkkkK6gXta",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        },
        "outputId": "c524371c-636d-4b44-eb6d-c53b4659d038"
      },
      "source": [
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "score = model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(X_test, y_test))\n",
        "score = model.evaluate(X_test, y_test, batch_size=BATCH_SIZE)\n",
        "print(\"\\nTest score: \", score[0])\n",
        "print(\"Test accuracy: \", score[1])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/20\n",
            "25000/25000 [==============================] - 26s 1ms/sample - loss: 0.6732 - accuracy: 0.6296 - val_loss: 0.6323 - val_accuracy: 0.8345\n",
            "Epoch 2/20\n",
            "25000/25000 [==============================] - 25s 1ms/sample - loss: 0.4608 - accuracy: 0.8412 - val_loss: 0.3602 - val_accuracy: 0.8600\n",
            "Epoch 3/20\n",
            "25000/25000 [==============================] - 26s 1ms/sample - loss: 0.2783 - accuracy: 0.8891 - val_loss: 0.3148 - val_accuracy: 0.8708\n",
            "Epoch 4/20\n",
            "25000/25000 [==============================] - 26s 1ms/sample - loss: 0.2167 - accuracy: 0.9182 - val_loss: 0.2894 - val_accuracy: 0.8791\n",
            "Epoch 5/20\n",
            "25000/25000 [==============================] - 26s 1ms/sample - loss: 0.1735 - accuracy: 0.9368 - val_loss: 0.2894 - val_accuracy: 0.8780\n",
            "Epoch 6/20\n",
            "25000/25000 [==============================] - 26s 1ms/sample - loss: 0.1337 - accuracy: 0.9553 - val_loss: 0.2933 - val_accuracy: 0.8749\n",
            "Epoch 7/20\n",
            "25000/25000 [==============================] - 26s 1ms/sample - loss: 0.1046 - accuracy: 0.9668 - val_loss: 0.3035 - val_accuracy: 0.8712\n",
            "Epoch 8/20\n",
            "25000/25000 [==============================] - 26s 1ms/sample - loss: 0.0779 - accuracy: 0.9770 - val_loss: 0.3170 - val_accuracy: 0.8670\n",
            "Epoch 9/20\n",
            "25000/25000 [==============================] - 25s 1ms/sample - loss: 0.0593 - accuracy: 0.9850 - val_loss: 0.3316 - val_accuracy: 0.8650\n",
            "Epoch 10/20\n",
            "25000/25000 [==============================] - 26s 1ms/sample - loss: 0.0460 - accuracy: 0.9885 - val_loss: 0.3531 - val_accuracy: 0.8608\n",
            "Epoch 11/20\n",
            "25000/25000 [==============================] - 26s 1ms/sample - loss: 0.0353 - accuracy: 0.9920 - val_loss: 0.3663 - val_accuracy: 0.8586\n",
            "Epoch 12/20\n",
            "25000/25000 [==============================] - 25s 1ms/sample - loss: 0.0258 - accuracy: 0.9952 - val_loss: 0.3843 - val_accuracy: 0.8565\n",
            "Epoch 13/20\n",
            "25000/25000 [==============================] - 26s 1ms/sample - loss: 0.0202 - accuracy: 0.9960 - val_loss: 0.4018 - val_accuracy: 0.8572\n",
            "Epoch 14/20\n",
            "25000/25000 [==============================] - 25s 1ms/sample - loss: 0.0171 - accuracy: 0.9971 - val_loss: 0.4264 - val_accuracy: 0.8541\n",
            "Epoch 15/20\n",
            "25000/25000 [==============================] - 25s 1ms/sample - loss: 0.0133 - accuracy: 0.9980 - val_loss: 0.4380 - val_accuracy: 0.8540\n",
            "Epoch 16/20\n",
            "25000/25000 [==============================] - 25s 1ms/sample - loss: 0.0112 - accuracy: 0.9981 - val_loss: 0.4446 - val_accuracy: 0.8549\n",
            "Epoch 17/20\n",
            "25000/25000 [==============================] - 25s 1ms/sample - loss: 0.0097 - accuracy: 0.9986 - val_loss: 0.4577 - val_accuracy: 0.8539\n",
            "Epoch 18/20\n",
            "25000/25000 [==============================] - 25s 1ms/sample - loss: 0.0082 - accuracy: 0.9988 - val_loss: 0.4738 - val_accuracy: 0.8526\n",
            "Epoch 19/20\n",
            "25000/25000 [==============================] - 26s 1ms/sample - loss: 0.0073 - accuracy: 0.9992 - val_loss: 0.4791 - val_accuracy: 0.8517\n",
            "Epoch 20/20\n",
            "25000/25000 [==============================] - 26s 1ms/sample - loss: 0.0067 - accuracy: 0.9990 - val_loss: 0.4970 - val_accuracy: 0.8509\n",
            "25000/25000 [==============================] - 2s 89us/sample - loss: 0.4970 - accuracy: 0.8509\n",
            "\n",
            "Test score:  0.49695072531700135\n",
            "Test accuracy:  0.85092\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_tt32PMhxBq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}